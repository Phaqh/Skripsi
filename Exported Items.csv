"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"4B363ES9","conferencePaper","2011","Piccolo, Elio; Squillero, Giovanni","Adaptive opponent modelling for the iterated prisoner's dilemma","2011 IEEE Congress of Evolutionary Computation (CEC)","","","10.1109/CEC.2011.5949705","https://ieeexplore.ieee.org/document/5949705/","This paper describes the design of Laran, an intelligent player for the iterated prisoner's dilemma. Laran is based on an evolutionary algorithm, but instead of using evolution as a mean to define a suitable strategy, it uses evolution to model the behavior of its adversary. In some sense, it understands its opponent, and then exploits such knowledge to devise the best possible conduct. The internal model of the opponent is continuously adapted during the game to match the actual outcome of the game, taking into consideration all played actions. Whether the model is correct, Laran is likely to gain constant advantages and eventually win. A prototype of the proposed approach was matched against twenty players implementing state-of-the art strategies. Results clearly demonstrated the claims.","2011-06","2025-11-04 08:56:57","2025-11-04 09:58:46","2025-11-04 08:56:16","836-841","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/paqh/Zotero/storage/CA4NN9XQ/Piccolo and Squillero - 2011 - Adaptive opponent modelling for the iterated prisoner's dilemma.pdf","","Evolutionary algorithms; FSM; Gain constant; games; Internal models; Iterated prisoner's dilemma; State of the art","Adaptation models; Cloning; Computational modeling; Evolution (biology); evolutionary algorithm; Evolutionary computation; Game theory; Games; iterated prisoner's dilemma","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2011 IEEE Congress of Evolutionary Computation (CEC)","","","","","","","","","","","","","","",""
"Q9SGDVLN","journalArticle","2016","Gaudesi, Marco; Piccolo, Elio; Squillero, Giovanni; Tonda, Alberto","Exploiting Evolutionary Modeling to Prevail in Iterated Prisoner’s Dilemma Tournaments","IEEE Transactions on Computational Intelligence and AI in Games","","1943-0698","10.1109/TCIAIG.2015.2439061","https://ieeexplore.ieee.org/document/7114231/","The iterated prisoner's dilemma is a famous model of cooperation and conflict in game theory. Its origin can be traced back to the Cold War, and countless strategies for playing it have been proposed so far, either designed by hand or automatically generated by computers. In the 2000s, scholars started focusing on adaptive players, that is, able to classify their opponent's behavior and adopt an effective counter-strategy. The player presented in this paper, pushes such idea even further: it builds a model of the current adversary from scratch, without relying on any pre-defined archetypes, and tweaks it as the game develops using an evolutionary algorithm; at the same time, it exploits the model to lead the game into the most favorable continuation. Models are compact nondeterministic finite state machines; they are extremely efficient in predicting opponents' replies, without being completely correct by necessity. Experimental results show that such a player is able to win several one-to-one games against strong opponents taken from the literature, and that it consistently prevails in round-robin tournaments of different sizes.","2016-09","2025-11-04 08:56:57","2025-11-04 09:58:49","2025-11-04 08:56:19","288-300","","3","8","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/paqh/Zotero/storage/5Y77VQI5/Gaudesi et al. - 2016 - Exploiting Evolutionary Modeling to Prevail in Iterated Prisoner’s Dilemma Tournaments.pdf","","Automatically generated; Cold wars; Different sizes; Evolutionary algorithms; Evolutionary models; Game theory; Iterated prisoner's dilemma; Non-deterministic finite state machine; Opponent modeling; Round robin tournaments; Routers","Adaptation models; Computational modeling; Games; iterated prisoner’s dilemma; Mathematical model; nondeterministic finite state machine; opponent modeling; Sociology; Statistics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABBIPEPA","conferencePaper","2013","Park, Hyunsoo; Kim, Kyung-Joong","Opponent modeling with incremental active learning: A case study of Iterative Prisoner's Dilemma","2013 IEEE Conference on Computational Inteligence in Games (CIG)","","","10.1109/CIG.2013.6633665","https://ieeexplore.ieee.org/document/6633665/","What's the most important sources of information to guess the internal strategy of your opponents? The best way is to play games against them and infer their strategy from the experience. For novice players, they should play lot of games to identify other's strategy successfully. However, experienced players usually play small number of games to model other's strategy. The secret is that they intelligently design their plays to maximize the chance of discovering the most uncertain parts. Similarly, in this paper, we propose to use an incremental active learning for modeling opponents. It refines the other's models incrementally by cycling “estimation (inference)“ and “exploration (playing games)” steps. Experimental results with Iterative Prisoner's Dilemma games show that the proposed method can reveal other's strategy successfully.","2013-08","2025-11-04 08:56:57","2025-11-04 09:59:08","2025-11-04 08:56:22","1-2","","","","","","Opponent modeling with incremental active learning","","","","","","","","","","","","IEEE Xplore","","ISSN: 2325-4289","","/home/paqh/Zotero/storage/WVSWGB5P/Park and Kim - 2013 - Opponent modeling with incremental active learning A case study of Iterative Prisoner's Dilemma.pdf","","Active Learning; Artificial intelligence; Estimation-exploration algorithms; Game theory; Internal strategies; Iterative methods; Iterative prisoner's dilemmas; Opponent modeling; Sources of informations; Theory of minds","estimation-exploration algorithm; game theory; Games; Genetic algorithms; iterative prisoner's dilemma; Observers; Reverse engineering; Robots; theory of mind; Trajectory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2013 IEEE Conference on Computational Inteligence in Games (CIG)","","","","","","","","","","","","","","",""
"UHH6AFKD","conferencePaper","2024","Qiao, Xinyu; Han, Congyin; Guo, Tainde","O2M: Online Opponent Modeling in Online General-Sum Matrix Games","2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)","","","10.1109/ICAIRC64177.2024.10899996","https://ieeexplore.ieee.org/document/10899996/","This paper focuses on strategy learning in online general-sum games, specifically addressing online matrix games (MGs), where two players control parameters and face an unknown, stochastically varying payoff matrix. The goal is to learn strategies that maximize long-term rewards within this dynamic and uncertain game environment. We introduce Online Opponent Modeling (O2M), a novel algorithm designed to overcome the limitations of OMG-RFTL, which struggles in general-sum game settings. O2M incorporates opponent modeling techniques into online game frameworks, alleviating the non-stationarity issue inherent in multi-agent systems and enabling agents to achieve superior rewards.","2024-12","2025-11-04 08:56:57","2025-11-04 09:53:06","2025-11-04 08:56:25","358-361","","","","","","O2M","","","","","","","","","","","","IEEE Xplore","","","","/home/paqh/Zotero/storage/BSW9J3D3/Qiao et al. - 2024 - O2M Online Opponent Modeling in Online General-Sum Matrix Games.pdf","","","Artificial intelligence; Faces; Games; Heuristic algorithms; Matrix Game; Multi-agent systems; Online learning; Opponent modeling; Robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)","","","","","","","","","","","","","","",""
"YGYUYYH2","conferencePaper","2002","Bjarnason, R.V.; Peterson, T.S.","Multi-agent learning via implicit opponent modeling","Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)","","","10.1109/CEC.2002.1004470","https://ieeexplore.ieee.org/document/1004470/","We present a learning algorithm for two player stochastic games. The algorithm generates optimal deterministic finite automata (DFA) strategies against opponents who can be modeled by probabilistic action automata. The algorithm generates dynamic history trees based on statistical tests to eliminate state aliasing. Experiments are conducted in an iterated prisoner's dilemma environment.","2002-05","2025-11-04 08:56:57","2025-11-04 09:59:05","2025-11-04 08:56:28","1534-1539 vol.2","","","2","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/paqh/Zotero/storage/PEMU7DEW/Bjarnason and Peterson - 2002 - Multi-agent learning via implicit opponent modeling.pdf","","Algorithms; Aliasing; Automata theory; Deterministic finite automata; Game theory; Iterated prisoner's dilemma; Multi-agent learning; Opponent modeling; Stochastic game; Trees (mathematics)","Computational modeling; Computer science; Doped fiber amplifiers; Heuristic algorithms; History; Learning automata; Nash equilibrium; Stochastic processes; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)","","","","","","","","","","","","","","",""
"EAQ3CLF9","conferencePaper","2002","Clymer, J.R.","Simulation-based engineering of complex systems using EXTEND+MFG+OpEMCSS","Proceedings of the Winter Simulation Conference","","","10.1109/WSC.2002.1172878","https://ieeexplore.ieee.org/document/1172878/","A Complex Adaptive System (CAS) is a network of self-organizing, intelligent agents that share knowledge and adapt their operations in order to achieve overall system goals. Three things are needed to understand, design, and evaluate CAS. First, a mathematical model or way-of-thinking about CAS, called Context-Sensitive Systems (CSS) theory, is required to provide a solid foundation upon which to represent and describe the kinds of interactions that occur among the CAS agents during system operation. Second, a graphical modeling language is required that implements CSS theory in a way that enhances visualization and understanding of CAS. Third, a systems design and evaluation tool is required that makes it easy to apply CSS theory, expressed using a graphical modeling language, to understand, design, and evaluate CAS. As an example, an OpEMCSS model of two intelligent agents is discussed that learn rules and maximize their average reward in the prisoner's dilemma game.","2002-12","2025-11-04 08:56:57","2025-11-04 09:53:18","2025-11-04 08:56:31","147-156 vol.1","","","1","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/paqh/Zotero/storage/TB6FWAKC/Clymer - 2002 - Simulation-based engineering of complex systems using EXTEND+MFG+OpEMCSS.pdf","","","Adaptive systems; Air traffic control; Cascading style sheets; Content addressable storage; Data engineering; Databases; Design engineering; Intelligent agent; Modeling; Systems engineering and theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the Winter Simulation Conference","","","","","","","","","","","","","","",""
"UMY7IVTX","conferencePaper","2025","Zhu, Lei; Zhu, Yuying; Xia, Chengyi","Evolutionary Dynamics of Cooperation and Extortion on Networks With Fitness-Dependent Rules","2025 Joint International Conference on Automation-Intelligence-Safety (ICAIS) & International Symposium on Autonomous Systems (ISAS)","","","10.1109/ICAISISAS64483.2025.11051564","https://ieeexplore.ieee.org/document/11051564/","This study establishes a multi-agent modeling framework to analyze the evolutionary dynamics of zero-determinant strategies on regular networks under hybrid social dilemmas integrating Prisoner’s Dilemma and Snowdrift game. By formulating fitness-driven dynamical equations, theoretical and numerical results reveal that cooperators exhibit dominance in steady-state under pairwise comparison rules, whereas extortion strategies demonstrate significant invasion characteristics in scale-free networks under death-birth update rule. Notably, the influence of both temptation and extortion factors on cooperation evolution exhibits a pronounced rule-dependent phenomenon. By decoupling the multidimensional coupling effects of network topology, strategy update rule and payoff parameters, we establish critical criteria for cooperation evolution. These findings provide a dynamical theoretical basis for illustrating cooperation emergence mechanisms in complex social networks and designing intervention strategies to enhance collective cooperation.","2025-05","2025-11-04 08:56:57","2025-11-04 09:58:48","2025-11-04 08:56:35","1-6","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2996-3850","","/home/paqh/Zotero/storage/VKZ5HZSN/Zhu et al. - 2025 - Evolutionary Dynamics of Cooperation and Extortion on Networks With Fitness-Dependent Rules.pdf","","Dynamical equation; Dynamics; Evolutionary dynamics; Evolutionary game dynamic; Evolutionary games; Game theory; Intelligent agents; Modelling framework; Multi agent systems; Multi-Agent Model; Network topology; Prisoner dilemma game; Regular networks; Social dilemmas; Zero determinant strategy","Analytical models; Couplings; Dynamical equations; Evolutionary game dynamics; Games; Mathematical models; Numerical models; Simulation; Social networking (online); Steady-state","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2025 Joint International Conference on Automation-Intelligence-Safety (ICAIS) & International Symposium on Autonomous Systems (ISAS)","","","","","","","","","","","","","","",""
"NX9WUARR","conferencePaper","2010","Lee, Seung-Hyun; Park, Han-Saem; Cho, Sung-Bae","Empirical analysis of international trade market using evolutionary multi-agent modeling with game theory","IEEE Congress on Evolutionary Computation","","","10.1109/CEC.2010.5586525","https://ieeexplore.ieee.org/document/5586525/","Multi-agent based approach is reasonable and expandable methodology for simulating social phenomenon. In this paper, we simulate a developmental aspect of international trade market using multi-agent based modeling. Evolutionary computation is used to construct adaptive agent model which continuously evolves its trading strategy. Through the experiments, we show that our simulation model effectively reflects tendency of real-world international trade and existing economic research. Particularly, we find interesting result from the analysis of prevalent strategy of agents, and developmental aspects in international trade market.","2010-07","2025-11-04 08:56:57","2025-11-04 09:50:55","2025-11-04 08:56:38","1-7","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/paqh/Zotero/storage/ADMACRJ3/Lee et al. - 2010 - Empirical analysis of international trade market using evolutionary multi-agent modeling with game t.pdf","","","Adaptation model; Biological system modeling; Computational modeling; Economic indicators; International trade; Trade agreements","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IEEE Congress on Evolutionary Computation","","","","","","","","","","","","","","",""
"UE2H6GVG","conferencePaper","1998","Carmel, D.; Markovitch, S.","How to explore your opponent's strategy (almost) optimally","Proceedings International Conference on Multi Agent Systems (Cat. No.98EX160)","","","10.1109/ICMAS.1998.699033","https://ieeexplore.ieee.org/document/699033/","Presents a lookahead-based exploration strategy for a model-based learning agent that enables exploration of the opponent's behavior during interaction in a multi-agent system. Instead of holding one model, the model-based agent maintains a mixed opponent model, a distribution over a set of models that reflects its uncertainty about the opponent's strategy. Every action is evaluated according to its long run contribution to the expected utility and to the knowledge regarding the opponent's strategy. We present an efficient algorithm that returns an almost optimal exploration strategy against a given mixed model, and a learning method for acquiring a mixed model consistent with the opponent's past behavior. We report experimental results in the Iterated Prisoner's Dilemma game that demonstrate the superiority of the lookahead-based exploration strategy over other exploration methods.","1998-07","2025-11-04 08:56:57","2025-11-04 09:58:52","2025-11-04 08:56:44","64-71","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/paqh/Zotero/storage/3UTQXZCB/Carmel and Markovitch - 1998 - How to explore your opponent's strategy (almost) optimally.pdf","","Expected utility; Exploration methods; Exploration strategies; Iterated prisoner's dilemma; Learning methods; Model-based OPC; Modeling-based learning; Multi agent systems; Opponent modeling","Books; Economic forecasting; History; Learning automata; Multiagent systems; Neural networks; Power generation economics; Predictive models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Multi Agent Systems (Cat. No.98EX160)","","","","","","","","","","","","","","",""
"CHMIJD36","journalArticle","2010","Tsang, Jeffrey","The Parametrized Probabilistic Finite-State Transducer Probe Game Player Fingerprint Model","IEEE Transactions on Computational Intelligence and AI in Games","","1943-0698","10.1109/TCIAIG.2010.2062512","https://ieeexplore.ieee.org/document/5535135/","Fingerprinting operators generate functional signatures of game players and are useful for their automated analysis independent of representation or encoding. The theory for a fingerprinting operator which returns the length-weighted probability of a given move pair occurring from playing the investigated agent against a general parametrized probabilistic finite-state transducer (PFT) is developed, applicable to arbitrary iterated games. Results for the distinguishing power of the 1-state opponent model, uniform approximability of fingerprints of arbitrary players, analyticity and Lipschitz continuity of fingerprints for logically possible players, and equicontinuity of the fingerprints of bounded-state probabilistic transducers are derived. Algorithms for the efficient computation of special instances are given; the shortcomings of a previous model, strictly generalized here from a simple projection of the new model, are explained in terms of regularity condition violations, and the extra power and functional niceness of the new fingerprints demonstrated. The 2-state deterministic finite-state transducers (DFTs) are fingerprinted and pairwise distances computed; using this the structure of DFTs in strategy space is elucidated.","2010-09","2025-11-04 08:56:57","2025-11-04 09:53:28","2025-11-04 08:56:50","208-224","","3","2","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/paqh/Zotero/storage/QEEAMQSM/Tsang - 2010 - The Parametrized Probabilistic Finite-State Transducer Probe Game Player Fingerprint Model.pdf","","","Automated analysis; Biological information theory; Biological system modeling; combinatorial mathematics; Computational intelligence; Encoding; Evolution (biology); Evolutionary computation; Fingerprint recognition; game theory; Game theory; Probes; stochastic automata; Transducers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DCT3MXBD","journalArticle","2016","Lippi, Marco","Statistical Relational Learning for Game Theory","IEEE Transactions on Computational Intelligence and AI in Games","","1943-0698","10.1109/TCIAIG.2015.2490279","https://ieeexplore.ieee.org/document/7297840/","In this paper, we motivate the use of models and algorithms from the area of Statistical Relational Learning (SRL) as a framework for the description and the analysis of games. SRL combines the powerful formalism of first-order logic with the capability of probabilistic graphical models in handling uncertainty in data and representing dependencies between random variables: for this reason, SRL models can be effectively used to represent several categories of games, including games with partial information, graphical games and stochastic games. Inference algorithms can be used to approach the opponent modeling problem, as well as to find Nash equilibria or Pareto optimal solutions. Structure learning algorithms can be applied, in order to automatically extract probabilistic logic clauses describing the strategies of an opponent with a high-level, human-interpretable formalism. Experiments conducted using Markov logic networks, one of the most used SRL frameworks, show the potential of the approach.","2016-12","2025-11-04 08:56:57","2025-11-04 09:53:22","2025-11-04 08:56:54","412-425","","4","8","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/paqh/Zotero/storage/XGTLZIB6/Lippi - 2016 - Statistical Relational Learning for Game Theory.pdf","","","Game theory; Games; Graphical models; Inference algorithms; Machine learning; Markov random fields; probabilistic logic; Probabilistic logic","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LS96QFLL","conferencePaper","2023","Lv, Mingze; Liu, Jiaqi; Guo, Bin; Ding, Yasan; Zhang, Yun; Yu, Zhiwen","Inducing Coordination in Multi-Agent Repeated Game through Hierarchical Gifting Policies","2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart Systems (MASS)","","","10.1109/MASS58611.2023.00041","https://ieeexplore.ieee.org/document/10298392/","Coordination, i.e., multiple autonomous agents in a system to achieve a common goal, is critical for distributed systems since it can increase the overall reward among all agents. However, The dynamic environment and selfish agents pose challenges to learning coordination behavior from historical interaction data in a long-term interaction environment. Previous works mostly focus on one-shot or short-term distributed agent interaction environments, which often leads to selfish or lazy behavior in long-term interaction environments, i.e., prioritizing individual optimal strategies over cooperative strategies. This behavior is mainly due to the lack of historical memory or incomplete use of historical interaction data to guide the current interaction strategy. In this paper, we propose a hierarchical peer-rewarding mechanism, hierarchical gifting, that allows each agent to dynamically assign some of their rewards to other agents based on historical interaction data and guide the agents towards more coordinated behavior while ensuring that agents remain selfish and decentralized. Specifically, we first propose an auxiliary opponent modeling task so that agents can infer opponents’ types through historical interaction trajectories. In addition, we design a hierarchical gifting strategy that dynamically changes during execution based on known opponents’ types. We employ a theoretical framework that captures the benefit of hierarchical gifting in converging to the coordinated behavior by characterizing the equilibria’s basins of attraction in a dynamical system. With hierarchical gifting, we demonstrate increased coordinated behavior of different risk, general-sum coordination games to the prosocial equilibrium both via numerical analysis and experiments.","2023-09","2025-11-04 08:56:57","2025-11-04 09:58:54","2025-11-04 08:56:57","279-287","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2155-6814","","/home/paqh/Zotero/storage/XTZ33ETF/Lv et al. - 2023 - Inducing Coordination in Multi-Agent Repeated Game through Hierarchical Gifting Policies.pdf","","Autonomous agents; Coordinated behavior; Coordination; Distributed systems; Dynamic environments; Dynamical systems; Game theory; Hierarchical systems; Interaction environment; Learning systems; Long-term interaction; Multi agent; Multi agent systems; Multi-agent reinforcement learning; Reinforcement learning; Repeated games; Risk assessment; Selfish Agents","Behavioral sciences; Game Theory; Games; Multi-agent Reinforcement Learning; Multi-agent Systems; Numerical analysis; Task analysis; Trajectory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart Systems (MASS)","","","","","","","","","","","","","","",""
"284G37M8","journalArticle","2013","Stone, Peter; Kaminka, Gal A.; Kraus, Sarit; Rosenschein, Jeffrey S.; Agmon, Noa","Teaching and leading an ad hoc teammate: Collaboration without pre-coordination","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2013.07.003","https://www.sciencedirect.com/science/article/pii/S0004370213000696","As autonomous agents proliferate in the real world, both in software and robotic settings, they will increasingly need to band together for cooperative activities with previously unfamiliar teammates. In such ad hoc team settings, team strategies cannot be developed a priori. Rather, an agent must be prepared to cooperate with many types of teammates: it must collaborate without pre-coordination. This article defines two aspects of collaboration in two-player teams, involving either simultaneous or sequential decision making. In both cases, the ad hoc agent is more knowledgeable of the environment, and attempts to influence the behavior of its teammate such that they will attain the optimal possible joint utility.","2013","2025-11-04 09:41:47","2025-11-04 09:41:47","","35-65","","","203","","","","","","","","","","","","","","","","","","","","","-armed bandits; Autonomous agents; Game theory; Multiagent systems; Teamwork","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D6MD5PXE","journalArticle","2010","Gal, Ya'akov; Grosz, Barbara; Kraus, Sarit; Pfeffer, Avi; Shieber, Stuart","Agent decision-making in open mixed networks","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2010.09.002","https://www.sciencedirect.com/science/article/pii/S0004370210001451","Computer systems increasingly carry out tasks in mixed networks, that is in group settings in which they interact both with other computer systems and with people. Participants in these heterogeneous human–computer groups vary in their capabilities, goals, and strategies; they may cooperate, collaborate, or compete. The presence of people in mixed networks raises challenges for the design and the evaluation of decision-making strategies for computer agents. This paper describes several new decision-making models that represent, learn and adapt to various social attributes that influence people's decision-making and presents a novel approach to evaluating such models. It identifies a range of social attributes in an open-network setting that influence people's decision-making and thus affect the performance of computer-agent strategies, and establishes the importance of learning and adaptation to the success of such strategies. The settings vary in the capabilities, goals, and strategies that people bring into their interactions. The studies deploy a configurable system called Colored Trails (CT) that generates a family of games. CT is an abstract, conceptually simple but highly versatile game in which players negotiate and exchange resources to enable them to achieve their individual or group goals. It provides a realistic analogue to multi-agent task domains, while not requiring extensive domain modeling. It is less abstract than payoff matrices, and people exhibit less strategic and more helpful behavior in CT than in the identical payoff matrix decision-making context. By not requiring extensive domain modeling, CT enables agent researchers to focus their attention on strategy design, and it provides an environment in which the influence of social factors can be better isolated and studied.","2010","2025-11-04 09:41:47","2025-11-04 09:41:47","","1460-1480","","18","174","","","","","","","","","","","","","","","","","","","","","Human–Computer decision-making; Negotiation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S5QG792A","journalArticle","2020","Doshi, Prashant; Gmytrasiewicz, Piotr; Durfee, Edmund","Recursively modeling other agents for decision making: A research perspective","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2019.103202","https://www.sciencedirect.com/science/article/pii/S000437021930027X","Individuals exhibit theory of mind, attributing beliefs, intent, and mental states to others as explanations of observed actions. Dennett's intentional stance offers an analogous abstraction for computational agents seeking to understand, explain, or predict others' behaviors. These recognized theories provide a formal basis to ongoing investigations of recursive modeling. We review and situate various frameworks for recursive modeling that have been studied in game- and decision- theories, and have yielded methods useful to AI researchers. Sustained attention given to these frameworks has produced new analyses and methods with an aim toward making recursive modeling practicable. Indeed, we also review some emerging uses and the insights these yielded, which are indicative of pragmatic progress in this area. The significance of these frameworks is that higher-order reasoning is critical to correctly recognizing others' intent or outthinking opponents. Such reasoning has been utilized in academic, business, military, security, and other contexts both to train and inform decision-making agents in organizational and strategic contexts, and also to more realistically predict and best respond to other agents' intent.","2020","2025-11-04 09:41:48","2025-11-04 09:41:48","","103202","","","279","","","","","","","","","","","","","","","","","","","","","Decision theory; Game theory; Hierarchical beliefs; Multiagent systems; Recursive modeling; Theory of mind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LR7LW8VE","journalArticle","2016","Pereira, Gonçalo; Prada, Rui; Santos, Pedro A.","Integrating social power into the decision-making of cognitive agents","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2016.08.003","https://www.sciencedirect.com/science/article/pii/S0004370216300868","Social power is a pervasive feature with acknowledged impact in a multitude of social processes. However, despite its importance, common approaches to social power interactions in multi-agent systems are rather simplistic and lack a full comprehensive view of the processes involved. In this work, we integrated a comprehensive model of social power dynamics into a cognitive agent architecture based on an operationalization of different bases of social power inspired by theoretical background research in social psychology. The model was implemented in an agent framework that was subsequently used to generate the behavior of virtual characters in an interactive virtual environment. We performed a user study to assess users' perceptions of the agents and found evidence supporting both the social power capabilities provided by the model and their value for the creation of believable and interesting scenarios. We expect that these advances and the collected evidence can be used to support the development of agent systems with an enriched capacity for social agent simulation.","2016","2025-11-04 09:41:48","2025-11-04 09:41:48","","1-44","","","241","","","","","","","","","","","","","","","","","","","","","Autonomous agents; Behavior expressiveness; Cognitive architecture; Social intelligence; Social power","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N8SFC4BF","journalArticle","2025","Oswald, Yannick","Artificial Utopia: Simulation and artificially intelligent agents for exploring Utopian and democratized futures","Futures","","0016-3287","https://doi.org/10.1016/j.futures.2025.103695","https://www.sciencedirect.com/science/article/pii/S0016328725001570","Prevailing top-down systems in politics and economics struggle to keep pace with the pressing challenges of the 21st century, such as climate change, social inequality and conflict. Bottom-up democratization and participatory approaches in politics and economics are increasingly seen as promising alternatives to confront and overcome these issues, often with ‘utopian’ overtones, as proponents believe they may dramatically reshape political, social and ecological futures for the better and in contrast to contemporary authoritarian tendencies across various countries. Institutional specifics and the associated collective human behavior or culture remains little understood and debated, however. In this article, I propose a novel research agenda focusing on ‘utopian’ democratization efforts with formal and computational methods as well as with artificial intelligence – I call this agenda ‘Artificial Utopia’. Artificial Utopias provide safe testing grounds for new political ideas and economic policies ‘in-silico’ with reduced risk of negative consequences as compared to testing ideas in real-world contexts. An increasing number of advanced simulation and intelligence methods, that aim at representing human cognition and collective decision-making in more realistic ways, could benefit this process. This includes agent-based modeling, reinforcement learning, large language models and more. I clarify what some of these simulation approaches can contribute to the study of Artificial Utopias with the help of two institutional examples; the citizen assembly and the democratic firm. Finally, I discuss open questions and future research directions related to the broader Artificial Utopia agenda.","2025","2025-11-04 09:41:48","2025-11-04 09:41:48","","103695","","","174","","","","","","","","","","","","","","","","","","","","","Artificial intelligence; Computational simulation; Democratization; Future studies; Political economy; Utopia","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXYVL3TA","journalArticle","1991","Radner, Roy","Dynamic games in organization theory","Journal of Economic Behavior & Organization","","0167-2681","https://doi.org/10.1016/0167-2681(91)90049-4","https://www.sciencedirect.com/science/article/pii/0167268191900494","","1991","2025-11-04 09:41:48","2025-11-04 09:41:48","","217-260","","1","16","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5X5NWWI3","journalArticle","2023","Kenton, Zachary; Kumar, Ramana; Farquhar, Sebastian; Richens, Jonathan; MacDermott, Matt; Everitt, Tom","Discovering agents","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2023.103963","https://www.sciencedirect.com/science/article/pii/S0004370223001091","Causal models of agents have been used to analyse the safety aspects of machine learning systems. But identifying agents is non-trivial – often the causal model is just assumed by the modeller without much justification – and modelling failures can lead to mistakes in the safety analysis. This paper proposes the first formal causal definition of agents – roughly that agents are systems that would adapt their policy if their actions influenced the world in a different way. From this we derive the first causal discovery algorithm for discovering the presence of agents from empirical data, given a set of variables and under certain assumptions. We also provide algorithms for translating between causal models and game-theoretic influence diagrams. We demonstrate our approach by resolving some previous confusions caused by incorrect causal modelling of agents.","2023","2025-11-04 09:41:48","2025-11-04 09:41:48","","103963","","","322","","","","","","","","","","","","","","","","","","","","","Artificial intelligence; Causality; Game theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBJ8EMYL","journalArticle","2015","Juvina, Ion; Lebiere, Christian; Gonzalez, Cleotilde","Modeling trust dynamics in strategic interaction","Journal of Applied Research in Memory and Cognition","","2211-3681","https://doi.org/10.1016/j.jarmac.2014.09.004","https://www.sciencedirect.com/science/article/pii/S2211368114000813","We present a computational cognitive model that explains transfer of learning across two games of strategic interaction – Prisoner's Dilemma and Chicken. We summarize prior research showing that, when these games are played in sequence, the experience acquired in the first game influences the players’ behavior in the second game. The same model accounts for human data in both games. The model explains transfer effects with the aid of a trust mechanism that determines how rewards change depending on the dynamics of the interaction between players. We conclude that factors pertaining to the game or the individual are insufficient to explain the whole range of transfer effects and factors pertaining to the interaction between players should be considered as well.","2015","2025-11-04 09:41:48","2025-11-04 09:41:48","","197-211","","3","4","","","","","","","","","","","","","","","","","","","","","Cognitive modeling; Social Dilemmas; Strategic interaction; Transfer of learning; Trust dynamics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RA5WD3VC","journalArticle","2013","Weerd, Harmen de; Verbrugge, Rineke; Verheij, Bart","How much does it help to know what she knows you know? An agent-based simulation study","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2013.05.004","https://www.sciencedirect.com/science/article/pii/S0004370213000441","In everyday life, people make use of theory of mind by explicitly attributing unobservable mental content such as beliefs, desires, and intentions to others. Humans are known to be able to use this ability recursively. That is, they engage in higher-order theory of mind, and consider what others believe about their own beliefs. In this paper, we use agent-based computational models to investigate the evolution of higher-order theory of mind. We consider higher-order theory of mind across four different competitive games, including repeated single-shot and repeated extensive form games, and determine the advantage of higher-order theory of mind agents over their lower-order theory of mind opponents. Across these four games, we find a common pattern in which first-order and second-order theory of mind agents clearly outperform opponents that are more limited in their ability to make use of theory of mind, while the advantage for deeper recursion to third-order theory of mind is limited in comparison.","2013","2025-11-04 09:41:48","2025-11-04 09:41:48","","67-92","","","199-200","","","","","","","","","","","","","","","","","","","","","Agent-based models; Evolution of theory of mind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRSY3ZVS","journalArticle","2020","Crandall, Jacob W.","When autonomous agents model other agents: An appeal for altered judgment coupled with mouths, ears, and a little more tape","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2019.103219","https://www.sciencedirect.com/science/article/pii/S0004370219300396","Agent modeling has rightfully garnered much attention in the design and study of autonomous agents that interact with other agents. However, despite substantial progress to date, existing agent-modeling methods too often (a) have unrealistic computational requirements and data needs; (b) fail to properly generalize across environments, tasks, and associates; and (c) guide behavior toward inefficient (myopic) solutions. Can these challenges be overcome? Or are they just inherent to a very complex problem? In this reflection, I argue that some of these challenges may be reduced by, first, modeling alternative processes than what is often modeled by existing algorithms and, second, considering more deeply the role of non-binding communication signals. Additionally, I believe that progress in developing autonomous agents that effectively interact with other agents will be enhanced as we develop and utilize a more comprehensive set of measurement tools and benchmarks. I believe that further development of these areas is critical to creating autonomous agents that effectively model and interact with other agents.","2020","2025-11-04 09:41:48","2025-11-04 09:41:48","","103219","","","280","","","","","","","","","","","","","","","","","","","","","Agent modeling; Autonomous agents; Multi-agent systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRMJQMYS","journalArticle","1997","Kraus, Sarit","Negotiation and cooperation in multi-agent environments","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/S0004-3702(97)00025-8","https://www.sciencedirect.com/science/article/pii/S0004370297000258","Automated intelligent agents inhabiting a shared environment must coordinate their activities. Cooperation—not merely coordination—may improve the performance of the individual agents or the overall behavior of the system they form. Research in Distributed Artificial Intelligence (DAI) addresses the problem of designing automated intelligent systems which interact effectively. DAI is not the only field to take on the challenge of understanding cooperation and coordination. There are a variety of other multi-entity environments in which the entities coordinate their activity and cooperate. Among them are groups of people, animals, particles, and computers. We argue that in order to address the challenge of building coordinated and collaborated intelligent agents, it is beneficial to combine AI techniques with methods and techniques from a range of multi-entity fields, such as game theory, operations research, physics and philosophy. To support this claim, we describe some of our projects, where we have successfully taken an interdisciplinary approach. We demonstrate the benefits in applying multi-entity methodologies and show the adaptations, modifications and extensions necessary for solving the DAI problems","1997","2025-11-04 09:41:48","2025-11-04 09:41:48","","79-97","","1","94","","","","","","","","","","","","","","","","","","","","","Cooperation; Distributed Artificial Intelligence; Multi-agent systems; Negotiation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4VMXUSLF","journalArticle","1992","Canning, David","Average behavior in learning models","Journal of Economic Theory","","0022-0531","https://doi.org/10.1016/0022-0531(92)90045-J","https://www.sciencedirect.com/science/article/pii/002205319290045J","We examine a general class of adaptive behavior models in which the distant past has only a weak effect on current actions, and assume that agents sometimes make mistakes, to show that average behavior (averaged over time) converges, with probability one, to a unique limit. Mistakes generate global convergence and are an equilibrium selection device; for small mistake probabilities the equilibrium selected is close to an equilibrium of the model without mistakes. The overlapping generations model, and learning in games with bounded memory, fit into this framework and are examined as examples of the result.","1992","2025-11-04 09:41:48","2025-11-04 09:41:48","","442-472","","2","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZPMAA238","journalArticle","1996","Kraus, Sarit","An overview of incentive contracting","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/0004-3702(95)00059-3","https://www.sciencedirect.com/science/article/pii/0004370295000593","Agents may contract some of their tasks to other agents even when they do not share a common goal. An agent may try to contract some of the tasks that it cannot perform by itself, or that may be performed more efficiently by other agents. One self-motivated agent may convince another self-motivated agent to help it with its task, by promises of rewards, even if the agents are not assumed to be benevolent. We propose techniques that provide efficient ways for agents to make incentive contracts in varied situations: when agents have full information about the environment and each other, or when agents do not know the exact state of the world. We consider situations of repeated encounters, cases of asymmetric information, situations where the agents lack information about each other, and cases where an agent subcontracts a task to a group of agents. Situations in which there is competition among possible contractor agents or possible manager agents are also considered. In all situations we assume that the contractor can choose a level of effort when carrying out the task and we would like the contractor to carry out the task efficiently without the need of close observation by the manager.","1996","2025-11-04 09:41:48","2025-11-04 09:41:48","","297-346","","2","83","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A77T3Q9S","journalArticle","2025","Gómez, Alejandra López de Aberasturi; Sierra, Carles; Sabater-Mir, Jordi","Grounded predictions of teamwork as a one-shot game: A multiagent multi-armed bandits approach","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2025.104307","https://www.sciencedirect.com/science/article/pii/S0004370225000268","Humans possess innate collaborative capacities. However, effective teamwork often remains challenging. This study delves into the feasibility of collaboration within teams of rational, self-interested agents who engage in teamwork without the obligation to contribute. Drawing from psychological and game theoretical frameworks, we formalise teamwork as a one-shot aggregative game, integrating insights from Steiner's theory of group productivity. We characterise this novel game's Nash equilibria and propose a multiagent multi-armed bandit system that learns to converge to approximations of such equilibria. Our research contributes value to the areas of game theory and multiagent systems, paving the way for a better understanding of voluntary collaborative dynamics. We examine how team heterogeneity, task typology, and assessment difficulty influence agents' strategies and resulting teamwork outcomes. Finally, we empirically study the behaviour of work teams under incentive systems that defy analytical treatment. Our agents demonstrate human-like behaviour patterns, corroborating findings from social psychology research.","2025","2025-11-04 09:41:49","2025-11-04 09:41:49","","104307","","","341","","","","","","","","","","","","","","","","","","","","","Aggregative games; Cooperative AI; Group productivity theory; Multiagent multi-armed bandits","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5RWB9S2M","journalArticle","2007","Sandholm, Tuomas","Perspectives on multiagent learning","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2007.02.004","https://www.sciencedirect.com/science/article/pii/S0004370207000525","I lay out a slight refinement of Shoham et al.'s taxonomy of agendas that I consider sensible for multiagent learning (MAL) research. It is not intended to be rigid: senseless work can be done within these agendas and additional sensible agendas may arise. Within each agenda, I identify issues and suggest directions. In the computational agenda, direct algorithms are often more efficient, but MAL plays a role especially when the rules of the game are unknown or direct algorithms are not known for the class of games. In the descriptive agenda, more emphasis should be placed on establishing what classes of learning rules actually model learning by multiple humans or animals. Also, the agenda is, in a way, circular. This has a positive side too: it can be used to verify the learning models. In the prescriptive agendas, the desiderata need to be made clear and should guide the design of MAL algorithms. The algorithms need not mimic humans' or animals' learning. I discuss some worthy desiderata; some from the literature do not seem well motivated. The learning problem is interesting both in cooperative and noncooperative settings, but the concerns are quite different. For many, if not most, noncooperative settings, future work should increasingly consider the learning itself strategically. Lower bounds cut across the agendas. They can be derived on the computational complexity and on the number of interactions needed.","2007","2025-11-04 09:41:49","2025-11-04 09:41:49","","382-391","","7","171","","","","","","","","","","","","","","","","","","","","","Game theory; Learning in games; Multiagent learning; Reinforcement learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B5SGFILF","journalArticle","2007","Monderer, Dov; Tennenholtz, Moshe","Learning equilibrium as a generalization of learning to optimize","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2007.01.002","https://www.sciencedirect.com/science/article/pii/S0004370207000069","We argue that learning equilibrium is an appropriate generalization to multi-agent systems of the concept of learning to optimize in single-agent setting. We further define and discuss the concept of weak learning equilibrium.","2007","2025-11-04 09:41:49","2025-11-04 09:41:49","","448-452","","7","171","","","","","","","","","","","","","","","","","","","","","Learning; Learning equilibrium; Machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ALNEEUZ","journalArticle","2007","Chang, Yu-Han","No regrets about no-regret","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2006.12.007","https://www.sciencedirect.com/science/article/pii/S0004370207000045","No-regret is described as one framework that game theorists and computer scientists have converged upon for designing and evaluating multi-agent learning algorithms. However, Shoham, Powers, and Grenager also point out that the framework has serious deficiencies, such as behaving sub-optimally against certain reactive opponents. But all is not lost. With some simple modifications, regret-minimizing algorithms can perform in many of the ways we wish multi-agent learning algorithms to perform, providing safety and adaptability against reactive opponents. We argue that the research community should have no regrets about no-regret methods.","2007","2025-11-04 09:41:49","2025-11-04 09:41:49","","434-439","","7","171","","","","","","","","","","","","","","","","","","","","","Game theory; Multi-agent learning; Regret-minimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6EV27PQ9","journalArticle","2025","Méndez-Naya, Luciano","Delegation and strategic altruism: A theoretical approach","Mathematical Social Sciences","","0165-4896","https://doi.org/10.1016/j.mathsocsci.2025.102465","https://www.sciencedirect.com/science/article/pii/S0165489625000800","In this paper we introduce two refinements of Nash equilibria for extensive form games: the quasi-stable equilibrium and the stable equilibrium. We then introduce the general strategic game with delegates and study new solutions in that context. We apply the new solution concepts to symmetric n-player games in which each player has two strategies. The main conclusion is that, in the prisoner’s dilemma, if the punishment payoff is sufficient, both players obtain the cooperative payoff when they choose strategically altruistic delegates.","2025","2025-11-04 09:41:49","2025-11-04 09:41:49","","102465","","","138","","","","","","","","","","","","","","","","","","","","","Altruism; Delegates; Equilibrium; Games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L7DUN9AN","journalArticle","2023","Di, Changyan; Zhou, Qingguo; Shen, Jun; Wang, Jinqiang; Zhou, Rui; Wang, Tianyi","The coupling effect between the environment and strategies drives the emergence of group cooperation","Chaos, Solitons & Fractals","","0960-0779","https://doi.org/10.1016/j.chaos.2023.114138","https://www.sciencedirect.com/science/article/pii/S0960077923010391","The emergence of cooperation is a central issue in understanding collective behavior and evolution. The eco-evolutionary game model introduces a human–environment coupling mechanism, revealing that the feedback between strategies and the relevant environment is a key element in sustaining long-term cooperation. Previous theoretical studies have observed periodic oscillations between cooperative and defective actions under certain conditions. However, such investigations assume cooperators hold a benefit advantage over defectors, which does not fundamentally illuminate how cooperation emerges. Our paper emphasizes that understanding this issue requires considering inherent human memory characteristics. We refine the eco-evolutionary game model using reinforcement learning, constructing a multi-agent system that couples environment and memory-based decision-making. Comprehensive analyses encompass collective and individual perspectives. Our findings show that with the memory mechanism, oscillations between collective cooperation and defection can still occur, even if defection remains a strict Nash equilibrium. Cooperation emerges from the group’s random exploratory actions in depleted environments, altering the environment’s trends. A positive feedback loop forms among the environment, individual rewards, and actions, stabilizing cooperation as a favorable individual strategy at that point. However, established group cooperation leads individuals seeking optimal behavior to transition from cooperators to defectors through exploration, resulting in cooperation collapse. Subsequently, the memory mechanism reengages, diluting defectors’ expected payoffs and initiating a new round of exploratory behavior within the group. Our results unveil the micro-level mechanisms driving cyclic oscillations, enhancing our understanding of the environment-strategy interplay.","2023","2025-11-04 09:41:49","2025-11-04 09:41:49","","114138","","","176","","","","","","","","","","","","","","","","","","","","","Eco-evolutionary game; Emergence of cooperation; Reinforcement learning; Social dilemma","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6LBK744Y","journalArticle","2016","Albrecht, Stefano V.; Crandall, Jacob W.; Ramamoorthy, Subramanian","Belief and truth in hypothesised behaviours","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2016.02.004","https://www.sciencedirect.com/science/article/pii/S0004370216300236","There is a long history in game theory on the topic of Bayesian or “rational” learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.","2016","2025-11-04 09:41:49","2025-11-04 09:41:49","","63-94","","","235","","","","","","","","","","","","","","","","","","","","","Autonomous agents; Game theory; Multiagent systems; Type-based method","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ANKZ4RPC","journalArticle","2018","Atkinson, Katie; Bench-Capon, Trevor","Taking account of the actions of others in value-based reasoning","Artificial Intelligence","","0004-3702","https://doi.org/10.1016/j.artint.2017.09.002","https://www.sciencedirect.com/science/article/pii/S0004370217301078","Practical reasoning, reasoning about what actions should be chosen, is highly dependent both on the individual values of the agent concerned and on what others choose to do. Hitherto, computational models of value-based argumentation for practical reasoning have required assumptions to be made about the beliefs and preferences of other agents. Here we present a new method for taking the actions of others into account that does not require these assumptions: the only beliefs and preferences considered are those of the agent engaged in the reasoning. Our new formalism draws on utility-based approaches and expresses the reasoning in the form of arguments and objections, to enable full integration with value-based practical reasoning. We illustrate our approach by showing how value-based reasoning is modelled in two scenarios used in experimental economics, the Ultimatum Game and the Prisoner's Dilemma, and we present an evaluation of our approach in terms of these experiments. The evaluation demonstrates that our model is able to reproduce computationally the results of ethnographic experiments, serving as an encouraging validation exercise.","2018","2025-11-04 09:41:49","2025-11-04 09:41:49","","1-20","","","254","","","","","","","","","","","","","","","","","","","","","Argumentation schemes; Expected utility; Practical reasoning; Prisoner's Dilemma; Ultimatum Game; Value-based reasoning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FLNLJISJ","journalArticle","2025","Kahalimoghadam, Masoud; Thompson, Russell G.; Rajabifard, Abbas","An intelligent multi-agent system for last-mile logistics","Transportation Research Part E: Logistics and Transportation Review","","1366-5545","https://doi.org/10.1016/j.tre.2025.104191","https://www.sciencedirect.com/science/article/pii/S1366554525002327","Operational efficiency in last-mile logistics (LML) is often hindered by fluctuating e-commerce demand, unforeseen disruptions, and diverse stakeholders with evolving objectives. This paper aims to evaluate the effectiveness of Physical Internet hubs (PI-hubs) in addressing LML challenges by developing an intelligent multi-agent system (iMAS) that focuses on stakeholders’ interactions. In the iMAS, carriers, shippers, and Physical Internet managers (PI-Managers) are considered learning agents. In this complex scenario, the distribution network (DN) structure is dynamic, transitioning from a single-tier system to a two-tier network when carriers and shippers utilize PI-hubs. Bayesian Q-learning optimizes action selection by balancing exploration and exploitation, while fair reward distribution aligns agent incentives, improving cooperation, stability, and performance in dynamic, multi-agent environments. Simulations involving varying combinations of learning agents are performed. Two delivery vehicle types are also included in the collaborative vehicle routing problem, forming the iMAS environment. The simulation results are compared with the base case where agents do not engage in learning. Findings suggest that when PI-managers engage in learning, there is an increase in the percentage of PI-hub usage and a decrease in total vehicle kilometers traveled (VKT), highlighting the effectiveness of PI-hubs in alleviating the adverse impacts of freight vehicle mobility within metropolitan areas. The impact of the initial PI-hub fee policy on DN efficiency, including PI-hub usage, VKT, carriers’ and shippers’ costs, and PI-Manager profit, is assessed through extensive sensitivity analysis. The iMAS acts as a decision support system enabling policymakers to evaluate various policies and actions, aiding the identification of optimal decisions within the LML framework.","2025","2025-11-04 09:41:49","2025-11-04 09:41:49","","104191","","","200","","","","","","","","","","","","","","","","","","","","","Bayesian reinforcement learning; Distribution network design; Last-mile delivery; Multi-agent system; Nash social welfare; Q-learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJGIQ5SQ","journalArticle","2025","Perera, Isuri; de Nijs, Frits; Garcia, Julián","Learning to cooperate against ensembles of diverse opponents","Neural Computing and Applications","","","10.1007/s00521-024-10511-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214668528&doi=10.1007%2Fs00521-024-10511-9&partnerID=40&md5=10fe010167eb8ddf09ffc6248af3f874","The emergence of cooperation in decentralized multi-agent systems is challenging; naive implementations of learning algorithms typically fail to converge or converge to equilibria without cooperation. Opponent modeling techniques, combined with reinforcement learning, have been successful in promoting cooperation, but face challenges when other agents are plentiful or anonymous. We envision environments in which agents face a sequence of interactions with different and heterogeneous agents. Inspired by models of evolutionary game theory, we introduce RL agents that forgo explicit modeling of others. Instead, they augment their reward signal by considering how to best respond to others assumed to be rational against their own strategy. This technique not only scales well in environments with many agents, but can also outperform opponent modeling techniques across a range of cooperation games. Agents that use the algorithm we propose can successfully maintain and establish cooperation when playing against an ensemble of diverse agents. This finding is robust across different kinds of games and can also be shown not to disadvantage agents in purely competitive interactions. While cooperation in pairwise settings is foundational, interactions across large groups of diverse agents are likely to be the norm in future applications where cooperation is an emergent property of agent design, rather than a design goal at the system level. The algorithm we propose here is a simple and scalable step in this direction. © 2025 Elsevier B.V., All rights reserved.","2025","2025-11-04 09:46:12","2025-11-04 09:58:56","","18835 - 18849","","23","37","","","","","","","","","","","","","","","","","Type: Article","","","","Adversarial machine learning; Best response; Cooperation; Decentralised; Decentralized systems; Evolutionary game theory; Federated learning; Game theory; Iterated prisoner’s dilemma; Learning algorithms; Learning systems; Modelling techniques; Multi agent systems; Multiagent systems (MASs); Opponent models; Population games; Reinforcement learning; Reinforcement learnings","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HUCUNB6E","journalArticle","2025","Li, Kai; Huang, Wenhan; Li, Chenchen; Deng, Xiaotie","Exploiting a No-Regret Opponent in Repeated Zero-Sum Games; 重复零和博弈中对无遗憾对手进行盘剥的研究","Journal of Shanghai Jiaotong University (Science)","","","10.1007/s12204-023-2610-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001082101&doi=10.1007%2Fs12204-023-2610-2&partnerID=40&md5=b91d9d28bdade788c8985552e1a51657","In repeated zero-sum games, instead of constantly playing an equilibrium strategy of the stage game, learning to exploit the opponent given historical interactions could typically obtain a higher utility. However, when playing against a fully adaptive opponent, one would have difficulty identifying the opponent’s adaptive dynamics and further exploiting its potential weakness. In this paper, we study the problem of optimizing against the adaptive opponent who uses no-regret learning. No-regret learning is a classic and widely-used branch of adaptive learning algorithms. We propose a general framework for online modeling no-regret opponents and exploiting their weakness. With this framework, one could approximate the opponent’s no-regret learning dynamics and then develop a response plan to obtain a significant profit based on the inferences of the opponent’s strategies. We employ two system identification architectures, including the recurrent neural network (RNN) and the nonlinear autoregressive exogenous model, and adopt an efficient greedy response plan within the framework. Theoretically, we prove the approximation capability of our RNN architecture at approximating specific no-regret dynamics. Empirically, we demonstrate that during interactions at a low level of non-stationarity, our architectures could approximate the dynamics with a low error, and the derived policies could exploit the no-regret opponent to obtain a decent utility. © 2025 Elsevier B.V., All rights reserved.","2025","2025-11-04 09:46:12","2025-11-04 09:58:49","","385 - 398","","2","30","","","","","","","","","","","","","","","","","Type: Article","","","","A; Dynamical systems; Dynamics; Learning algorithms; Learning systems; Network architecture; No-regret learning; Opponent exploitation; Opponent models; Recurrent neural network; Recurrent neural networks; Religious buildings; Repeated games; Response plans; System-identification; TP 18; Zero-sum game","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FPPN88CV","journalArticle","2023","Freire, Ismael T.; Arsiwalla, X. D.; Puigbò, Jordi Ysard; Verschure, P.","Modeling Theory of Mind in Dyadic Games Using Adaptive Feedback Control","Information (Switzerland)","","","10.3390/info14080441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168770847&doi=10.3390%2Finfo14080441&partnerID=40&md5=87cd85e9f9554ecd20d4f8b610b2302d","A major challenge in cognitive science and AI has been to understand how intelligent autonomous agents might acquire and predict the behavioral and mental states of other agents in the course of complex social interactions. How does such an agent model the goals, beliefs, and actions of other agents it interacts with? What are the computational principles to model a Theory of Mind (ToM)? Deep learning approaches to address these questions fall short of a better understanding of the problem. In part, this is due to the black-box nature of deep networks, wherein computational mechanisms of ToM are not readily revealed. Here, we consider alternative hypotheses seeking to model how the brain might realize a ToM. In particular, we propose embodied and situated agent models based on distributed adaptive control theory to predict the actions of other agents in five different game-theoretic tasks (Harmony Game, Hawk-Dove, Stag Hunt, Prisoner’s Dilemma, and Battle of the Exes). Our multi-layer control models implement top-down predictions from adaptive to reactive layers of control and bottom-up error feedback from reactive to adaptive layers. We test cooperative and competitive strategies among seven different agent models (cooperative, greedy, tit-for-tat, reinforcement-based, rational, predictive, and internal agents). We show that, compared to pure reinforcement-based strategies, probabilistic learning agents modeled on rational, predictive, and internal phenotypes perform better in game-theoretic metrics across tasks. The outlined autonomous multi-agent models might capture systems-level processes underlying a ToM and suggest architectural principles of ToM from a control-theoretic perspective. © 2023 Elsevier B.V., All rights reserved.","2023","2025-11-04 09:46:12","2025-11-04 09:59:04","","","","8","14","","","","","","","","","","","","","","","","","Type: Article","","","","Adaptive control systems; Adaptive feedback control; Agent modeling; Autonomous agents; Behavioral state; Cognitive architectures; Cognitive science; Cognitive systems; Computation theory; Control theory; Deep learning; Feedback; Forecasting; Game theory; Game-theoretic; Intelligent agents; Intelligent autonomous agents; Learning systems; Model theory; Multi agent systems; Reinforcement learning; Reinforcement learnings; Theory of minds","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N358VPGE","journalArticle","2023","Hu, Yudong; Han, Congying; Li, Haoran; Guo, Tiande","Modeling opponent learning in multiagent repeated games","Applied Intelligence","","","10.1007/s10489-022-04249-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144725258&doi=10.1007%2Fs10489-022-04249-x&partnerID=40&md5=b14f22e5ee23ff8a5f6ea09addf8507c","Multiagent reinforcement learning (MARL) has been used extensively in the game environment. One of the main challenges in MARL is that the environment of the agent system is dynamic, and the other agents are also updating their strategies. Therefore, modeling the opponents’ learning process and adopting specific strategies to shape learning is an effective way to obtain better training results. Previous studies such as DRON, LOLA and SOS approximated the opponent’s learning process and gave effective applications. However, these studies modeled only transient changes in opponent strategies and lacked stability in the improvement of equilibrium efficiency. In this article, we design the MOL (modeling opponent learning) method based on the Stackelberg game. We use best response theory to approximate the opponents’ preferences for different actions and explore stable equilibrium with higher rewards. We find that MOL achieves better results in several games with classical structures (the Prisoner’s Dilemma, Stackelberg Leader game and Stag Hunt with 3 players), and in randomly generated bimatrix games. MOL performs well in competitive games played against different opponents and converges to stable points that score above the Nash equilibrium in repeated game environments. The results may provide a reference for the definition of equilibrium in multiagent reinforcement learning systems, and contribute to the design of learning objectives in MARL to avoid local disadvantageous equilibrium and improve general efficiency. © 2023 Elsevier B.V., All rights reserved.","2023","2025-11-04 09:46:12","2025-11-04 09:59:02","","17194 - 17210","","13","53","","","","","","","","","","","","","","","","","Type: Article","","/home/paqh/Zotero/storage/MEDYB3MV/Hu et al. - 2023 - Modeling opponent learning in multiagent repeated games.pdf","","Agent systems; Efficiency; Fertilizers; Game environment; Learning methods; Learning process; Learning systems; Multi agent; Multi agent systems; Multi-agent reinforcement learning; Opponent models; Reinforcement learning; Repeated games; Shape learning; Transient changes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YNZTK9UY","journalArticle","2023","Lu, Peng; Chen, Dianhan; Liu, Weiping","OBTAINING THE COMMON RISK MATRIX FROM TERRORISM ATTACKS SIMULATIONS","Journal of Nonlinear and Convex Analysis","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189699533&partnerID=40&md5=ba6bdbb0a1456a93fdf84adee1110710","Under common risks (mass shooting), the Prisoner’s dilemma domains individual behaviors and social outcomes. Given certain common knowledge, some civilians (heroes) may fight bravely against the shooters. Traditional field experiments cannot be used to estimate this knowledge, because participants know that “this is not real”. Thus, we use computational method to obtain common risk matrix. First, we use agent-based modeling to simulate real target case. Then, we obtain the optimal solution, with the highest matchiness. Based on optimal solution, we explore counterfactuals. Finally, we obtain this common risk matrix, with paired rewards of both civilians and shooters. In game theory, strategies of players and the payoff matrix is theoretically given. In this work, this common risk matrix can be obtained by agent-based simulations. © 2024 Elsevier B.V., All rights reserved.","2023","2025-11-04 09:46:12","2025-11-04 09:59:06","","1869 - 1888","","8","24","","","","","","","","","","","","","","","","","Type: Article","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S7C5WPJW","journalArticle","2020","Elhamer, Zineb; Suzuki, Reiji; Arita, Takaya","The effects of population size and information update rates on the emergent patterns of cooperative clusters in a large-scale social particle swarm model","Artificial Life and Robotics","","","10.1007/s10015-019-00558-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074536816&doi=10.1007%2Fs10015-019-00558-6&partnerID=40&md5=f9805479bd27a789b9c3c146d2816666","We study the impact of network size in the context of interactions within social network services (SNS) on cooperation among its users, as well as that of the speed of information update about other neighbors during interaction, using an enhanced version of a swarm model that uses prisoner’s dilemma as social interaction strategy and that models users’ interactions through kinematics. We focus on the speed of information update about social environments and study the relationships between the resulting patterns of cooperation in different information update rates. We observed the large variations among emerging many cooperative clusters in size, speed, and cooperation rate in the large population. Moreover, cooperation was more promoted when the information update rate was high, in contrast to low update rate where the population converged to a few large clusters with many wandering defectors. © 2020 Elsevier B.V., All rights reserved.","2020","2025-11-04 09:46:12","2025-11-04 09:59:16","","149 - 158","","1","25","","","","","","","","","","","","","","","","","Type: Article","","","","Information updates; Large population; Multi agent systems; Multi-Agent Model; Population sizes; Population statistics; Social environment; Social interactions; Social network service (SNS); Social network services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HKTGN2W","conferencePaper","2019","Wang, Weixun; Wang, Yixi; Hao, Jianye; Taylor, Matthew E.","Achieving cooperation through deep multiagent reinforcement learning in sequential prisoner’s dilemmas","ACM International Conference Proceeding Series","","","10.1145/3356464.3357712","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075428156&doi=10.1145%2F3356464.3357712&partnerID=40&md5=f3900b979cd42f8d14b73dbb5ee134ba","The Iterated Prisoner’s Dilemma has guided research on social dilemmas for decades. However, it distinguishes between only two atomic actions: cooperate and defect. In real-world prisoner’s dilemmas, these choices are temporally extended and different strategies may correspond to sequences of actions, reflecting grades of cooperation. We introduce a Sequential Prisoner’s Dilemma (SPD) game to better capture the aforementioned characteristics. In this work, we propose a deep multiagent reinforcement-learning approach that investigates the evolution of mutual cooperation in SPD games. Our approach consists of two phases. The first phase is offline: it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network. The second phase is online: an agent adaptively selects its policy based on the detected degree of opponent cooperation. The effectiveness of our approach is demonstrated in two representative SPD 2D games: the Apple-Pear game and the Fruit Gathering game. Experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents. © 2022 Elsevier B.V., All rights reserved.","2019","2025-11-04 09:46:12","2025-11-04 09:58:45","","","","","","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Atomic actions; Cooperation degree; Deep learning; Detection networks; Fruits; Multi agent systems; Multi-agent reinforcement learning; Multiagent reinforcement learning approach; Mutual Cooperation; Opponent modeling; Reinforcement learning; Social dilemmas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JNF3SBVQ","journalArticle","2018","Zuckerman, Inon; Cheng, Kan Leung; Nau, Dana S.","Modeling agent’s preferences by its designer’s social value orientation","Journal of Experimental and Theoretical Artificial Intelligence","","","10.1080/0952813X.2018.1430856","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041230161&doi=10.1080%2F0952813X.2018.1430856&partnerID=40&md5=6a7b9acb28517b65ce3b669b2c8a4b0a","Human social preferences have been shown to play an important role in many areas of decision-making. There is evidence from the social science literature that human preferences in interpersonal interactions depend partly on a measurable personality trait called, Social Value Orientation (SVO). Automated agents are often written by humans to serve as their delegates when interacting with other agents. Thus, one might expect an agent’s behaviour to be influenced by the SVO of its human designer. With that in mind, we present the following: first, we explore, discuss and provide a solution to the question of how SVO tests that were designed for humans can be used to evaluate agents’ social preferences. Second, we show that in our example domain there is a medium–high positive correlation between the social preferences of agents and their human designers. Third, we exemplify how the SVO information of the designer can be used to improve the performance of some other agents playing against those agents, and lastly, we develop and exemplify the behavioural signature SVO model which allows us to better predict performances when interactions are repeated and behaviour is adapted. © 2018 Elsevier B.V., All rights reserved.","2018","2025-11-04 09:46:12","2025-11-04 09:58:59","","257 - 277","","2","30","","","","","","","","","","","","","","","","","Type: Article","","","","Agent modelling; Artificial intelligence; Cognitive modelling; Decision making; Repeated games; Social preference; Social value orientations; Software engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VD5UTS5Q","conferencePaper","2018","Foerster, Jakob Nicolaus; Chen, Richard Y.; Al-Shedivat, Maruan; Whiteson, Shimon A.; Abbeel, Pieter; Mordatch, Igor","Learning with opponent-learning awareness","Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055312121&partnerID=40&md5=c0ae874f64b2694b4e74077397821c7d","Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi- agent reinforcement learning, but also can be extended to hierarc hical reinforcement learning, generative adversarial networks and decentralised optimization. In all these settings the presence of mult iple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule inc ludes an additional term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Prel iminary results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma (IPD), while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to infinitely repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents can successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the likelihood ratio policy gradient estimator, making the method suitable for model-free reinforcement learning. This method thus scales to large parameter and input spaces and nonlinear function approximators. We also apply LOLA to a grid world task with an embedded social dilemma using deep recurrent policies and opponent modelling. Again, by explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. © 2018 Elsevier B.V., All rights reserved.","2018","2025-11-04 09:46:13","2025-11-04 09:58:57","","122 - 130","","","1","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Adversarial networks; Autonomous agents; Deep learning; Game theory; Gradient-based method; Independent learning; Intelligent agents; Iterated prisoners' dilemma; Learning algorithms; Multi agent systems; Multi-agent learning; Multi-agent reinforcement learning; Multi-agent setting; Reinforcement learning; Round robin tournaments; Routers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJW87SG8","conferencePaper","2017","Hoegen, Rens; Stratou, Giota; Gratch, Jonathan","Incorporating emotion perception into opponent modeling for social dilemmas","Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046442195&partnerID=40&md5=d56b7d0c768a3bcce731d0c67bc1c503","Many everyday decisions involve a social dilemma: cooperation can enhance joint gains, but also make one vulnerable to exploitation. Emotion and emotional signaling is an important element of how people resolve these dilemmas. With the rise of affective computing, emotion is also an important element of how people resolve these dilemmas with machines. In this article, we learn a predictive model of how people make decisions in an iterative social dilemma. We further show that model accuracy improves by incorpo-rating a player's emotional displays as input to this model, and provide some insight into which emotions influence social decisions. Finally, we show how this model can be used to perform ""social planning"": i.e., to generate a sequence of actions and expressions that achieve social goals (such as maximizing individual rewards). These techniques can be used to enhance machine-understanding of human behavior, as social decision-AIDS, or to drive the actions of virtual and robotic agents. © 2018 Elsevier B.V., All rights reserved.","2017","2025-11-04 09:46:13","2025-11-04 09:58:53","","801 - 809","","","2","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Affective behaviors; Affective Computing; Autonomous agents; Behavioral research; Computation theory; Decision support systems; Game theory; Human computer interaction; Human interactions; Iterative methods; Machine understanding; Multi agent systems; Player modeling; Predictive modeling; Sequence of actions; Social dilemmas; Virtual reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IA48ULUI","conferencePaper","2015","Omoregie, Kester O.; Chiemeke, Stella Chinye; Oduntan, Evelyn B.","Modeling state space search technique for a real world adversarial problem solving","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033569440&partnerID=40&md5=3847d222901cd6845454d38a0b8752b8","In problem solving, there is a search for the appropriate solution. A state space is a problem domain consisting of the start state, the goal state and the operations that will necessitate the various moves from the start state to the goal state. Each move operation takes one away from the start state and closer to the goal state. In this work we have attempted implementing this concept in adversarial problem solving, which is a more complex problem space. We noted that real world adversarial problems vary in their types and complexities, and therefore solving an adversarial problem would depend on the nature of the adversarial problem itself. Specifically, we examined a real world case, “the prisoner’s dilemma” which is a critical, mutually independent, decision making adversarial problem. We combined the idea of the Thagard’s Theory of Explanatory Coherence (TEC) with Bayes’ theorem of conditional probability to construct the model of an opponent that includes the opponent’s model of the agent. A further conversion of the model into a series of state space structures led us into the use of breadth-first search strategy to arrive at our decision goal. © 2017 Elsevier B.V., All rights reserved.","2015","2025-11-04 09:46:13","2025-11-04 09:59:03","","166 - 171","","","1","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Adversarial Problems; Breadth-first search; Conditional probabilities; Cybernetics; Decision making; Evaluation function; Heuristics; Mutually independents; Problem solving; State space search; State space structures","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4IHAMSVU","conferencePaper","2015","Hernandez-Leal, Pablo; de Cote, Enrique Munoz; Sucar, Luis Enrique","Opponent modeling against non-stationary strategies (doctoral consortium)","Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944687686&partnerID=40&md5=f246422c396737a9d40c2235c7cda1df","Most state of the art learning algorithms do not fare well with agents (computer or humans) that change their behaviour in time. This is the case because they usually do not model the other agents' behaviour and instead make some assumptions that for real scenarios are too restrictive. Furthermore, considering that many applications demand different types of agents to work together this should be an important problem to solve. We contribute to the state of the art with opponent modeling algorithms. In particular we proposed 3 approaches for learning against non-stationary opponents in repeated games. Experimentally we tested our approaches on three domains including a real world scenario which consists of bidding in energy markets. © 2015 Elsevier B.V., All rights reserved.","2015","2025-11-04 09:46:13","2025-11-04 09:59:07","","1989 - 1990","","","3","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Algorithms; Autonomous agents; Energy markets; Learning; Learning algorithms; Markov Decision Processes; Markov processes; Multi agent systems; Nonstationary; Opponent modeling; Real-world scenario; Repeated games; State of the art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3LE4LPA","journalArticle","2014","Hernandez-Leal, Pablo; de Cote, Enrique Munoz; Sucar, Luis Enrique","Exploration strategies to detect strategy switches","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944701134&partnerID=40&md5=fc320c2eac155795109719e539fa8d00","Opponent modelling refers to the problem of learning the opponent's strategy. However, for an agent to be successful in learning and interacting against many different types of opponents (e.g. deterministic, strategic or behavioural switching) it should excel at detecting switches consistently, explore the state space efficiently (sample complexity of exploration) and encourage revisiting far-visited state-action pairs (as opposed to single agent exploration). This work proposes a learning algorithm that uses two types of exploration to cope with such uncertainties i) sample complexity of exploration and ii) drift exploration, which is less common. Our contribution is R-max#, an algorithm for fast learning non-stationary opponent strategies in repeated games which efficiently revisits far-visited state-action pairs handling both types of exploration. Experiments in two domains show that R-max# in combination with a switch detection mechanism outperforms the state of the art algorithms against non-stationary opponents. © 2017 Elsevier B.V., All rights reserved.","2014","2025-11-04 09:46:13","2025-11-04 09:58:50","","","","","","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Detection mechanism; Exploration strategies; Intelligent agents; Learning algorithms; Markov Decision Processes; Markov processes; Natural resources exploration; Nonstationary; R-MAX; Repeated games; Sample complexity; State-of-the-art algorithms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QAXEG2YH","journalArticle","2014","Hernandez-Leal, Pablo; de Cote, Enrique Munoz; Sucar, Luis Enrique","Using a priori information for fast learning against non-stationary opponents","Lecture Notes in Computer Science","","","10.1007/978-3-319-12027-0_43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921689491&doi=10.1007%2F978-3-319-12027-0_43&partnerID=40&md5=a2b5b1b9fa2e85c60b0ed2bf6529f85c","For an agent to be successful in interacting against many different and unknown types of opponents it should excel at learning fast a model of the opponent and adapt online to non-stationary (changing) strategies. Recent works have tackled this problem by continuously learning models of the opponent while checking for switches in the opponent strategy. However, these approaches fail to use a priori information which can be useful for a faster detection of the opponent model. Moreover, if an opponent uses only a finite set of strategies, then maintaining a list of those strategies would also provide benefits for future interactions, in case of opponents who return to previous strategies (such as periodic opponents). Our contribution is twofold, first, we propose an algorithm that can use a priori information, in the form of a set of models, in order to promote a faster detection of the opponent model. The second is an algorithm that while learning new models keeps a record of them in case the opponent reuses one of those. Our approach outperforms the state of the art algorithms in the field (in terms of model quality and cumulative rewards) in the domain of the iterated prisoner’s dilemma against a non-stationary opponent that switches among different strategies. © 2015 Elsevier B.V., All rights reserved.","2014","2025-11-04 09:46:13","2025-11-04 09:59:18","","536 - 547","","","8864","","","","","","","","","","","","","","","","","Type: Article","","","","Fast learning; Finite set; Information use; Learning models; Model checking; Model qualities; Nonstationary; Opponent modeling; Priori information; State-of-the-art algorithms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KXWUSWGC","journalArticle","2014","Hernandez-Leal, Pablo; de Cote, Enrique Munoz; Sucar, Luis Enrique","A framework for learning and planning against switching strategies in repeated games","Connection Science","","","10.1080/09540091.2014.885294","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899947927&doi=10.1080%2F09540091.2014.885294&partnerID=40&md5=e325b415d31ab81376f8ed88d702ace4","Intelligent agents, human or artificial, often change their behaviour as they interact with other agents. For an agent to optimise its performance when interacting with such agents, it must be capable of detecting and adapting according to such changes. This work presents an approach on how to effectively deal with non-stationary switching opponents in a repeated game context. Our main contribution is a framework for online learning and planning against opponents that switch strategies. We present how two opponent modelling techniques work within the framework and prove the usefulness of the approach experimentally in the iterated prisoner's dilemma, when the opponent is modelled as an agent that switches between different strategies (e.g. TFT, Pavlov and Bully). The results of both models were compared against each other and against a state-of-the-art non-stationary reinforcement learning technique. Results reflect that our approach obtains competitive results without needing an offline training phase, as opposed to the state-of-the-art techniques. © 2014 Taylor & Francis. © 2014 Elsevier B.V., All rights reserved.","2014","2025-11-04 09:46:13","2025-11-04 09:58:42","","103 - 122","","2","26","","","","","","","","","","","","","","","","","Type: Article","","","","Intelligent agents; Iterated prisoner's dilemma; learning; Modelling techniques; Non-stationary environment; Planning; Reinforcement learning; Reinforcement learning techniques; Repeated games; State-of-the-art techniques; Switching strategies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FPXZGD4N","journalArticle","2013","Chakraborty, Doran; Agmon, Noa; Stone, Peter","Targeted opponent modeling of memory-bounded agents","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944701639&partnerID=40&md5=f77acb2e31ae80ff2d420f6c3308f8bb","In a repeated game, a memory-bounded agent selects its next action by basing its policy on a fixed window of past L plays. Traditionally, approaches that attempt to model memory-bounded agents, do so by modeling them based on the past L joint actions. Since the number of possible L sized joint actions grows exponentially with L, these approaches are restricted to modeling agents with a small L. This paper explores an alternative, more efficient mechanism for modeling memory-bounded agents based on high-level features derived from the past L plays. Called Targeted Opponent Modeler against Memory-Bounded Agents, or Tommba, our approach successfully models memory-bounded agents, in a sample efficient manner, given a priori knowledge of a feature set that includes the correct features. Tommba is fully implemented, with successful empirical results in a couple of challenging surveillance based tasks. © 2017 Elsevier B.V., All rights reserved.","2013","2025-11-04 09:46:13","2025-11-04 09:59:15","","","","","","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Feature sets; High-level features; Intelligent agents; Joint actions; Learning; Model agents; Models; Opponent modeling; Priori knowledge; Repeated games; Software agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IUDZ7GXI","journalArticle","2013","Hernandez-Leal, Pablo; de Cote, Enrique Munoz; Sucar, Luis Enrique","Learning against non-stationary opponents","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899987874&partnerID=40&md5=3780507f1a8f7e5d8016f598df5e9005","In multiagent systems, in order to make the best decisions, each agent has to take into account not only the strategy used by other agents but also how those strategies might change in the future. This is further exacerbated in open environments where strategies cannot be assumed to be rational. This paper studies repeated interactions between an agent and an opponent that changes its strategy over time (it is non-stationary). Our main contribution is a framework for fast learning changing non-stationary strategies. The agent uses decision trees to learn the most up to date opponent's strategy. Then, its learned model is continuously re-evaluated to assess strategy switches. Our method detects such strategy switches by measuring tree similarities. Aside from its fast learning process, decision trees can provide an easy interpretation of the opponent model. We evaluated the proposed approach in the iterated prisoner's dilemma, outperforming state of the art algorithms in predictive accuracy when facing non-stationary strategies. © 2017 Elsevier B.V., All rights reserved.","2013","2025-11-04 09:46:13","2025-11-04 09:58:54","","","","","","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Best decision; Decision trees; Forestry; Game theory; Intelligent agents; Iterated prisoner's dilemma; Markov Decision Processes; Markov processes; Multi agent systems; Open environment; Opponent modeling; Predictive accuracy; State-of-the-art algorithms; Tree similarities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4H2U67DW","conferencePaper","2013","Hernandez-Leal, Pablo; de Cote, Enrique Munoz; Sucar, Luis Enrique","Modeling non-stationary opponents","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899415477&partnerID=40&md5=2bbc95c04efbd575137f7378d2a7f871","This paper studies repeated interactions between an agent and an unknown opponent that changes its strategy over time. We propose a framework for learning switching non-stationary strategies. The approach uses decision trees to learn the most up to date opponent's strategy. Then, the agent's strategy is computed by transforming the tree into a Markov Decision Process (MDP), whose solution dictates the optimal way of playing against the learned strategy. The agent's learnt model is continuously re-evaluated to assess strategy switches. Our method detects such strategy switches by measuring tree similarities, and reveals whether the opponent has changed its strategy and a new model has to be learned. We evaluated the proposed approach in the iterated prisoner's dilemma, outperforming common strategies against stationary and non-stationary opponents. Copyright © 2013, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved. © 2014 Elsevier B.V., All rights reserved.","2013","2025-11-04 09:46:13","2025-11-04 09:59:00","","1135 - 1136","","","2","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Autonomous agents; Common strategy; Decision trees; Forestry; Game theory; Iterated prisoner's dilemma; Markov Decision Processes; Markov processes; Multi agent systems; Nonstationary; Opponent modeling; Tree similarities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UWY6R9TB","conferencePaper","2011","Mescheder, Daniel; Tüyls, Karl; Kaisers, Michael","Opponent Modeling with pomdps","Belgian/Netherlands Artificial Intelligence Conference","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874009652&partnerID=40&md5=7135e0a8f2827eff908d33843df17bd4","Reinforcement Learning techniques such as Q-learning are commonly studied in the context of two-player repeated games. However, Q-learning fails to converge to best response behavior even against simple strategies such as Tit-for-two-Tat. Opponent Modeling (OM) can be used to overcome this problem. This article shows that OM based on Partially Observable Markov Decision Processes (POMDPs) can represent a large class of opponent strategies. A variation of McCallum's Utile Distinction Memory algorithm is presented as a means to compute such a POMDP opponent model. This technique is based on Baum-Welch maximum likelihood estimation and uses a t-test to adjust the number of model states. Experimental results demonstrate that this algorithm can identify the structure of strategies against which pure Q-learning is insufficient. This provides a basis for best response behavior against a larger class of strategies. © 2013 Elsevier B.V., All rights reserved.","2011","2025-11-04 09:46:13","2025-11-04 09:59:09","","","","","","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Algorithms; Artificial intelligence; Baum-Welch; Best response; Maximum likelihood estimation; Model state; Opponent modeling; Opponent models; Partially observable Markov decision process; Q-learning; Reinforcement learning techniques; Repeated games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JYS956N4","conferencePaper","2011","Cheng, Kan Leung; Zuckerman, Inon; Nau, Dana S.; Golbeck, Jennifer A.","The life game: Cognitive strategies for repeated stochastic games","","","","10.1109/PASSAT/SocialCom.2011.62","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856171370&doi=10.1109%2FPASSAT%2FSocialCom.2011.62&partnerID=40&md5=946eb03a86019db2bce719b7cbee0919","Standard models in bio-evolutionary game theory involve repetitions of a single stage game (e.g., the Prisoner's Dilemma or the Stag Hunt); but it is clear that repeatedly playing the same stage game is not an accurate model of most individuals'lives. Rather, individuals'interactions with others correspond to many different kinds of stage games. In this work, we concentrate on discovering behavioral strategies that are successful for the life game, in which the stage game is chosen stochastically at each iteration. We present a cognitive agent model based on Social Value Orientation (SVO) theory. We provide extensive evaluations of our model's performance, both against standard agents from the game theory literature and against a large set of life-game agents written by students in two different countries. Our empirical results suggest that for life-game strategies to be successful in environments with such agents, it is important (i) to be unforgiving with respect to trust behavior and (ii) to use adaptive, fine-grained opponent models of the other agents. © 2011 IEEE. © 2012 Elsevier B.V., All rights reserved.","2011","2025-11-04 09:46:13","2025-11-04 09:59:17","","95 - 102","","","","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Cognitive agents; Cognitive Strategy; Empirical results; Game theory; Non-zero-sum games; Opponent models; Prisoner's Dilemma; Repeated games; Single stage; Social value orientation; Social values; Stage game; Standard model; Stochastic game; Stochastic models; Stochastic systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AUZJKC8R","conferencePaper","2011","Wang, Zhikun; Boularias, Abdeslam; Muelling, Katharina; Peters, Jan","Modeling opponent actions for table-tennis playing robot","Proceedings of the National Conference on Artificial Intelligence","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055054674&partnerID=40&md5=7873b62d6539d54ec3e23d75a1e635e9","Opponent modeling is a critical mechanism in repeated games. It allows a player to adapt its strategy in order to better respond to the presumed preferences of its opponents. We introduce a modeling technique that adaptively balances safety and exploitability. The opponent's strategy is modeled with a set of possible strategies that contains the actual one with high probability. The algorithm is safe as the expected payoff is above the minimax payoff with high probability, and can exploit the opponent's preferences when sufficient observations are obtained. We apply the algorithm to a robot table-tennis setting where the robot player learns to prepare to return a served ball. By modeling the human players, the robot chooses a forehand, backhand or middle preparation pose before they serve. The learned strategies can exploit the opponent's preferences, leading to a higher rate of successful returns. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved. © 2011 Elsevier B.V., All rights reserved.","2011","2025-11-04 09:46:13","2025-11-04 09:59:01","","1828 - 1829","","","2","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Algorithms; Artificial intelligence; High probability; High rate; Human players; Intelligent robots; Learn+; Minimax; Modeling technique; Modelling techniques; Opponent modeling; Opponent models; Repeated games; Robots; Sports; Table-tennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KCGEEM5B","conferencePaper","2011","Wang, Zhikun; Boularias, Abdeslam; Muelling, Katharina; Peters, Jan","Balancing Safety and Exploitability in Opponent Modeling","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989955456&partnerID=40&md5=0ea60bebb7134b7b446861ddb38152bc","Opponent modeling is a critical mechanism in repeated games. It allows a player to adapt its strategy in order to better respond to the presumed preferences of his opponents. We introduce a new modeling technique that adaptively balances exploitability and risk reduction. An opponent's strategy is modeled with a set of possible strategies that contain the actual strategy with a high probability. The algorithm is safe as the expected payoff is above the minimax payoff with a high probability, and can exploit the opponents' preferences when sufficient observations have been obtained. We apply them to normal-form games and stochastic games with a finite number of stages. The performance of the proposed approach is first demonstrated on repeated rock-paper-scissors games. Subsequently, the approach is evaluated in a human-robot table-tennis setting where the robot player learns to prepare to return a served ball. By modeling the human players, the robot chooses a forehand, backhand or middle preparation pose before they serve. The learned strategies can exploit the opponent's preferences, leading to a higher rate of successful returns. © 2023 Elsevier B.V., All rights reserved.","2011","2025-11-04 09:46:14","2025-11-04 09:58:47","","1515 - 1520","","","","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Artificial intelligence; Finite number; High probability; Human players; Minimax; Modeling technique; Modelling techniques; Normal form games; Opponent modeling; Opponent models; Performance; Repeated games; Risk reductions; Risks reduction; Robots; Stochastic game; Stochastic systems; Table-tennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JTCYYCUV","journalArticle","2011","Wunder, Michael; Yaros, John Robert; Kaisers, Michael; Littman, Michael L.","Using iterated reasoning to predict opponent strategies","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899434410&partnerID=40&md5=785874528961efd8bd45cfa3239b487a","The field of multiagent decision making is extending its tools from classical game theory by embracing reinforcement learning, statistical analysis, and opponent modeling. For example, behavioral economists conclude from experimental results that people act according to levels of reasoning that form a ""cognitive hierarchy"" of strategies, rather than merely following the hyper-rational Nash equilibrium solution concept. This paper expands this model of the iterative reasoning process by widening the notion of a level within the hierarchy from one single strategy to a distribution over strategies, leading to a more general framework of multi- Agent decision making. It provides a measure of sophistication for strategies and can serve as a guide for designing good strategies for multiagent games, drawing it's main strength from predicting opponent strategies. We apply these lessons to the recently introduced Lemonade-stand Game, a simple setting that includes both collaborative and competitive elements, where an agent's score is critically dependent on its responsiveness to opponent behavior. The opening moves are significant to the end result and simple heuristics have achieved faster cooperation than intricate learning schemes. Using results from the past two real-world tournaments, we show how the submitted entries fit naturally into our model and explain why the top agents were successful. Copyright © 2011, International Foundation for Autonomous Agents and Multiagent Systems. © 2014 Elsevier B.V., All rights reserved.","2011","2025-11-04 09:46:14","2025-11-04 09:59:19","","553 - 560","","","1","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Autonomous agents; Beverages; Classical game theory; Cognitive model; Decision making; Iterated reasoning; Multi agent systems; Multi-agent decision making; Multi-agent games; POMDPs; Reasoning process; Reinforcement learning; Repeated games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2XA2HXTE","journalArticle","2010","Chakraborty, Doran; Stone, Peter","Convergence, targeted optimality, and safety in multiagent learning","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881579266&partnerID=40&md5=a6d0ad3ccdf10e2a30dd57709312b8c4","This paper introduces a novel multiagent learning algorithm which achieves convergence, targeted optimality against memory bounded adversaries, and safety, in arbitrary repeated games. Called CMLeS, its most novel aspect is the manner in which it guarantees (in a PAC sense) targeted optimality against memory-bounded adversaries, via efficient exploration and exploitation. CMLeS is fully implemented and we present empirical results demonstrating its effectiveness. © 2013 Elsevier B.V., All rights reserved.","2010","2025-11-04 09:46:14","2025-11-04 09:58:48","","38 - 44","","","","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Autonomous agents; Exploration and exploitation; Intelligent agents; Multi agent systems; Multi-agent learning; Multiagent learning algorithm; Opponent modeling; Optimality; Optimization; Repeated games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQD48RQE","journalArticle","2010","Al-Haddad, Rawad N.; Sukthankar, Gita R.","A psychologically-inspired agent for iterative Prisoner's dilemma","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957859219&partnerID=40&md5=24efe4d1036088610a5f32ccf9cd867f","In this paper, a psychologically-inspired model for an Iterative Prisoner's dilemma (IPD) agent is proposed. This model is inspired by the ""psychic apparatus"" theory that was developed by Sigmund Freud in 1940. The model captures an agent with a true ""character"" by concurrently supporting the three constructs of personality: the Super-Ego, which represents the ideal part of the agent that always tries to elicit cooperation from opponents, the Id, which is characterized by its willingness to defect all the time to achieve instant gratification, and the Ego, which is the intelligent, realistic, part of the agent that relies on opponent-modeling techniques to decide on the best next move. These three constructs compete against each other in order to take control of the agent. This model was successfully prototyped and participated in a simulated IPD tournament along with other benchmark strategies. ""FREUD"", as the agent is called, achieved outstanding results in this mini-tournament by winning with a good margin. Our model represents a novel abstraction for IPD agent architecture that is potentially applicable to any decision-making task that requires evaluating the benefit of competitive vs. cooperative behavior. Copyright © 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. © 2010 Elsevier B.V., All rights reserved.","2010","2025-11-04 09:46:14","2025-11-04 09:58:44","","2 - 7","","","","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Agent architectures; Artificial intelligence; Co-operative behaviors; Computer simulation; Decision making; Instant gratification; Iterative prisoner's dilemmas; Modeling technique","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UI95UNR5","journalArticle","2010","Zeng, Yifeng; Doshi, Prashant J.","Model identification in interactive influence diagrams using mutual information","Web Intelligence and Agent Systems","","","10.3233/WIA-2010-0194","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954925198&doi=10.3233%2FWIA-2010-0194&partnerID=40&md5=9400c824361e9e0e4712bce0dacf65ed","Interactive influence diagrams (I-IDs) offer a transparent and intuitive representation for the decision-making problem in multiagent settings. They ascribe procedural models such as influence diagrams and I-IDs to model the behavior of other agents. Procedural models offer the benefit of understanding how others arrive at their behaviors. Accurate behavioral models of others facilitate optimal decision-making in multiagent settings. However, identifying the true models of other agents is a challenging task. Given the assumption that the true model of the other agent lies within the set of models that we consider, we may utilize standard Bayesian learning to update the likelihood of each model given the observation histories of others' actions. However, as model spaces are often bounded, the true models of others may not be present in the model space. We then seek to identify models that are relevant to the observed behaviors of others and show how the agent may learn to identify these models. We evaluate the performance of our method on three repeated games and provide theoretical and empirical results in support. © 2010 - IOS Press and the authors. All rights reserved. © 2010 Elsevier B.V., All rights reserved.","2010","2025-11-04 09:46:14","2025-11-04 09:58:58","","313 - 327","","3","8","","","","","","","","","","","","","","","","","Type: Article","","","","Bayesian learning; Bayesian networks; Behavioral model; Decision making; Decision-making problem; Empirical results; Graphic methods; Influence diagram; Model identification; Model spaces; Multi-agent setting; Mutual informations; Opponent modeling; Optimal decisions; Repeated games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJVMJCKH","journalArticle","2010","Silverman, Barry G.","Systems social seience: A design inquiry approach for stabilization and reconstruction of social systems","Intelligent Decision Technologies","","","10.3233/IDT-2010-0069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013602825&doi=10.3233%2FIDT-2010-0069&partnerID=40&md5=a8bbd4e8c3b06f97bfcbf1af4731c547","This paper explores novel approaches under the design inquiry paradigm that promise to help organizations better understand and solve socio-technical dilemmas. Design inquiry is contrasted with scientific inquiry (Section 1). Section 2 presents a meso-scale model of models methodology for design inquiry that synthesizes systems science, agent modeling and simulation, knowledge management architectures, and domain theories and knowledge. The goal is to focus computational science on exploring underlying mechanisms (white box modeling) and to support reflective theorizing and discourse to explain social dilemmas and potential resolutions. Section 3 then describes an evolving agent modeling and simulation testbed while Section 4 offers two gameworld applications that implement this approach and that serve as an example of the new types of instruments useful for systems social science. The conclusions wrapup by reviewing lessons learned about 10 criteria that have guided this research. © 2010 - IOS Press and the authors. © 2017 Elsevier B.V., All rights reserved.","2010","2025-11-04 09:46:14","2025-11-04 09:59:14","","51 - 74","","1","4","","","","","","","","","","","","","","","","","Type: Article","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2RB7TKZB","journalArticle","2008","Rezaei, Golriz; Kirley, Michael","Heterogeneous payoffs and social diversity in the spatial prisoner's Dilemma game","Lecture Notes in Computer Science","","","10.1007/978-3-540-89694-4_59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449085235&doi=10.1007%2F978-3-540-89694-4_59&partnerID=40&md5=955a7bc354f23f9421ea1c30197cc81c","In this paper, we investigate the role of heterogeneous payoff values and social diversity in a spatial version of the iterated prisoner's dilemma game. Typically, a fixed number of agents play the game over a specified number of rounds. At each time step, the agents receive a fixed reward based on the strategy they have adopted and the corresponding payoff (or reward) matrix. We argue that such restrictions are unlikely to be fulfilled in real-life situations. Subsequently, we introduce additional features into the game. Here, each agent has an additional age attribute that can be used to control the number of iterations of the game an agent actually participates in. We also introduce dynamic payoff values that are correlated with particular agent experience levels. Numerical simulations show that the proposed heterogeneous agent model promotes the evolution of cooperation in some circumstances. © 2008 Springer Berlin Heidelberg. © 2009 Elsevier B.V., All rights reserved.","2008","2025-11-04 09:46:14","2025-11-04 09:58:51","","585 - 594","","","5361 LNAI","","","","","","","","","","","","","","","","","Type: Conference paper","","","","Agent modeling; Agents; Game theory; Numerical simulations; Time stepping","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXR6CZJW","journalArticle","2007","Hingston, Philip; Dyer, Dan; Barone, Luigi C.; French, Tim; Kendall, G. F.","Opponent modelling, evolution, and the iterated prisoner’s dilemma","","","","10.1142/9789812770684_0007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549125758&doi=10.1142%2F9789812770684_0007&partnerID=40&md5=f0735dc9e007e439f7675b90cf1ffc2a","The following sections are included: • Introduction Opponent modelling Modeller, the competition entry Anatomy of the modeller Competition performance • Opponent Modelling Versus Evolution The new experiments • Conclusions • References. © 2021 Elsevier B.V., All rights reserved.","2007","2025-11-04 09:46:14","2025-11-04 09:59:10","","139 - 170","","","","","","","","","","","","","","","","","","","","Type: Book chapter","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2VSE2F8","journalArticle","2005","Burns, Tom R.; Roszkowska, Ewa","Social judgment in multi-agent systems","","","","10.1017/CBO9780511610721.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928279139&doi=10.1017%2FCBO9780511610721.018&partnerID=40&md5=4a10f5941a131eb4f4b934830bed461b","INTRODUCTION Game theory in its several variants can be viewed as a contribution to multi-agent modeling. One relevant development of classical game theory, Generalized Game Theory (GGT), entails its extension and generalization through the formulation of the mathematical theory of rules and rule complexes (Gomolińska, 1999, 2004; Burns & Gomolińska, 1998; Burns & Roszkowska, 2004). Informally speaking, a rule complex is a set consisting of rules and/or other rule complexes. Social theory concepts such as norm, value, belief, role, social relationship, and institution as well as game can be defined in a uniform way in terms of rules and rule complexes. This has led to a number of applications: among others, the formalization of social relationships, roles, and judgment and action modalities (Burns & Gomolińska, 2000; Burns, Gomolińska, & Meeker, 2001; among others); reconceptualization of prisoners' dilemma game and other classical games as socially embedded games (Burns, Gomolińska, & Meeker, 2001; Burns & Roszkowska, 2004); models of societal conflict resolution and regulation (Burns, Caldas, & Roszkowska, 2005; Burns & Roszkowska, 2005); rethinking the Nash equilibrium (Burns & Roszkowska, 2004); fuzzy games and equilibria (Burns & Roszkowska, 2004; Roszkowska & Burns, 2002); sociocognitive analysis (Burns & Gomolińska, 2001; Roszkowska & Burns, 2002); simulation studies in whichGGTis applied, for instance, in the formulation of multi-agent simulation models of regulatory processes (Burns, Caldas, & Roszkowska, 2005). In the GGT approach, a well-specified game at time t, G t), is a particular multi-agent interaction situation where the participating actors typically have defined roles and role relationships. Most modern social systems of interest can be characterized in thisway. That is, there are already pre-existing institutional arrangements or social structures shaping and regulating interaction (see Figure 17.2). © 2015 Elsevier B.V., All rights reserved.","2005","2025-11-04 09:46:14","2025-11-04 09:59:12","","409 - 416","","","","","","","","","","","","","","","","","","","","Type: Book chapter","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"77HLPFIQ","journalArticle","1999","Carmel, David; Markovitch, Shaul","Exploration Strategies for Model-based Learning in Multi-agent Systems","Autonomous Agents and Multi-Agent Systems","","","10.1023/A:1010007108196","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033423368&doi=10.1023%2FA%3A1010007108196&partnerID=40&md5=cba0d7bfe148d510c584cd80b3f6e343","An agent that interacts with other agents in multi-agent systems can benefit significantly from adapting to the others. When performing active learning, every agent's action affects the interaction process in two ways: The effect on the expected reward according to the current knowledge held by the agent, and the effect on the acquired knowledge, and hence, on future rewards expected to be received. The agent must therefore make a tradeoff between the wish to exploit its current knowledge, and the wish to explore other alternatives, to improve its knowledge for better decisions in the future. The goal of this work is to develop exploration strategies for a model-based learning agent to handle its encounters with other agents in a common environment. We first show how to incorporate exploration methods usually used in reinforcement learning into model-based learning. We then demonstrate the risk involved in exploration - an exploratory action taken by the agent can yield a better model of the other agent but also carries the risk of putting the agent into a much worse position. We present the lookahead-based exploration strategy that evaluates actions according to their expected utility, their expected contribution to the acquired knowledge, and the risk they carry. Instead of holding one model, the agent maintains a mixed opponent model, a belief distribution over a set of models that reflects its uncertainty about the opponent's strategy. Every action is evaluated according to its long run contribution to the expected utility and to the knowledge regarding the opponent's strategy. Risky actions are more likely to be detected by considering their expected outcome according to the alternative models of the opponent's behavior. We present an efficient algorithm that returns an almost optimal exploration plan against the mixed model and provide a proof of its correctness and an analysis of its complexity. We report experimental results in the Iterated Prisoner's Dilemma domain, comparing the capabilities of the different exploration strategies. The experiments demonstrate the superiority of lookahead-based exploration over other exploration methods. © 2018 Elsevier B.V., All rights reserved.","1999","2025-11-04 09:46:15","2025-11-04 09:58:50","","141 - 172","","2","2","","","","","","","","","","","","","","","","","Type: Article","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AG733UHK","journalArticle","2025","Macmillan-Scott, Olivia; Musolesi, Mirco","(Ir)rationality in AI: state of the art, research challenges and open questions","Artificial Intelligence Review","","1573-7462","10.1007/s10462-025-11341-4","https://link.springer.com/10.1007/s10462-025-11341-4","Abstract             The concept of rationality is central to the field of artificial intelligence (AI). Whether we are seeking to simulate human reasoning, or trying to achieve bounded optimality, our goal is generally to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in AI, and sets out the open questions in this area. We consider how the understanding of rationality in other fields has influenced its conception within AI, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we examine irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area remains limited. Methods that have up to now been developed for other purposes, namely adversarial scenarios, may be adapted to suit interactions with artificial agents. We further discuss the interplay between human and artificial agents, and the role that rationality plays within this interaction; many questions remain in this area, relating to potentially irrational behaviour of both humans and artificial agents.","2025-08-22","2025-11-04 09:48:47","2025-11-04 09:48:47","2025-11-04 09:48:46","352","","11","58","","Artif Intell Rev","(Ir)rationality in AI","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"45TH49YE","journalArticle","2025","Jin, Yue; Wei, Shuangqing; Montana, Giovanni","Achieving collective welfare in multi-agent reinforcement learning via suggestion sharing","Machine Learning","","0885-6125, 1573-0565","10.1007/s10994-025-06823-z","https://link.springer.com/10.1007/s10994-025-06823-z","Abstract             In human society, the conflict between self-interest and collective well-being often obstructs efforts to achieve shared welfare. Related concepts like the Tragedy of the Commons and Social Dilemmas frequently manifest in our daily lives. As artificial agents increasingly serve as autonomous proxies for humans, we propose a novel multi-agent reinforcement learning (MARL) method to address this issue - learning policies to maximise collective returns even when individual agents’ interests conflict with the collective one. Unlike traditional cooperative MARL solutions that involve sharing rewards, values, and policies or designing intrinsic rewards to encourage agents to learn collectively optimal policies, we propose a novel MARL approach where agents exchange action suggestions. Our method reveals less private information compared to sharing rewards, values, or policies, while enabling effective cooperation without the need to design intrinsic rewards. Our algorithm is supported by our theoretical analysis that establishes a bound on the discrepancy between collective and individual objectives, demonstrating how sharing suggestions can align agents’ behaviours with the collective objective. Experimental results demonstrate that our algorithm performs competitively with baselines that rely on value or policy sharing or intrinsic rewards.","2025-08","2025-11-04 09:48:47","2025-11-04 09:48:47","2025-11-04 09:48:46","190","","8","114","","Mach Learn","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6SK32GV","journalArticle","2022","De Weerd, Harmen; Verbrugge, Rineke; Verheij, Bart","Higher-order theory of mind is especially useful in unpredictable negotiations","Autonomous Agents and Multi-Agent Systems","","1387-2532, 1573-7454","10.1007/s10458-022-09558-6","https://link.springer.com/10.1007/s10458-022-09558-6","Abstract                            In social interactions, people often reason about the beliefs, goals and intentions of others. This               theory of mind               allows them to interpret the behavior of others, and predict how they will behave in the future. People can also use this ability recursively: they use               higher-order theory of mind               to reason about the theory of mind abilities of others, as in “he thinks that I don’t know that he sent me an anonymous letter”. Previous agent-based modeling research has shown that the usefulness of higher-order theory of mind reasoning can be useful across competitive, cooperative, and mixed-motive settings. In this paper, we cast a new light on these results by investigating how the predictability of the environment influences the effectiveness of higher-order theory of mind. Our results show that the benefit of (higher-order) theory of mind reasoning is strongly dependent on the predictability of the environment. We consider agent-based simulations in repeated one-shot negotiations in a particular negotiation setting known as Colored Trails. When this environment is highly predictable, agents obtain little benefit from theory of mind reasoning. However, if the environment has more observable features that change over time, agents without the ability to use theory of mind experience more difficulties predicting the behavior of others accurately. This in turn allows theory of mind agents to obtain higher scores in these more dynamic environments. These results suggest that the human-specific ability for higher-order theory of mind reasoning may have evolved to allow us to survive in more complex and unpredictable environments.","2022-10","2025-11-04 09:48:47","2025-11-04 09:48:47","2025-11-04 09:48:46","30","","2","36","","Auton Agent Multi-Agent Syst","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/home/paqh/Zotero/storage/M6GLR97I/De Weerd et al. - 2022 - Higher-order theory of mind is especially useful in unpredictable negotiations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BW3FA3CB","journalArticle","2017","De Weerd, Harmen; Verbrugge, Rineke; Verheij, Bart","Negotiating with other minds: the role of recursive theory of mind in negotiation with incomplete information","Autonomous Agents and Multi-Agent Systems","","1387-2532, 1573-7454","10.1007/s10458-015-9317-1","http://link.springer.com/10.1007/s10458-015-9317-1","","2017-03","2025-11-04 09:48:47","2025-11-04 09:48:47","2025-11-04 09:48:46","250-287","","2","31","","Auton Agent Multi-Agent Syst","Negotiating with other minds","","","","","","","en","","","","","DOI.org (Crossref)","","","","/home/paqh/Zotero/storage/VVE2XCM5/De Weerd et al. - 2017 - Negotiating with other minds the role of recursive theory of mind in negotiation with incomplete in.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""