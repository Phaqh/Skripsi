\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{tikz}
\usepackage{parskip}
\usepackage{forest}
\usepackage{cuted}
\usepackage{fvextra}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{tabularx}
 

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
% \usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\forestset{
  mynode/.style={
    draw,
    rounded corners,
    align=center,
    inner sep=4pt
  },
  header/.style={
    text width=2.8cm,
    inner sep=2pt,
    font=\bfseries
  },
  citation/.style={
    text width=2.8cm,
    inner sep=2pt,
    font=\footnotesize
  }
}


\begin{document}

\title{Opponent Modelling in Repeated Strategic Interaction: Assumptions, Methods, and Evaluation Practices}

\author{\IEEEauthorblockN{Faqih Mahardika}
\IEEEauthorblockA{\textit{Department of Computer Science and Electronics} \\
\textit{Gadjah Mada University} \\
Yogyakarta, Indonesia \\
fm.faqihmahardika@gmail.com}
}

\maketitle

\begin{abstract}
Cooperation and conflict are fundamental features of interaction in human societies, biological systems, and artificial-agent environments, where agents repeatedly face strategic trade-offs between short-term self-interest and long-term collective outcomes. Opponent modelling plays a central role in such settings by enabling agents to infer, anticipate, and adapt to the behavior of others, with applications ranging from negotiation and market interactions to multi-agent artificial intelligence systems. This literature review synthesizes prior work on opponent modelling in repeated strategic games, using the Iterated Prisoner's Dilemma as a canonical instantiation of repeated social dilemmas rather than as a restrictive domain. The review analyzes existing approaches along three dimensions: assumptions about opponent behavior, opponent-modelling methodologies, and evaluation practices. The surveyed literature exhibits substantial diversity in opponent behavior assumptions, including stationary, reactive, learning-based, and population-mediated opponents, often embedded implicitly within experimental setups. Methodologically, approaches span gradient-based learning, deep reinforcement learning, recursive belief reasoning, evolutionary dynamics, and system identification. Evaluation practices predominantly emphasize outcome-based metrics such as cooperation rate, average payoff, and equilibrium-related measures. While informative for long-horizon performance, these metrics are typically applied under unconstrained interaction horizons, limiting insight into the efficiency and timeliness of opponent identification and adaptation. This review highlights a structural gap in existing evaluations and underscores the need for horizon-aware assessment frameworks that better reflect the constraints of real-world repeated interactions.
\end{abstract}

\begin{IEEEkeywords}
Opponent Modelling, Iterated Prisoner's Dilemma, Online Adaptation, Repeated Games
\end{IEEEkeywords}

\section{Introduction}
Cooperation and conflict are fundamental features of interaction in human societies, biological systems, and artificial-agent environments. Game theory provides a rigorous analytical framework for studying such strategic interdependence, and foundational work on the evolution of cooperation demonstrates how behavioural patterns emerge when agents repeatedly face dilemmas that require balancing self-interest with mutual benefit~\cite{axelrod_evolution_1981, axelrod_evolution_nodate}. Among these models, the Prisoner's Dilemma (PD) has become the canonical representation of situations in which individual rationality conflicts with collective welfare. In the one-shot PD, mutual cooperation yields the highest joint payoff, yet the incentive structure leads rational players to defect, exposing a core tension that recurs in many social, economic, and political contexts.

When the game is repeated as the Iterated Prisoner's Dilemma (IPD), the strategic landscape changes significantly. Repetition enables agents to condition their actions on previous interactions, giving rise to behaviours such as reciprocity, retaliation, forgiveness, and trust-building. Axelrod's influential IPD tournaments demonstrated that simple contingent strategies most notably Tit-for-Tat can foster stable cooperation even in competitive environments, illustrating how long-term interaction and memory contribute to the emergence of cooperative norms without centralised enforcement~\cite{axelrod_evolution_nodate, axelrod_evolution_1981}. These results established IPD as a fundamental tool for analysing adaptive, history-dependent decision-making.

The relevance of IPD dynamics extends across multiple domains. In biology, theories of reciprocal altruism describe how repeated encounters favour contingent cooperation, outlining conditions under which helping behaviour can evolve among self-interested organisms~\cite{trivers_evolution_1971}. Evolutionary game theory formalises these mechanisms through the concept of evolutionarily stable strategies, showing how certain behavioural rules can persist under selective pressures~\cite{smith_evolution_nodate}. In the social sciences, repeated-game frameworks explain human cooperation, norm enforcement, and punishment systems in public-goods settings~\cite{charness_cooperation_2021}. In economics and resource management, problems such as commercial fishing demonstrate how strategic interdependence in shared environments mirrors the structure of repeated dilemmas~\cite{leung_analysis_1976}. Across these fields, the IPD serves as a versatile lens for understanding how cooperation emerges, stabilises, or collapses depending on incentives, information structures, and institutional conditions.

Although classical analyses of the IPD provide essential insights, their simplifying assumptions often diverge from the complexities of real strategic interaction. Real agents either human, biological, or artificial frequently operate under uncertainty, possess limited observability, and adapt their behaviour dynamically. These realities have motivated growing interest in opponent modelling, which equips an agent with the ability to infer an opponent's strategy, behavioural tendencies, or intentions from observed actions. Contemporary surveys highlight a broad spectrum of opponent-modelling techniques across adversarial and multi-agent domains, underscoring their importance for enabling adaptive decision-making in repeated games, negotiation, autonomous systems, and competitive learning environments~\cite{nashed_survey_nodate}. Within the IPD, such capabilities are critical: sustaining cooperation depends on recognising cooperative partners, detecting exploiters, and adapting effectively to non-stationary or deceptive strategies.

Therefore, this study aims to systematically review the existing literature on opponent modelling in the Iterated Prisoner's Dilemma. The review is guided by the following research questions:
\begin{itemize}
    \item \textbf{RQ1:} What opponent behavior assumptions are used in opponent modelling for the Iterated Prisoner's Dilemma?
    \item \textbf{RQ2:} What methodological approaches to opponent modelling have been applied in the Iterated Prisoner's Dilemma?
    \item \textbf{RQ3:} How is the effectiveness of opponent-modelling strategies evaluated in the Iterated Prisoner's Dilemma?
\end{itemize}

To address these questions, Section~2 outlines the Systematic Literature Review (SLR) methodology used to identify, screen, and synthesise relevant studies. Section~3 presents the results and discussion of the review from three analytical perspectives: (i) Opponent behavior assumption comparison, (ii) methodological approach comparison, and (iii) evaluation-based comparison. Section~4 concludes the literature review.

\section{Methodology}
A systematic review will be employed to address the research questions. This method applies a transparent and structured procedure to collect relevant studies, appraise their quality, and integrate their findings in order to answer a well-defined research question~\cite{page_prisma_2021}.

\subsection{Inclusion and Exclusion Criteria}

Studies were included in the review if they satisfied all of the following conditions:

\begin{itemize}
    \item \textbf{Topic relevance:} The study presents a substantive discussion, method, or empirical analysis of opponent modelling, opponent-aware decision making, or explicit reasoning about other agent's strategies in \emph{repeated strategic interaction settings}. This includes, not limited to, the Iterated Prisoner's Dilemma (IPD), repeated general-sum games, repeated zero-sum games, and structurally comparable repeated social dilemmas where agents condition behavior on observed opponent actions.
    
    \item \textbf{Structural comparability:} The game or interaction setting involves repeated encounters, strategic interdependence, and observable opponent actions, enabling adaptation or reasoning over opponent behavior across time. The IPD is treated as a canonical instance within this broader class of repeated strategic games.
    
    \item \textbf{Scholarly quality:} The work is published in a peer-reviewed journal or international conference proceedings.
    
    \item \textbf{Publication window:} The study was published between 2019 and 2025. This window was selected to capture contemporary opponent-modelling approaches developed in the modern learning-based multi-agent systems era.
    
    \item \textbf{Accessibility:} The full text is available in English and accessible for analysis.
\end{itemize}

Studies were excluded if they met any of the following conditions:

\begin{itemize}
    \item \textbf{Irrelevant focus:} The study does not address opponent modelling, opponent-aware adaptation, or reasoning about other agents, or without repeated interaction settings.
    
    \item \textbf{Generic multi-agent learning:} The study applies multi-agent reinforcement learning or learning-in-games techniques without incorporating opponent-aware adaptation, either explicitly or implicitly.

    \item \textbf{Purely theoretical without opponent reasoning:} The paper provides only abstract or normative theoretical analysis and does not propose, analyze, or operationalize any mechanism for opponent modelling, opponent inference, or opponent-aware decision making in repeated interaction settings.
    
    \item \textbf{Outside the publication window:} The study was published outside the defined six-year publication window.
    
    \item \textbf{Non-English or inaccessible:} The full text is not available in English or cannot be accessed.
\end{itemize}


\subsection{Information Sources and Search Strategy}

The literature search was conducted using four major academic databases: IEEE Xplore, ScienceDirect, Scopus, and SpringerLink.  The search strategy prioritised canonical repeated dilemma formulations and opponent-aware work to ensure conceptual focus. Broader multi-agent learning literature was considered where opponent differentiation was present, but not exhaustively reviewed. The following query was applied across databases: 


\begin{center}
\colorbox{gray!15}{%
\begin{minipage}{0.9\linewidth}
\centering
\ttfamily
("opponent modelling" OR "opponent modeling" OR "opponent model" OR "agent modeling")\\
AND\\
("prisoner's dilemma" OR "iterated prisoner's dilemma" OR "IPD" OR "repeated game" OR "social dilemma")
\end{minipage}}
\end{center}



Database-specific filters were applied to ensure the relevance and accessibility of retrieved publications. 
\begin{enumerate}
    \item \textbf{IEEE Xplore} (04--11--2025): Search applied to all fields.
    \item \textbf{ScienceDirect} (04--11--2025): Search applied to all fields; restricted to Open Access publications.
    \item \textbf{Scopus} (04--11--2025): Search applied to all fields.
    \item \textbf{SpringerLink} (04--11--2025): Search applied to all fields; restricted to Open Access publications.
\end{enumerate}

\subsection{Research Procedure}
The literature search was conducted following the defined search strategy, with keywords adapted to the specific query capabilities of each database. All identified records were collected and managed using a reference manager to remove duplicates and ineligible entries during the initial screening. The remaining studies were then screened based on their titles and abstracts to determine relevance. Full-text versions of studies that passed this stage were retrieved and assessed for eligibility as part of the systematic review.

Data extraction focused on gathering information relevant to Opponent Modelling in the Iterated Prisoner's Dilemma, including environmental settings, modelling approaches, and evaluation metrics. The extracted data were subsequently analyzed and compared to identify patterns of the different approaches. These findings are then used to address the research questions outlined for this review.

\section{Results}

At each stage of the selection process illustrated in Figure ~\ref{fig:prisma}, n denotes the number of studies considered. During the identification phase, 10 records were removed as duplicates based on their titles and abstracts. An additional 48 records were excluded for failing to meet the inclusion criteria. At the screening stage, 26 papers were removed because they were not relevant to this study (for example, they did not involve opponent modelling or did not use the IPD as the research context), and 6 papers were excluded because they were non-primary studies, resulting in a total of 32 removals. One record could not be retrieved due to access limitations. Consequently, 20 records were selected for full-text review.

Among the 20 full-text articles obtained, 2 were excluded because they focused on IPD population simulations or did not present any opponent modelling. Another 2 were excluded because they were non-primary studies. Three additional papers were excluded for focusing solely on theoretical aspects of the IPD.

After applying all exclusion criteria, 13 studies were included in the final systematic review. The titles and characteristics of these studies are presented in Table ~\ref{tab:paper_characteristics}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/PrismaENG.png}
    \caption{Flow diagram for study selection.}
    \label{fig:prisma}
\end{figure}

\begin{table}[ht]
\centering
\caption{Characteristics of Included Studies}
\label{tab:paper_characteristics}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{p{0.3cm} p{4.3cm} p{2.8cm}}
\hline
\textbf{Ref.} & \textbf{Title} & \textbf{Characteristics} \\ \hline

\cite{qiao_o2m_2024} &
O2M: Online Opponent Modeling in Online General-Sum Matrix Games &
online opponent-modeling for dynamic general-sum games. \\ \hline

\cite{zhu_evolutionary_2025} &
Evolutionary Dynamics of Cooperation and Extortion on Networks With Fitness-Dependent Rules &
Analyzes zero-determinant strategies. \\ \hline

\cite{lv_inducing_2023} &
Inducing Coordination in Multi-Agent Repeated Game through Hierarchical Gifting Policies &
Proposes a hierarchical gifting mechanism. \\ \hline

\cite{gomez_grounded_2025} &
Grounded predictions of teamwork as a one-shot game: A multiagent multi-armed bandits approach &
Models voluntary teamwork.\\ \hline

\cite{di_coupling_2023} &
The coupling effect between the environment and strategies drives the emergence of group cooperation &
Analyze memory environment coupling. \\ \hline

\cite{perera_learning_2025} &
Learning to cooperate against ensembles of diverse opponents &
Scalable RL approach that fosters cooperation. \\ \hline

\cite{li_exploiting_2025} &
Exploiting a No-Regret Opponent in Repeated Zero-Sum Games &
Framework for exploiting no-regret opponents. \\ \hline

\cite{freire_modeling_2023} &
Modeling Theory of Mind in Dyadic Games Using Adaptive Feedback Control &
Proposes control-theoretic ToM agents. \\ \hline

\cite{hu_modeling_2023} &
Modeling opponent learning in multiagent repeated games &
Introduces a Stackelberg-based method. \\ \hline

\cite{elhamer_effects_2020} &
The effects of population size and information update rates on the emergent patterns of cooperative clusters in a large-scale social particle swarm model &
Faster information updates in SNS networks.\\ \hline

\cite{wang_achieving_2019} &
Achieving cooperation through deep multiagent reinforcement learning in sequential prisoner's dilemmas &
Introduces a sequential PD and a deep RL method.\\ \hline

\cite{jin_achieving_2025} &
Achieving collective welfare in multi-agent reinforcement learning via suggestion sharing &
Introduces a MARL method with sharing action suggestions.\\ \hline

\cite{de_weerd_higher-order_2022} &
Higher-order theory of mind is especially useful in unpredictable negotiations &
Analyze higher-order ToM in unpredictable environments.\\ \hline

\end{tabular}
\end{table}

\section{Synthesis / Discussion}
integrates findings across these perspectives to identify persistent limitations and articulate the research gap. While a broad set of attributes was extracted, only methodologically discriminative dimensions are used for direct comparison; environment and evaluation attributes are summarized separately.

\subsection{What opponent behavior assumptions are used in opponent modelling for the Iterated Prisoner's Dilemma?}
To operationalize \textit{opponent behavior assumptions} across heterogeneous formulations, we summarize observable behavioral properties rather than internal learning mechanisms. The table below provides a descriptive categorization based on whether an opponent's actions (i) depend on environmental conditions, (ii) are mediated by population-level interactions, (iii) respond directly to the focal agent's actions, and (iv) exhibit action divergence under equivalent recent interaction histories. These criteria are intentionally behavior-centric and evaluated at the level of interaction traces, without assuming access to the opponent's internal model, update rules, or optimization objectives.

This categorization is not intended as a formal or exhaustive taxonomy of opponent modeling methods. Instead, it serves as an analytical aid to support comparison across studies that adopt different game settings, learning paradigms, and modeling abstractions. In cases where a paper does not explicitly define opponent assumptions, classifications are derived conservatively from the described experimental setup and observed interaction dynamics.


\begin{table}[ht]
\centering
\caption{Opponent behavior assumption based on observable behavioral dependencies.\\}

\label{tab:opponent-behavior}
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\columnwidth}{cccc X p{2.5cm}}
\toprule
\textbf{Env.} 
& \textbf{Pop.} 
& \textbf{Agent}
& \textbf{Div.}
& \textbf{Opponent Behaviour Category}
& \textbf{Ref.} \\
\midrule
-- & -- & \checkmark & -- & Reactive & \cite{jin_achieving_2025} \\
\addlinespace

-- & \checkmark & -- & -- & Population-Conformist & \cite{gomez_grounded_2025} \\
\addlinespace

-- & \checkmark & \checkmark & -- & Contextual Reactive & \cite{elhamer_effects_2020} \\
\addlinespace

-- & -- & \checkmark & \checkmark & Learning Opponent & \cite{qiao_o2m_2024, lv_inducing_2023, li_exploiting_2025, freire_modeling_2023, hu_modeling_2023, wang_achieving_2019,de_weerd_higher-order_2022} \\
\addlinespace

-- & \checkmark & \checkmark & \checkmark & Population-Contextual Strategic & \cite{perera_learning_2025} \\
\addlinespace

\checkmark & \checkmark & -- & \checkmark & Heterogeneous Collective Behavior & \cite{zhu_evolutionary_2025} \\
\addlinespace

\checkmark & \checkmark & \checkmark & \checkmark & Environment-Conditioned Strategic & \cite{di_coupling_2023} \\
\bottomrule
\end{tabularx}

\begin{flushleft}
\vspace{2pt}
\footnotesize
\noindent\textit{Note:} \\
Env. — behavior varies across environment within the same game;\\
Pop. — behavior is mediated by population-level interactions;\\
Agent — behavior responds directly to the focal agent's actions;\\
Div. — action divergence occurs on equivalent recent interaction histories.\\
Checkmarks indicate the presence of a dependency.
\end{flushleft}

\end{table}
Several consistent patterns emerge from the behavioral categorization in Table \ref{tab:opponent-behavior}. Substantial portion of recent studies model opponents whose actions are explicitly agent-responsive and exhibit action divergence \cite{qiao_o2m_2024, zhu_evolutionary_2025, lv_inducing_2023, di_coupling_2023, perera_learning_2025, li_exploiting_2025, freire_modeling_2023, hu_modeling_2023, wang_achieving_2019, de_weerd_higher-order_2022}, indicating an assumption of learning or strategic adaptation. Reflects a growing research emphasis on robustness against non-stationary and adaptive opponents rather than optimization against fixed strategies.

Moreover, population-mediated behavior appears primarily in studies of evolutionary or networked environments, where opponent actions are shaped indirectly through aggregate dynamics rather than direct bilateral adaptation \cite{zhu_evolutionary_2025, gomez_grounded_2025, di_coupling_2023, perera_learning_2025, elhamer_effects_2020}. These settings often decouple individual agent responsiveness from population-level change, leading to behavioral patterns that differ qualitatively from pairwise learning scenarios. Notably, only a subset of works combine environment-dependent, population-mediated, and agent-responsive behaviors\cite{di_coupling_2023}, suggesting that fully context-conditioned strategic opponents remain comparatively underexplored.

\subsection{What methodological approaches to opponent modelling have been applied in the Iterated Prisoner's Dilemma?}
The distribution of opponent behavior assumptions in Table~\ref{tab:opponent-behavior} reflects a broader methodological shift in opponent modeling research, from controlled and stationary assumptions toward behaviorally rich, adaptive, and heterogeneous opponents. While this characterization highlights the nature of the opponents considered, it does not capture how agents are designed to cope with such complexity. Therefore, in Table~\ref{tab:opponent_modelling_paradigm} reorganize prior work from the perspective of agent-side modelling, categorizing approaches by their dominant opponent modelling paradigm and corresponding learning mechanism.

\begin{table}[ht]
\centering
\caption{Comparison of opponent modelling approaches by dominant modelling paradigm and learning mechanism.}
\label{tab:opponent_modelling_paradigm}
\renewcommand{\arraystretch}{1.25}
\begin{tabularx}{\columnwidth}{p{2.75cm} p{3.5cm} X}
\hline
\textbf{Modelling Paradigm} 
& \textbf{Learning / Update Mechanism} 
& \textbf{Papers} \\
\hline

Reactive reinforcement learning
& Value-based RL (DQN) 
& \cite{lv_inducing_2023} \\

Gradient-based opponent shaping
& Gradient descent / opponent-aware updates 
& \cite{qiao_o2m_2024, hu_modeling_2023, wang_achieving_2019}\\

Recursive belief reasoning
& Bayesian belief update / cognitive hierarchy 
& \cite{freire_modeling_2023, de_weerd_higher-order_2022} \\

Population-based training
& Policy-gradient MARL (population sampling) 
& \cite{perera_learning_2025} \\

Evolutionary population dynamics
& Fitness-based strategy imitation 
& \cite{zhu_evolutionary_2025, di_coupling_2023, elhamer_effects_2020} \\

Bandit-based learning-in-games
& Multi-armed bandit / smooth best response 
& \cite{gomez_grounded_2025} \\

System identification
& Autoregressive model with exogenous inputs (NARX) 
& \cite{li_exploiting_2025} \\

Communication-driven coordination
& Policy-gradient with communication objective 
& \cite{jin_achieving_2025} \\

\hline
\end{tabularx}
\end{table}

Table~\ref{tab:opponent_modelling_paradigm} categorizes prior work by modelling paradigm and learning mechanism, the listed approaches also differ in whether opponent adaptation is handled explicitly or implicitly. Explicit opponent modelling approaches, such as gradient-based opponent shaping~\cite{qiao_o2m_2024, hu_modeling_2023, wang_achieving_2019}, recursive belief reasoning~\cite{freire_modeling_2023, de_weerd_higher-order_2022}, and system identification~\cite{li_exploiting_2025}, construct internal representations of opponent behavior. In contrast, implicit approaches including reactive reinforcement learning~\cite{lv_inducing_2023}, population-based training\cite{perera_learning_2025}, evolutionary dynamics \cite{zhu_evolutionary_2025, di_coupling_2023, elhamer_effects_2020} and Bandit-based learning-in-games \cite{gomez_grounded_2025} adapt without maintaining explicit opponent model. This highlights alternative design philosophies for addressing non-stationarity opponent.

\subsection{How is the effectiveness of opponent-modelling strategies evaluated in the Iterated Prisoner's Dilemma?}
Evaluation choices implicitly define what constitutes success in adaptive multi-agent interaction, such as cooperation outcomes, robustness to exploitation, stability, or predictive accuracy. Table~\ref{tab:evaluation_metrics} provides a descriptive overview of the evaluation environments and metrics used in prior work, with the aim of enabling cross-paper comparison and highlighting recurring evaluation patterns and omissions rather than standardizing or ranking criteria.

\begin{table}[ht]
\centering
\caption{Evaluation environments and metrics in studies}
\label{tab:evaluation_metrics}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\columnwidth}{c >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
\hline
\textbf{Ref.} & \textbf{Evaluation Environment} & \textbf{Metrics} \\
\hline
\cite{qiao_o2m_2024} & Symmetric self-play & MSE during offline training; latent memory accuracy \\

\cite{zhu_evolutionary_2025} & Scale-free network with zero-determinant strategies & Frequency of cooperation (C) and exploitation (E) \\

\cite{lv_inducing_2023} & Adaptive opponents (averaged over multiple opponents) & Reward value \\

\cite{gomez_grounded_2025} & Custom teamwork-game simulator (aggregative public good games); synthetic experiments & Aggregate team productivity; $\chi^2$ goodness-of-fit to equilibrium; convergence to Nash equilibrium; individual contribution \\

\cite{di_coupling_2023} & Evolutionary game simulator on structured networks & Cooperation level; fraction of cooperators; phase-transition thresholds \\

\cite{perera_learning_2025} & Repeated matrix games with synthetic opponent populations & Average payoff; cooperation rate; robustness across opponent sets; generalization \\

\cite{li_exploiting_2025} & Repeated zero-sum games against Hedge, OMD, and Regret Matching & Prediction error; cumulative payoff; robustness to non-stationarity \\

\cite{freire_modeling_2023} & Repeated matrix games; embodied continuous-time robotic simulations & Efficacy; stability; prediction accuracy \\

\cite{hu_modeling_2023} & Repeated matrix-game simulations & Average payoff; convergence speed; equilibrium selection \\

\cite{elhamer_effects_2020} & Large-scale continuous-space simulations (FLAME GPU) & Cooperation rate; cooperative cluster size and count; agent speed; cluster stability \\

\cite{wang_achieving_2019} & Custom 2D SPD environments (Fruit Gathering; Apple--Pear games) & Average individual reward; total social welfare; cooperation degree detection accuracy \\

\cite{jin_achieving_2025} & MARL benchmarks (Cleanup, Harvest, Sequential PD, Tragedy of the Commons) & Normalized return; cooperation rate; convergence speed; policy discrepancy (MSE) \\

\cite{de_weerd_higher-order_2022} & Simulated Colored Trails with increasing environmental unpredictability & Allocator score; responder score; total social welfare \\
\hline
\end{tabularx}
\end{table}

Table~\ref{tab:evaluation_metrics} reveals a strong concentration of evaluation practices around aggregate performance outcomes, most notably cooperation rate \cite{di_coupling_2023,elhamer_effects_2020,wang_achieving_2019,jin_achieving_2025}, average payoff \cite{lv_inducing_2023,perera_learning_2025,li_exploiting_2025,wang_achieving_2019,de_weerd_higher-order_2022}, and convergence to equilibrium\cite{gomez_grounded_2025,hu_modeling_2023,jin_achieving_2025}. These metrics are well aligned with long-horizon repeated interaction settings, where early exploratory losses can be amortized over time and stable behavioral patterns eventually emerge.

However, comparatively little attention is given to interaction constraints, such as limited game horizon, finite learning budget, or the opportunity cost incurred during opponent probing. Even when non-stationary or adaptive opponents are considered, evaluation is typically conducted under conditions where prolonged interaction is assumed \cite{qiao_o2m_2024}, and performance is assessed after convergence rather than during the learning phase itself.

As a result, existing evaluation protocols tend to under-represent scenarios in which exploration is costly, miscoordination is irreversible, or early decisions dominate total return. This gap is particularly salient for online opponent modelling, where identifying opponent behavior necessarily requires intervention through action, yet the cost of such intervention is rarely isolated as an evaluation dimension. Consequently, current metrics provide limited insight into how efficiently an agent balances opponent identification against immediate performance under constrained interaction settings.


\section{Conclusion}
This review synthesized the opponent-modelling literature in repeated strategic interaction by examining how opponent behavior is assumed, how it is modelled, and how modelling effectiveness is evaluated, with a primary focus on canonical social dilemma settings commonly instantiated through the Iterated Prisoner’s Dilemma. The analysis reveals substantial diversity in opponent behavior assumptions, ranging from stationary and reactive opponents to adaptive and population-mediated dynamics. While many studies implicitly account for non-stationarity arising from learning dynamics or environmental coupling, these assumptions are frequently embedded within experimental designs rather than stated explicitly. As a result, modelling approaches that appear methodologically similar may operate under fundamentally different opponent behavior assumptions, complicating direct comparison across studies.

Across the reviewed literature, a broad range of modelling methodologies has been employed, reflecting differing views on the appropriate abstraction for representing adaptive opponents. Evaluation practices predominantly emphasize outcome-based metrics such as cooperation rate, average payoff, and equilibrium convergence. These metrics are effective for assessing long-horizon performance and stability, but they are typically applied in settings where interaction horizons are unconstrained or sufficiently long to amortize exploration costs. Consequently, although opponent identification and adaptation are central motivations of opponent modelling, comparatively limited attention is paid to the efficiency and timeliness of such identification under constrained interaction horizons. This observation highlights an opportunity for future research to examine opponent-modelling effectiveness in online settings where interaction opportunities are limited, exploration is costly, and early-stage adaptation is critical.

\begin{thebibliography}{00}

\bibitem{nashed_survey_nodate}
S. B. Nashed and S. Zilberstein, ``A Survey on Opponent Modeling in Adversarial Domains'' unpublished.

\bibitem{axelrod_evolution_nodate}
R. Axelrod, ``The Evolution of Cooperation'' unpublished.

\bibitem{axelrod_evolution_1981}
R. Axelrod and W. D. Hamilton, ``The Evolution of Cooperation'' \textit{Science}, 1981.

\bibitem{trivers_evolution_1971}
R. L. Trivers, ``The Evolution of Reciprocal Altruism'' \textit{The Quarterly Review of Biology}, vol. 46, no. 1, pp. 35--57, 1971.  
Available: https://www.journals.uchicago.edu/doi/10.1086/406755

\bibitem{charness_cooperation_2021}
Y. Chen, ``Cooperation and punishment in public goods experiments (by Ernst Fehr and Simon Gächter),'' in \textit{The Art of Experimental Economics}, G. Charness and M. Pingle, Eds., London: Routledge, 2021, pp. 134--143.  
Available: taylorfrancis.com/books/9781003019121/chapters/10.4324/9781003019121-12

\bibitem{leung_analysis_1976}
A. Leung and A.-Y. Wang, ``Analysis of Models for Commercial Fishing: Mathematical and Economical Aspects,'' \textit{Econometrica}, vol. 44, no. 2, p. 295, 1976.  
Available: https://www.jstor.org/stable/1912725

\bibitem{smith_evolution_nodate}
J. M. Smith, \textit{Evolution and the Theory of Games}. (Publication year not provided.)

\bibitem{page_prisma_2021}
M.~J.~Page, J.~E.~McKenzie, P.~M.~Bossuyt, I.~Boutron, T.~C.~Hoffmann, 
C.~D.~Mulrow, L.~Shamseer, J.~M.~Tetzlaff, E.~A.~Akl, S.~E.~Brennan, 
R.~Chou, J.~Glanville, J.~M.~Grimshaw, A.~Hróbjartsson, M.~M.~Lalu, 
T.~Li, E.~W.~Loder, E.~Mayo-Wilson, S.~McDonald, L.~A.~McGuinness, 
L.~A.~Stewart, J.~Thomas, A.~C.~Tricco, V.~A.~Welch, P.~Whiting, 
and D.~Moher, 
``The PRISMA 2020 statement: an updated guideline for reporting systematic reviews,'' 
\textit{BMJ}, vol.~372, p.~n71, Mar.\ 2021. 

\bibitem{qiao_o2m_2024}
X.~Qiao, C.~Han, and T.~Guo, ``O2M: Online Opponent Modeling in Online General-Sum Matrix Games,'' in \emph{2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)}, Dec. 2024, pp. 358--361. [Online]. Available: https://ieeexplore.ieee.org/document/10899996/. doi: 10.1109/ICAIRC64177.2024.10899996.

\bibitem{zhu_evolutionary_2025}
L.~Zhu, Y.~Zhu, and C.~Xia, ``Evolutionary Dynamics of Cooperation and Extortion on Networks With Fitness-Dependent Rules,'' in \emph{2025 Joint International Conference on Automation-Intelligence-Safety (ICAIS) \& International Symposium on Autonomous Systems (ISAS)}, May 2025, pp. 1--6. [Online]. Available: https://ieeexplore.ieee.org/document/11051564/. doi: 10.1109/ICAISISAS64483.2025.11051564.

\bibitem{lv_inducing_2023}
M.~Lv, J.~Liu, B.~Guo, Y.~Ding, Y.~Zhang, and Z.~Yu, ``Inducing Coordination in Multi-Agent Repeated Game through Hierarchical Gifting Policies,'' in \emph{2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart Systems (MASS)}, Sep. 2023, pp. 279--287. [Online]. Available: https://ieeexplore.ieee.org/document/10298392/. doi: 10.1109/MASS58611.2023.00041.

\bibitem{gomez_grounded_2025}
A.~L. de~A. Gómez, C.~Sierra, and J.~Sabater-Mir, ``Grounded predictions of teamwork as a one-shot game: A multiagent multi-armed bandits approach,'' \emph{Artificial Intelligence}, vol.~341, p. 104307, 2025. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0004370225000268. doi: 10.1016/j.artint.2025.104307.

\bibitem{di_coupling_2023}
C. Di, Q. Zhou, J. Shen, J. Wang, R. Zhou, and T. Wang, ``The coupling effect between the environment and strategies drives the emergence of group cooperation,'' \emph{Chaos, Solitons \& Fractals}, vol. 176, 2023, pp. 114138. doi: 10.1016/j.chaos.2023.114138.

\bibitem{perera_learning_2025}
I. Perera, F. de Nijs, and J. Garcia, ``Learning to cooperate against ensembles of diverse opponents,'' \emph{Neural Computing and Applications}, vol. 37, no. 23, 2025, pp. 18835--18849. doi: 10.1007/s00521-024-10511-9.

\bibitem{li_exploiting_2025}
K. Li, W. Huang, C. Li, and X. Deng, ``Exploiting a No-Regret Opponent in Repeated Zero-Sum Games'' \emph{Journal of Shanghai Jiaotong University (Science)}, vol. 30, no. 2, 2025, pp. 385--398. doi: 10.1007/s12204-023-2610-2.

\bibitem{freire_modeling_2023}
I. T. Freire, X. D. Arsiwalla, J. Y. Puigbò, and P. Verschure, ``Modeling Theory of Mind in Dyadic Games Using Adaptive Feedback Control,'' \emph{Information}, vol. 14, no. 8, 2023. doi: 10.3390/info14080441.

\bibitem{hu_modeling_2023}
Y. Hu, C. Han, H. Li, and T. Guo, ``Modeling opponent learning in multiagent repeated games,'' \emph{Applied Intelligence}, vol. 53, no. 13, 2023, pp. 17194--17210. doi: 10.1007/s10489-022-04249-x.

\bibitem{elhamer_effects_2020}
Z. Elhamer, R. Suzuki, and T. Arita, ``The effects of population size and information update rates on the emergent patterns of cooperative clusters in a large-scale social particle swarm model,'' \emph{Artificial Life and Robotics}, vol. 25, no. 1, 2020, pp. 149--158. doi: 10.1007/s10015-019-00558-6.

\bibitem{wang_achieving_2019}
W. Wang, Y. Wang, J. Hao, and M. E. Taylor, ``Achieving cooperation through deep multiagent reinforcement learning in sequential prisoner’s dilemmas,'' in \emph{ACM International Conference Proceeding Series}, 2019. doi: 10.1145/3356464.3357712.

\bibitem{jin_achieving_2025}
Y. Jin, S. Wei, and G. Montana, ``Achieving collective welfare in multi-agent reinforcement learning via suggestion sharing,'' \emph{Machine Learning}, vol. 114, no. 8, 2025, p. 190. doi: 10.1007/s10994-025-06823-z.

\bibitem{de_weerd_higher-order_2022}
H. De Weerd, R. Verbrugge, and B. Verheij, ``Higher-order theory of mind is especially useful in unpredictable negotiations,'' \emph{Autonomous Agents and Multi-Agent Systems}, vol. 36, no. 2, 2022, p. 30. doi: 10.1007/s10458-022-09558-6.



\end{thebibliography}


\end{document}