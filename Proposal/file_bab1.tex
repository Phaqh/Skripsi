\section{Latar Belakang}

Interaksi strategis berulang antara agen rasional merupakan fondasi penting dalam multi-agent reinforcement learning (MARL) (\cite{hernandez-leal_survey_2019}). Salah satu kerangka klasik yang sering digunakan untuk mempelajari dinamika adaptasi strategis adalah \textit{Repeated Iterated Prisoner's Dilemma} (IPD). Dalam permainan ini, dua agen berinteraksi secara berulang tanpa komunikasi eksplisit, dan negosiasi strategi terjadi murni melalui urutan aksi(\cite{axelrod_evolution_1981}).

Pada lingkungan IPD dengan \textit{stochastic termination} (geometric stopping), dinamika menjadi lebih kompleks karena horizon interaksi bersifat tidak pasti (\cite{axelrod_evolution_1981}). Kompleksitas ini meningkat ketika lawan bersifat adaptif dan non-stasioner, misalnya menggunakan algoritma pembelajaran daring seperti \textit{Hedge}, \textit{Online Mirror Descent}, atau pendekatan berbasis reinforcement learning. Dalam konteks ini, agen tidak mengetahui algoritma internal lawan dan harus beradaptasi hanya berdasarkan observasi historis (\cite{nisan_algorithmic_2008}).

Dalam konteks terminasi stokastik dengan distribusi geometrik, horizon efektif interaksi bersifat terbatas secara ekspektasi. Oleh karena itu, pemodelan prediksi jangka pendek menjadi lebih relevan dibandingkan analisis asimtotik jangka panjang, karena keputusan eksplorasi harus mempertimbangkan probabilitas terminasi yang meningkat seiring waktu (\cite{nisan_algorithmic_2008}).

Dalam lingkungan dengan horizon stokastik dan lawan non-stasioner, sinyal reward seringkali menjadi indikator yang terkonflasi antara kualitas kebijakan agen dan perubahan strategi lawan\textbf{(kasih referensi)}. Ketika reward digunakan secara langsung untuk memperbarui model lawan, terjadi risiko pencampuran antara inferensi dinamika lawan dan evaluasi utilitas tindakan. Akibatnya, proses pembelajaran dapat mengalami bias identifikasi dan ketidakstabilan adaptasi \textbf{(kasih referensi)}. Hingga saat ini, belum banyak pendekatan yang secara eksplisit memisahkan pemodelan belief terhadap lawan dari optimisasi reward, sekaligus mengevaluasi perolehan informasi epistemik jangka pendek dalam interaksi strategis berulang dengan terminasi stokastik.


Sebagian besar pendekatan opponent modelling dalam MARL berfokus pada inferensi tipe eksplisit, estimasi utilitas, atau pelatihan bersama berbasis reward. Pendekatan tersebut umumnya:
\begin{enumerate}
    \item Mengintegrasikan belief modelling langsung ke dalam objective reward,
    \item Berorientasi pada prediksi satu langkah ke depan,
    \item Tidak memisahkan modul belief dari optimisasi kebijakan,
    \item Tidak mengevaluasi \textit{epistemic information gain} secara eksplisit.
\end{enumerate}

Akibatnya, eksplorasi terhadap lawan seringkali bersifat implisit dan tidak terstruktur sehingga kurang robust terhadap perubahan algoritma maupun dinamika pembelajaran lawan.\textbf{(kasih referensi)}

Dalam konteks ini, ketidakpastian terhadap dinamika strategi lawan bukan sekadar noise statistik, melainkan komponen epistemik yang secara langsung memengaruhi kualitas pengambilan keputusan. Oleh karena itu, eksplorasi tidak hanya berkaitan dengan perolehan reward yang lebih tinggi, tetapi juga dengan reduksi ketidakpastian terhadap model lawan. Pendekatan yang tidak secara eksplisit mengkuantifikasi aspek epistemik ini berpotensi gagal beradaptasi ketika lawan mengubah kebijakannya secara dinamis.

Berdasarkan permasalahan tersebut, diperlukan suatu kerangka pemodelan yang (i) tidak mengasumsikan struktur parametrik eksplisit dari algoritma lawan, (ii) memisahkan secara konseptual dan komputasional belief modelling dari optimisasi reward, serta (iii) memungkinkan integrasi estimasi perolehan informasi epistemik ke dalam mekanisme seleksi aksi.

Dalam kerangka yang diusulkan, sinyal reward tetap digunakan untuk evaluasi dan seleksi kebijakan, namun tidak digunakan sebagai sinyal pelatihan dalam pembaruan belief terhadap lawan. Dengan demikian, proses inferensi dinamika lawan dipisahkan secara konseptual dari optimisasi utilitas agen.

Dengan pendekatan ini, belief terhadap dinamika lawan tidak hanya berfungsi sebagai model prediktif, tetapi menjadi komponen aktif dalam menentukan strategi eksplorasi dan eksploitasi agen.

Dalam penelitian ini, peningkatan keyakinan (confidence improvement) didefinisikan sebagai perubahan terukur pada distribusi prediktif belief akibat observasi kontrafaktual trajectory aksi agen, yang berfungsi sebagai proksi reduksi ketidakpastian epistemik jangka pendek.

Sebagai ilustrasi, estimasi tersebut dapat diintegrasikan ke dalam skema seleksi aksi berbasis prinsip seperti Upper Confidence Bound (UCB), meskipun perancangan mekanisme seleksi aksi bukan fokus utama penelitian ini.

\section{Rumusan Masalah}

Berdasarkan latar belakang tersebut, penelitian ini merumuskan permasalahan sebagai berikut:

\begin{enumerate}
    \item Bagaimana membangun modul pemodelan lawan yang sepenuhnya observasional dan tidak bergantung pada sinyal reward?
    \item Bagaimana melakukan prediksi multi-langkah jangka pendek ($k$-step forecasting) terhadap distribusi aksi lawan yang dikondisikan pada trajectory aksi agen?
    \item Bagaimana mengestimasi peningkatan keyakinan (\textit{confidence improvement}) yang didefinisikan sebagai perubahan distribusi prediktif belief
    % \item Bagaimana memanfaatkan estimasi peningkatan keyakinan tersebut dalam mekanisme seleksi aksi yang mempertimbangkan keseimbangan antara eksploitasi dan eksplorasi?

\end{enumerate}

\section{Batasan Penelitian}

Agar fokus penelitian tetap terjaga dan implementasi eksperimental stabil, penelitian ini dibatasi pada:

\begin{enumerate}
    \item Lingkungan \textit{Repeated Iterated Prisonerâ€™s Dilemma} dengan \textit{stochastic termination}.
    \item Lawan adaptif yang tidak diketahui algoritmanya, namun terbatas pada kelas pembelajaran daring seperti Hedge, Online Mirror Descent, serta pendekatan reinforcement learning berbasis policy update tanpa observasi struktur eksplisit lawan.
    \item Prediksi jangka pendek ($k$-step forecasting), tanpa pemodelan asimtotik jangka panjang.
    \item Tidak mencakup inferensi eksplisit terhadap struktur algoritma lawan.
    \item Tidak mengoptimalkan mutual information atau objective identifiability struktural secara eksplisit, melainkan menggunakan proksi berbasis perubahan distribusi prediktif belief.
\end{enumerate}

\section{Tujuan Penelitian}

Penelitian ini bertujuan untuk:

\begin{enumerate}
    \item Mengembangkan modul \textit{decoupled latent opponent belief} yang dilatih murni menggunakan forecasting loss tanpa sinyal reward.
    \item Mengimplementasikan mekanisme prediksi multi-langkah jangka pendek terhadap distribusi aksi lawan yang dikondisikan pada trajectory aksi agen.
    \item Merumuskan ukuran \textit{counterfactual confidence improvement} sebagai proksi reduksi ketidakpastian prediktif jangka pendek.
    \item Mengeksplorasi bagaimana komponen belief dan estimasi reduksi ketidakpastian dapat dimanfaatkan dalam mekanisme seleksi aksi yang mempertimbangkan eksploitasi dan eksplorasi.
\end{enumerate}

\section{Manfaat Penelitian}

Kontribusi utama penelitian ini adalah sebagai berikut:

\begin{enumerate}
    \item Mengusulkan modul belief lawan yang terpisah dari optimisasi reward tanpa asumsi struktur parametrik eksplisit terhadap algoritma lawan.
    \item Memperkenalkan pendekatan \textit{multi-step short-horizon forecasting} dalam konteks IPD adaptif.
    \item Mengusulkan estimasi \textit{counterfactual confidence improvement} sebagai ukuran eksplorasi epistemik berbasis prediksi.
    \item Memberikan formulasi eksplorasi epistemik berbasis pembaruan belief kontrafaktual yang dapat diaplikasikan pada lingkungan interaksi strategis non-stasioner.
    \item Memberikan kerangka eksplorasi epistemik jangka pendek pada interaksi strategis berulang non-stasioner yang tidak bergantung pada asumsi struktur algoritma lawan.
\end{enumerate}

\section{Sistematika Penelitian}

\begin{enumerate}
    \item Bab 1: Pendahuluan: Latar belakang, rumusan masalah, batasan, tujuan, manfaat, dan sistematika penelitian.
    \item Bab 2: Tinjauan Pustaka: Kajian literatur terkait opponent modelling, IPD, dan pendekatan pembelajaran daring.
    \item Bab 3: Kerangka Teoritis: Formulasi masalah, definisi metrik, dan arsitektur umum.
    \item Bab 4: Metode Penelitian: Desain eksperimen, algoritma, dan prosedur evaluasi.
    \item Bab 5: Jadwal Penelitian: Rencana waktu pelaksanaan penelitian.
\end{enumerate}
