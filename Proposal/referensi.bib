
@inproceedings{qiao_o2m_2024,
	title = {O2M: Online Opponent Modeling in Online General-Sum Matrix Games},
	url = {https://ieeexplore.ieee.org/document/10899996/},
	doi = {10.1109/ICAIRC64177.2024.10899996},
	shorttitle = {O2M},
	abstract = {This paper focuses on strategy learning in online general-sum games, specifically addressing online matrix games ({MGs}), where two players control parameters and face an unknown, stochastically varying payoff matrix. The goal is to learn strategies that maximize long-term rewards within this dynamic and uncertain game environment. We introduce Online Opponent Modeling (O2M), a novel algorithm designed to overcome the limitations of {OMG}-{RFTL}, which struggles in general-sum game settings. O2M incorporates opponent modeling techniques into online game frameworks, alleviating the non-stationarity issue inherent in multi-agent systems and enabling agents to achieve superior rewards.},
	eventtitle = {2024 4th International Conference on Artificial Intelligence, Robotics, and Communication ({ICAIRC})},
	pages = {358--361},
	booktitle = {2024 4th International Conference on Artificial Intelligence, Robotics, and Communication ({ICAIRC})},
	author = {Qiao, Xinyu and Han, Congyin and Guo, Tainde},
	urldate = {2025-11-04},
	date = {2024-12},
	keywords = {Artificial intelligence, Heuristic algorithms, Multi-agent systems, Robots, Games, Faces, Matrix Game, Online learning, Opponent modeling},
	file = {Full Text PDF:/home/paqh/Zotero/storage/BSW9J3D3/Qiao et al. - 2024 - O2M Online Opponent Modeling in Online General-Sum Matrix Games.pdf:application/pdf},
}

@inproceedings{zhu_evolutionary_2025,
	title = {Evolutionary Dynamics of Cooperation and Extortion on Networks With Fitness-Dependent Rules},
	url = {https://ieeexplore.ieee.org/document/11051564/},
	doi = {10.1109/ICAISISAS64483.2025.11051564},
	abstract = {This study establishes a multi-agent modeling framework to analyze the evolutionary dynamics of zero-determinant strategies on regular networks under hybrid social dilemmas integrating Prisoner's Dilemma and Snowdrift game. By formulating fitness-driven dynamical equations, theoretical and numerical results reveal that cooperators exhibit dominance in steady-state under pairwise comparison rules, whereas extortion strategies demonstrate significant invasion characteristics in scale-free networks under death-birth update rule. Notably, the influence of both temptation and extortion factors on cooperation evolution exhibits a pronounced rule-dependent phenomenon. By decoupling the multidimensional coupling effects of network topology, strategy update rule and payoff parameters, we establish critical criteria for cooperation evolution. These findings provide a dynamical theoretical basis for illustrating cooperation emergence mechanisms in complex social networks and designing intervention strategies to enhance collective cooperation.},
	eventtitle = {2025 Joint International Conference on Automation-Intelligence-Safety ({ICAIS}) \& International Symposium on Autonomous Systems ({ISAS})},
	pages = {1--6},
	booktitle = {2025 Joint International Conference on Automation-Intelligence-Safety ({ICAIS}) \& International Symposium on Autonomous Systems ({ISAS})},
	author = {Zhu, Lei and Zhu, Yuying and Xia, Chengyi},
	urldate = {2025-11-04},
	date = {2025-05},
	note = {{ISSN}: 2996-3850},
	keywords = {Mathematical models, Game theory, Games, Analytical models, Couplings, Dynamical equations, Evolutionary dynamics, Evolutionary game dynamics, Network topology, Numerical models, Simulation, Social networking (online), Steady-state, Zero determinant strategy, Multi agent systems, Dynamics, Dynamical equation, Evolutionary game dynamic, Evolutionary games, Intelligent agents, Modelling framework, Multi-Agent Model, Prisoner dilemma game, Regular networks, Social dilemmas},
	annotation = {Cited by: 0},
	annotation = {Cited by: 0},
	file = {Full Text PDF:/home/paqh/Zotero/storage/VKZ5HZSN/Zhu et al. - 2025 - Evolutionary Dynamics of Cooperation and Extortion on Networks With Fitness-Dependent Rules.pdf:application/pdf},
}

@inproceedings{lv_inducing_2023,
	title = {Inducing Coordination in Multi-Agent Repeated Game through Hierarchical Gifting Policies},
	url = {https://ieeexplore.ieee.org/document/10298392/},
	doi = {10.1109/MASS58611.2023.00041},
	abstract = {Coordination, i.e., multiple autonomous agents in a system to achieve a common goal, is critical for distributed systems since it can increase the overall reward among all agents. However, The dynamic environment and selfish agents pose challenges to learning coordination behavior from historical interaction data in a long-term interaction environment. Previous works mostly focus on one-shot or short-term distributed agent interaction environments, which often leads to selfish or lazy behavior in long-term interaction environments, i.e., prioritizing individual optimal strategies over cooperative strategies. This behavior is mainly due to the lack of historical memory or incomplete use of historical interaction data to guide the current interaction strategy. In this paper, we propose a hierarchical peer-rewarding mechanism, hierarchical gifting, that allows each agent to dynamically assign some of their rewards to other agents based on historical interaction data and guide the agents towards more coordinated behavior while ensuring that agents remain selfish and decentralized. Specifically, we first propose an auxiliary opponent modeling task so that agents can infer opponents' types through historical interaction trajectories. In addition, we design a hierarchical gifting strategy that dynamically changes during execution based on known opponents' types. We employ a theoretical framework that captures the benefit of hierarchical gifting in converging to the coordinated behavior by characterizing the equilibria's basins of attraction in a dynamical system. With hierarchical gifting, we demonstrate increased coordinated behavior of different risk, general-sum coordination games to the prosocial equilibrium both via numerical analysis and experiments.},
	eventtitle = {2023 {IEEE} 20th International Conference on Mobile Ad Hoc and Smart Systems ({MASS})},
	pages = {279--287},
	booktitle = {2023 {IEEE} 20th International Conference on Mobile Ad Hoc and Smart Systems ({MASS})},
	author = {Lv, Mingze and Liu, Jiaqi and Guo, Bin and Ding, Yasan and Zhang, Yun and Yu, Zhiwen},
	urldate = {2025-11-04},
	date = {2023-09},
	note = {{ISSN}: 2155-6814},
	keywords = {Risk assessment, Task analysis, Dynamical systems, Trajectory, Game theory, Games, Autonomous agents, Behavioral sciences, Coordination, Game Theory, Multi-agent Reinforcement Learning, Multi-agent Systems, Numerical analysis, Reinforcement learning, Learning systems, Multi agent systems, Repeated games, Multi agent, Multi-agent reinforcement learning, Coordinated behavior, Distributed systems, Dynamic environments, Hierarchical systems, Interaction environment, Long-term interaction, Selfish Agents},
	annotation = {Cited by: 0},
	annotation = {Cited by: 0},
	file = {Full Text PDF:/home/paqh/Zotero/storage/XTZ33ETF/Lv et al. - 2023 - Inducing Coordination in Multi-Agent Repeated Game through Hierarchical Gifting Policies.pdf:application/pdf},
}

@article{doshi_recursively_2020,
	title = {Recursively modeling other agents for decision making: A research perspective},
	volume = {279},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S000437021930027X},
	doi = {https://doi.org/10.1016/j.artint.2019.103202},
	abstract = {Individuals exhibit theory of mind, attributing beliefs, intent, and mental states to others as explanations of observed actions. Dennett's intentional stance offers an analogous abstraction for computational agents seeking to understand, explain, or predict others' behaviors. These recognized theories provide a formal basis to ongoing investigations of recursive modeling. We review and situate various frameworks for recursive modeling that have been studied in game- and decision- theories, and have yielded methods useful to {AI} researchers. Sustained attention given to these frameworks has produced new analyses and methods with an aim toward making recursive modeling practicable. Indeed, we also review some emerging uses and the insights these yielded, which are indicative of pragmatic progress in this area. The significance of these frameworks is that higher-order reasoning is critical to correctly recognizing others' intent or outthinking opponents. Such reasoning has been utilized in academic, business, military, security, and other contexts both to train and inform decision-making agents in organizational and strategic contexts, and also to more realistically predict and best respond to other agents' intent.},
	pages = {103202},
	journaltitle = {Artificial Intelligence},
	author = {Doshi, Prashant and Gmytrasiewicz, Piotr and Durfee, Edmund},
	date = {2020},
	keywords = {Game theory, Multiagent systems, Decision theory, Hierarchical beliefs, Recursive modeling, Theory of mind},
	file = {PDF:/home/paqh/Zotero/storage/YS6A24UR/Doshi et al. - 2020 - Recursively modeling other agents for decision making A research perspective.pdf:application/pdf},
}

@article{gomez_grounded_2025,
	title = {Grounded predictions of teamwork as a one-shot game: A multiagent multi-armed bandits approach},
	volume = {341},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370225000268},
	doi = {https://doi.org/10.1016/j.artint.2025.104307},
	abstract = {Humans possess innate collaborative capacities. However, effective teamwork often remains challenging. This study delves into the feasibility of collaboration within teams of rational, self-interested agents who engage in teamwork without the obligation to contribute. Drawing from psychological and game theoretical frameworks, we formalise teamwork as a one-shot aggregative game, integrating insights from Steiner's theory of group productivity. We characterise this novel game's Nash equilibria and propose a multiagent multi-armed bandit system that learns to converge to approximations of such equilibria. Our research contributes value to the areas of game theory and multiagent systems, paving the way for a better understanding of voluntary collaborative dynamics. We examine how team heterogeneity, task typology, and assessment difficulty influence agents' strategies and resulting teamwork outcomes. Finally, we empirically study the behaviour of work teams under incentive systems that defy analytical treatment. Our agents demonstrate human-like behaviour patterns, corroborating findings from social psychology research.},
	pages = {104307},
	journaltitle = {Artificial Intelligence},
	author = {Gómez, Alejandra López de Aberasturi and Sierra, Carles and Sabater-Mir, Jordi},
	date = {2025},
	keywords = {Aggregative games, Cooperative {AI}, Group productivity theory, Multiagent multi-armed bandits},
	file = {PDF:/home/paqh/Zotero/storage/8PF8XYH9/Gómez et al. - 2025 - Grounded predictions of teamwork as a one-shot game A multiagent multi-armed bandits approach.pdf:application/pdf},
}

@article{mendez-naya_delegation_2025,
	title = {Delegation and strategic altruism: A theoretical approach},
	volume = {138},
	issn = {0165-4896},
	url = {https://www.sciencedirect.com/science/article/pii/S0165489625000800},
	doi = {https://doi.org/10.1016/j.mathsocsci.2025.102465},
	abstract = {In this paper we introduce two refinements of Nash equilibria for extensive form games: the quasi-stable equilibrium and the stable equilibrium. We then introduce the general strategic game with delegates and study new solutions in that context. We apply the new solution concepts to symmetric n-player games in which each player has two strategies. The main conclusion is that, in the prisoner's dilemma, if the punishment payoff is sufficient, both players obtain the cooperative payoff when they choose strategically altruistic delegates.},
	pages = {102465},
	journaltitle = {Mathematical Social Sciences},
	author = {Méndez-Naya, Luciano},
	date = {2025},
	keywords = {Games, Altruism, Delegates, Equilibrium},
	file = {PDF:/home/paqh/Zotero/storage/KVJ5YB7N/Méndez-Naya - 2025 - Delegation and strategic altruism A theoretical approach.pdf:application/pdf},
}

@article{di_coupling_2023,
	title = {The coupling effect between the environment and strategies drives the emergence of group cooperation},
	volume = {176},
	issn = {0960-0779},
	url = {https://www.sciencedirect.com/science/article/pii/S0960077923010391},
	doi = {https://doi.org/10.1016/j.chaos.2023.114138},
	abstract = {The emergence of cooperation is a central issue in understanding collective behavior and evolution. The eco-evolutionary game model introduces a human–environment coupling mechanism, revealing that the feedback between strategies and the relevant environment is a key element in sustaining long-term cooperation. Previous theoretical studies have observed periodic oscillations between cooperative and defective actions under certain conditions. However, such investigations assume cooperators hold a benefit advantage over defectors, which does not fundamentally illuminate how cooperation emerges. Our paper emphasizes that understanding this issue requires considering inherent human memory characteristics. We refine the eco-evolutionary game model using reinforcement learning, constructing a multi-agent system that couples environment and memory-based decision-making. Comprehensive analyses encompass collective and individual perspectives. Our findings show that with the memory mechanism, oscillations between collective cooperation and defection can still occur, even if defection remains a strict Nash equilibrium. Cooperation emerges from the group's random exploratory actions in depleted environments, altering the environment's trends. A positive feedback loop forms among the environment, individual rewards, and actions, stabilizing cooperation as a favorable individual strategy at that point. However, established group cooperation leads individuals seeking optimal behavior to transition from cooperators to defectors through exploration, resulting in cooperation collapse. Subsequently, the memory mechanism reengages, diluting defectors' expected payoffs and initiating a new round of exploratory behavior within the group. Our results unveil the micro-level mechanisms driving cyclic oscillations, enhancing our understanding of the environment-strategy interplay.},
	pages = {114138},
	journaltitle = {Chaos, Solitons \& Fractals},
	author = {Di, Changyan and Zhou, Qingguo and Shen, Jun and Wang, Jinqiang and Zhou, Rui and Wang, Tianyi},
	date = {2023},
	keywords = {Reinforcement learning, Eco-evolutionary game, Emergence of cooperation, Social dilemma},
	file = {PDF:/home/paqh/Zotero/storage/GFYQ2PNB/Di et al. - 2023 - The coupling effect between the environment and strategies drives the emergence of group cooperation.pdf:application/pdf},
}

@article{perera_learning_2025,
	title = {Learning to cooperate against ensembles of diverse opponents},
	volume = {37},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214668528&doi=10.1007%2Fs00521-024-10511-9&partnerID=40&md5=10fe010167eb8ddf09ffc6248af3f874},
	doi = {10.1007/s00521-024-10511-9},
	abstract = {The emergence of cooperation in decentralized multi-agent systems is challenging; naive implementations of learning algorithms typically fail to converge or converge to equilibria without cooperation. Opponent modeling techniques, combined with reinforcement learning, have been successful in promoting cooperation, but face challenges when other agents are plentiful or anonymous. We envision environments in which agents face a sequence of interactions with different and heterogeneous agents. Inspired by models of evolutionary game theory, we introduce {RL} agents that forgo explicit modeling of others. Instead, they augment their reward signal by considering how to best respond to others assumed to be rational against their own strategy. This technique not only scales well in environments with many agents, but can also outperform opponent modeling techniques across a range of cooperation games. Agents that use the algorithm we propose can successfully maintain and establish cooperation when playing against an ensemble of diverse agents. This finding is robust across different kinds of games and can also be shown not to disadvantage agents in purely competitive interactions. While cooperation in pairwise settings is foundational, interactions across large groups of diverse agents are likely to be the norm in future applications where cooperation is an emergent property of agent design, rather than a design goal at the system level. The algorithm we propose here is a simple and scalable step in this direction. © 2025 Elsevier B.V., All rights reserved.},
	pages = {18835 -- 18849},
	number = {23},
	journaltitle = {Neural Computing and Applications},
	author = {Perera, Isuri and de Nijs, Frits and Garcia, Julián},
	date = {2025},
	note = {Type: Article},
	keywords = {Game theory, Cooperation, Reinforcement learning, Adversarial machine learning, Best response, Decentralised, Decentralized systems, Evolutionary game theory, Federated learning, Iterated prisoner's dilemma, Learning algorithms, Learning systems, Modelling techniques, Multi agent systems, Multiagent systems ({MASs}), Opponent models, Population games, Reinforcement learnings},
	annotation = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
	annotation = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
	file = {PDF:/home/paqh/Zotero/storage/6KM8IXVN/Perera et al. - 2025 - Learning to cooperate against ensembles of diverse opponents.pdf:application/pdf},
}

@article{li_exploiting_2025,
	title = {Exploiting a No-Regret Opponent in Repeated Zero-Sum Games},
	volume = {30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001082101&doi=10.1007%2Fs12204-023-2610-2&partnerID=40&md5=b91d9d28bdade788c8985552e1a51657},
	doi = {10.1007/s12204-023-2610-2},
	abstract = {In repeated zero-sum games, instead of constantly playing an equilibrium strategy of the stage game, learning to exploit the opponent given historical interactions could typically obtain a higher utility. However, when playing against a fully adaptive opponent, one would have difficulty identifying the opponent's adaptive dynamics and further exploiting its potential weakness. In this paper, we study the problem of optimizing against the adaptive opponent who uses no-regret learning. No-regret learning is a classic and widely-used branch of adaptive learning algorithms. We propose a general framework for online modeling no-regret opponents and exploiting their weakness. With this framework, one could approximate the opponent's no-regret learning dynamics and then develop a response plan to obtain a significant profit based on the inferences of the opponent's strategies. We employ two system identification architectures, including the recurrent neural network ({RNN}) and the nonlinear autoregressive exogenous model, and adopt an efficient greedy response plan within the framework. Theoretically, we prove the approximation capability of our {RNN} architecture at approximating specific no-regret dynamics. Empirically, we demonstrate that during interactions at a low level of non-stationarity, our architectures could approximate the dynamics with a low error, and the derived policies could exploit the no-regret opponent to obtain a decent utility. © 2025 Elsevier B.V., All rights reserved.},
	pages = {385 -- 398},
	number = {2},
	journaltitle = {Journal of Shanghai Jiaotong University (Science)},
	author = {Li, Kai and Huang, Wenhan and Li, Chenchen and Deng, Xiaotie},
	date = {2025},
	note = {Type: Article},
	keywords = {Dynamical systems, Learning algorithms, Learning systems, Opponent models, A, Dynamics, Network architecture, No-regret learning, Opponent exploitation, Recurrent neural network, Recurrent neural networks, Religious buildings, Repeated games, Response plans, System-identification, {TP} 18, Zero-sum game},
	annotation = {Cited by: 0},
	annotation = {Cited by: 0},
	file = {PDF:/home/paqh/Zotero/storage/4HZW42FG/Li et al. - 2025 - Exploiting a No-Regret Opponent in Repeated Zero-Sum Games\; 重复零和博弈中对无遗憾对手进行盘剥的研究.pdf:application/pdf},
}

@article{freire_modeling_2023,
	title = {Modeling Theory of Mind in Dyadic Games Using Adaptive Feedback Control},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168770847&doi=10.3390%2Finfo14080441&partnerID=40&md5=87cd85e9f9554ecd20d4f8b610b2302d},
	doi = {10.3390/info14080441},
	abstract = {A major challenge in cognitive science and {AI} has been to understand how intelligent autonomous agents might acquire and predict the behavioral and mental states of other agents in the course of complex social interactions. How does such an agent model the goals, beliefs, and actions of other agents it interacts with? What are the computational principles to model a Theory of Mind ({ToM})? Deep learning approaches to address these questions fall short of a better understanding of the problem. In part, this is due to the black-box nature of deep networks, wherein computational mechanisms of {ToM} are not readily revealed. Here, we consider alternative hypotheses seeking to model how the brain might realize a {ToM}. In particular, we propose embodied and situated agent models based on distributed adaptive control theory to predict the actions of other agents in five different game-theoretic tasks (Harmony Game, Hawk-Dove, Stag Hunt, Prisoner's Dilemma, and Battle of the Exes). Our multi-layer control models implement top-down predictions from adaptive to reactive layers of control and bottom-up error feedback from reactive to adaptive layers. We test cooperative and competitive strategies among seven different agent models (cooperative, greedy, tit-for-tat, reinforcement-based, rational, predictive, and internal agents). We show that, compared to pure reinforcement-based strategies, probabilistic learning agents modeled on rational, predictive, and internal phenotypes perform better in game-theoretic metrics across tasks. The outlined autonomous multi-agent models might capture systems-level processes underlying a {ToM} and suggest architectural principles of {ToM} from a control-theoretic perspective. © 2023 Elsevier B.V., All rights reserved.},
	number = {8},
	journaltitle = {Information (Switzerland)},
	author = {Freire, Ismael T. and Arsiwalla, X. D. and Puigbò, Jordi Ysard and Verschure, P.},
	date = {2023},
	note = {Type: Article},
	keywords = {Deep learning, Game theory, Autonomous agents, Agent modeling, Reinforcement learning, Learning systems, Multi agent systems, Reinforcement learnings, Intelligent agents, Adaptive control systems, Adaptive feedback control, Behavioral state, Cognitive architectures, Cognitive science, Cognitive systems, Computation theory, Control theory, Feedback, Forecasting, Game-theoretic, Intelligent autonomous agents, Model theory, Theory of minds},
	annotation = {Cited by: 3; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access},
	annotation = {Cited by: 3; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access},
	file = {PDF:/home/paqh/Zotero/storage/YGEBTDSA/Freire et al. - 2023 - Modeling Theory of Mind in Dyadic Games Using Adaptive Feedback Control.pdf:application/pdf},
}

@article{hu_modeling_2023,
	title = {Modeling opponent learning in multiagent repeated games},
	volume = {53},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144725258&doi=10.1007%2Fs10489-022-04249-x&partnerID=40&md5=b14f22e5ee23ff8a5f6ea09addf8507c},
	doi = {10.1007/s10489-022-04249-x},
	abstract = {Multiagent reinforcement learning ({MARL}) has been used extensively in the game environment. One of the main challenges in {MARL} is that the environment of the agent system is dynamic, and the other agents are also updating their strategies. Therefore, modeling the opponents' learning process and adopting specific strategies to shape learning is an effective way to obtain better training results. Previous studies such as {DRON}, {LOLA} and {SOS} approximated the opponent's learning process and gave effective applications. However, these studies modeled only transient changes in opponent strategies and lacked stability in the improvement of equilibrium efficiency. In this article, we design the {MOL} (modeling opponent learning) method based on the Stackelberg game. We use best response theory to approximate the opponents' preferences for different actions and explore stable equilibrium with higher rewards. We find that {MOL} achieves better results in several games with classical structures (the Prisoner's Dilemma, Stackelberg Leader game and Stag Hunt with 3 players), and in randomly generated bimatrix games. {MOL} performs well in competitive games played against different opponents and converges to stable points that score above the Nash equilibrium in repeated game environments. The results may provide a reference for the definition of equilibrium in multiagent reinforcement learning systems, and contribute to the design of learning objectives in {MARL} to avoid local disadvantageous equilibrium and improve general efficiency. © 2023 Elsevier B.V., All rights reserved.},
	pages = {17194 -- 17210},
	number = {13},
	journaltitle = {Applied Intelligence},
	author = {Hu, Yudong and Han, Congying and Li, Haoran and Guo, Tiande},
	date = {2023},
	note = {Type: Article},
	keywords = {Reinforcement learning, Learning systems, Multi agent systems, Opponent models, Repeated games, Agent systems, Efficiency, Fertilizers, Game environment, Learning methods, Learning process, Multi agent, Multi-agent reinforcement learning, Shape learning, Transient changes},
	annotation = {Cited by: 5; All Open Access; Hybrid Gold Open Access},
	annotation = {Cited by: 5; All Open Access; Hybrid Gold Open Access},
	file = {Full Text:/home/paqh/Zotero/storage/MEDYB3MV/Hu et al. - 2023 - Modeling opponent learning in multiagent repeated games.pdf:application/pdf},
}

@article{elhamer_effects_2020,
	title = {The effects of population size and information update rates on the emergent patterns of cooperative clusters in a large-scale social particle swarm model},
	volume = {25},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074536816&doi=10.1007%2Fs10015-019-00558-6&partnerID=40&md5=f9805479bd27a789b9c3c146d2816666},
	doi = {10.1007/s10015-019-00558-6},
	abstract = {We study the impact of network size in the context of interactions within social network services ({SNS}) on cooperation among its users, as well as that of the speed of information update about other neighbors during interaction, using an enhanced version of a swarm model that uses prisoner's dilemma as social interaction strategy and that models users' interactions through kinematics. We focus on the speed of information update about social environments and study the relationships between the resulting patterns of cooperation in different information update rates. We observed the large variations among emerging many cooperative clusters in size, speed, and cooperation rate in the large population. Moreover, cooperation was more promoted when the information update rate was high, in contrast to low update rate where the population converged to a few large clusters with many wandering defectors. © 2020 Elsevier B.V., All rights reserved.},
	pages = {149 -- 158},
	number = {1},
	journaltitle = {Artificial Life and Robotics},
	author = {Elhamer, Zineb and Suzuki, Reiji and Arita, Takaya},
	date = {2020},
	note = {Type: Article},
	keywords = {Multi agent systems, Multi-Agent Model, Information updates, Large population, Population sizes, Population statistics, Social environment, Social interactions, Social network service ({SNS}), Social network services},
	annotation = {Cited by: 4},
	annotation = {Cited by: 4},
	file = {PDF:/home/paqh/Zotero/storage/9N2MUC47/Elhamer et al. - 2020 - The effects of population size and information update rates on the emergent patterns of cooperative.pdf:application/pdf},
}

@inproceedings{wang_achieving_2019,
	title = {Achieving cooperation through deep multiagent reinforcement learning in sequential prisoner's dilemmas},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075428156&doi=10.1145%2F3356464.3357712&partnerID=40&md5=f3900b979cd42f8d14b73dbb5ee134ba},
	doi = {10.1145/3356464.3357712},
	abstract = {The Iterated Prisoner's Dilemma has guided research on social dilemmas for decades. However, it distinguishes between only two atomic actions: cooperate and defect. In real-world prisoner's dilemmas, these choices are temporally extended and different strategies may correspond to sequences of actions, reflecting grades of cooperation. We introduce a Sequential Prisoner's Dilemma ({SPD}) game to better capture the aforementioned characteristics. In this work, we propose a deep multiagent reinforcement-learning approach that investigates the evolution of mutual cooperation in {SPD} games. Our approach consists of two phases. The first phase is offline: it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network. The second phase is online: an agent adaptively selects its policy based on the detected degree of opponent cooperation. The effectiveness of our approach is demonstrated in two representative {SPD} 2D games: the Apple-Pear game and the Fruit Gathering game. Experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents. © 2022 Elsevier B.V., All rights reserved.},
	booktitle = {{ACM} International Conference Proceeding Series},
	author = {Wang, Weixun and Wang, Yixi and Hao, Jianye and Taylor, Matthew E.},
	date = {2019},
	note = {Type: Conference paper},
	keywords = {Deep learning, Opponent modeling, Reinforcement learning, Multi agent systems, Social dilemmas, Multi-agent reinforcement learning, Atomic actions, Cooperation degree, Detection networks, Fruits, Multiagent reinforcement learning approach, Mutual Cooperation},
	annotation = {Cited by: 1},
	annotation = {Cited by: 1},
	file = {PDF:/home/paqh/Zotero/storage/K7Q4J56S/Wang et al. - 2019 - Achieving cooperation through deep multiagent reinforcement learning in sequential prisoner's dilemm.pdf:application/pdf},
}

@article{jin_achieving_2025,
	title = {Achieving collective welfare in multi-agent reinforcement learning via suggestion sharing},
	volume = {114},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/10.1007/s10994-025-06823-z},
	doi = {10.1007/s10994-025-06823-z},
	abstract = {Abstract
            In human society, the conflict between self-interest and collective well-being often obstructs efforts to achieve shared welfare. Related concepts like the Tragedy of the Commons and Social Dilemmas frequently manifest in our daily lives. As artificial agents increasingly serve as autonomous proxies for humans, we propose a novel multi-agent reinforcement learning ({MARL}) method to address this issue - learning policies to maximise collective returns even when individual agents' interests conflict with the collective one. Unlike traditional cooperative {MARL} solutions that involve sharing rewards, values, and policies or designing intrinsic rewards to encourage agents to learn collectively optimal policies, we propose a novel {MARL} approach where agents exchange action suggestions. Our method reveals less private information compared to sharing rewards, values, or policies, while enabling effective cooperation without the need to design intrinsic rewards. Our algorithm is supported by our theoretical analysis that establishes a bound on the discrepancy between collective and individual objectives, demonstrating how sharing suggestions can align agents' behaviours with the collective objective. Experimental results demonstrate that our algorithm performs competitively with baselines that rely on value or policy sharing or intrinsic rewards.},
	pages = {190},
	number = {8},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Jin, Yue and Wei, Shuangqing and Montana, Giovanni},
	urldate = {2025-11-04},
	date = {2025-08},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/SUNF6TEX/Jin et al. - 2025 - Achieving collective welfare in multi-agent reinforcement learning via suggestion sharing.pdf:application/pdf},
}

@article{de_weerd_higher-order_2022,
	title = {Higher-order theory of mind is especially useful in unpredictable negotiations},
	volume = {36},
	issn = {1387-2532, 1573-7454},
	url = {https://link.springer.com/10.1007/s10458-022-09558-6},
	doi = {10.1007/s10458-022-09558-6},
	abstract = {Abstract
            
              In social interactions, people often reason about the beliefs, goals and intentions of others. This
              theory of mind
              allows them to interpret the behavior of others, and predict how they will behave in the future. People can also use this ability recursively: they use
              higher-order theory of mind
              to reason about the theory of mind abilities of others, as in “he thinks that I don't know that he sent me an anonymous letter”. Previous agent-based modeling research has shown that the usefulness of higher-order theory of mind reasoning can be useful across competitive, cooperative, and mixed-motive settings. In this paper, we cast a new light on these results by investigating how the predictability of the environment influences the effectiveness of higher-order theory of mind. Our results show that the benefit of (higher-order) theory of mind reasoning is strongly dependent on the predictability of the environment. We consider agent-based simulations in repeated one-shot negotiations in a particular negotiation setting known as Colored Trails. When this environment is highly predictable, agents obtain little benefit from theory of mind reasoning. However, if the environment has more observable features that change over time, agents without the ability to use theory of mind experience more difficulties predicting the behavior of others accurately. This in turn allows theory of mind agents to obtain higher scores in these more dynamic environments. These results suggest that the human-specific ability for higher-order theory of mind reasoning may have evolved to allow us to survive in more complex and unpredictable environments.},
	pages = {30},
	number = {2},
	journaltitle = {Autonomous Agents and Multi-Agent Systems},
	shortjournal = {Auton Agent Multi-Agent Syst},
	author = {De Weerd, Harmen and Verbrugge, Rineke and Verheij, Bart},
	urldate = {2025-11-04},
	date = {2022-10},
	langid = {english},
	file = {Full Text:/home/paqh/Zotero/storage/M6GLR97I/De Weerd et al. - 2022 - Higher-order theory of mind is especially useful in unpredictable negotiations.pdf:application/pdf},
}


@article{axelrod_evolution_1992,
	title = {The Evolution of Cooperation*},
	author = {Axelrod, Robert},
	date = {1992},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/2FLTW9EQ/Axelrod - The Evolution of Cooperation.pdf:application/pdf},
}

@article{axelrod_evolution_1981,
	title = {The Evolution of Cooperation},
	abstract = {Cooperation in organisms, whether bacteria or primates, has been a difficulty for evolutionary theory since Darwin. On the assumption that interactions between pairs of individuals occur on a probabilistic basis, a model is developed based on the concept of an evolutionarily stable strategy in the context of the Prisoner's Dilemma game. Deductions from the model, and the results of a computer tournament show how cooperation based on reciprocity can get started in an asocial world, can thrive while interacting with a wide range of other strategies, and can resist invasion once fully established. Potential applications include specific aspects of territoriality, mating, and disease.},
	author = {Axelrod, Robert and Hamilton, William D},
	date = {1981},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/D45VFM7S/Axelrod and Hamilton - 1981 - The Evolution of Cooperation.pdf:application/pdf},
}

@article{trivers_evolution_1971,
	title = {The Evolution of Reciprocal Altruism},
	volume = {46},
	issn = {0033-5770, 1539-7718},
	url = {https://www.journals.uchicago.edu/doi/10.1086/406755},
	doi = {10.1086/406755},
	abstract = {A model is presentedto account for the natural selection of what is termedreciprocally altruistic behavior. The model shows how selection can operate -against the cheater (non-reciprocator)in the system. Three instances of altruistic behavior are discussed, the evolution of which the model can explain: (1) behavior involved in cleaning symbioses;(2) warning cries in birds: and (3) human reciprocal altruism.},
	pages = {35--57},
	number = {1},
	journaltitle = {The Quarterly Review of Biology},
	shortjournal = {The Quarterly Review of Biology},
	author = {Trivers, Robert L.},
	urldate = {2025-12-08},
	date = {1971-03},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/DP2M25V4/Trivers - 1971 - The Evolution of Reciprocal Altruism.pdf:application/pdf},
}

@inbook{charness_cooperation_2021,
	location = {London},
	edition = {1},
	title = {Cooperation and punishment in public goods experiments (by Ernst Fehr and Simon Gächter)},
	isbn = {978-1-003-01912-1},
	url = {https://www.taylorfrancis.com/books/9781003019121/chapters/10.4324/9781003019121-12},
	pages = {134--143},
	booktitle = {The Art of Experimental Economics},
	publisher = {Routledge},
	author = {Chen, Yan},
	bookauthor = {Charness, Gary and Pingle, Mark},
	urldate = {2025-12-08},
	date = {2021-07-12},
	langid = {english},
	doi = {10.4324/9781003019121-12},
	file = {PDF:/home/paqh/Zotero/storage/QMHV75MN/Chen - 2021 - Cooperation and punishment in public goods experiments (by Ernst Fehr and Simon Gächter).pdf:application/pdf},
}

@article{leung_analysis_1976,
	title = {Analysis of Models for Commercial Fishing: Mathematical and Economical Aspects},
	volume = {44},
	issn = {00129682},
	url = {https://www.jstor.org/stable/1912725?origin=crossref},
	doi = {10.2307/1912725},
	shorttitle = {Analysis of Models for Commercial Fishing},
	pages = {295},
	number = {2},
	journaltitle = {Econometrica},
	shortjournal = {Econometrica},
	author = {Leung, Anthony and Wang, Ar-Young},
	urldate = {2025-12-08},
	date = {1976-03},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/IEPVQ5C9/Leung and Wang - 1976 - Analysis of Models for Commercial Fishing Mathematical and Economical Aspects.pdf:application/pdf},
}

@article{smith_evolution_1982,
	title = {Evolution and the Theory of Games},
	author = {Smith, John Maynard},
	date = {1982},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/QLS8UF9M/Smith - Evolution and the Theory of Games.pdf:application/pdf},
}

@article{nashed_survey_2022,
	title = {A Survey on Opponent Modeling in Adversarial Domains},
	abstract = {Opponent modeling is the ability to use prior knowledge and observations in order to predict the behavior of an opponent. This survey presents a comprehensive overview of existing opponent modeling techniques for adversarial domains, many of which must address stochastic, continuous, or concurrent actions, and sparse, partially observable payoﬀ structures. We discuss all the components of opponent modeling systems, including feature extraction, learning algorithms, and strategy abstractions. These discussions lead us to propose a new form of analysis for describing and predicting the evolution of game states over time. We then introduce a new framework that facilitates method comparison, analyze a representative selection of techniques using the proposed framework, and highlight common trends among recently proposed methods. Finally, we list several open problems and discuss future research directions inspired by {AI} research on opponent modeling and related research in other disciplines.},
	author = {Nashed, Samer B and Zilberstein, Shlomo},
	date = {2022},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/GK8TDDA8/Nashed and Zilberstein - A Survey on Opponent Modeling in Adversarial Domains.pdf:application/pdf},
}

@article{page_prisma_2021,
	title = {The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews},
	issn = {1756-1833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.n71},
	doi = {10.1136/bmj.n71},
	shorttitle = {The {PRISMA} 2020 statement},
	abstract = {{POINTS} To ensure a systematic review is valuable to users, authors should prepare a transparent, complete, and accurate account of why the review was done, what they did, and what they found The {PRISMA} 2020 statement provides updated reporting guidance for systematic reviews that reflects advances in methods to identify, select, appraise, and synthesise studies The {PRISMA} 2020 statement consists of a 27-item checklist, an expanded checklist that details reporting recommendations for each item, the {PRISMA} 2020 abstract checklist, and revised flow diagrams for original and updated reviews We anticipate that the {PRISMA} 2020 statement will benefit authors, editors, and peer reviewers of systematic reviews, and different users of reviews, including guideline developers, policy makers, healthcare providers, patients, and other stakeholders {BMJ}: first Ppruobtleisctheeddbaysc1o0.p1y1ri3g6h/ tb,minj.cnl7u1dionng 2fo9rMuasrecshr2el0a2t1e.{dDtoowtenlxot} aadnedddfartoammhitntipnsg:,//{AwIwtrwa}.ibnimnj.gc,oamn /dosnim1i0laDr eteccehmnboelrog2i0e2s5. by guest.},
	pages = {n71},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Page, Matthew J and {McKenzie}, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M and Hróbjartsson, Asbjørn and Lalu, Manoj M and Li, Tianjing and Loder, Elizabeth W and Mayo-Wilson, Evan and {McDonald}, Steve and {McGuinness}, Luke A and Stewart, Lesley A and Thomas, James and Tricco, Andrea C and Welch, Vivian A and Whiting, Penny and Moher, David},
	urldate = {2025-12-10},
	date = {2021-03-29},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/67G2ICWY/Page et al. - 2021 - The PRISMA 2020 statement an updated guideline for reporting systematic reviews.pdf:application/pdf},
}

@article{hernandez-leal_survey_2019,
	title = {A Survey and Critique of Multiagent Deep Reinforcement Learning},
	volume = {33},
	issn = {1387-2532, 1573-7454},
	url = {http://arxiv.org/abs/1810.05587},
	doi = {10.1007/s10458-019-09421-1},
	abstract = {Deep reinforcement learning ({RL}) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning ({MAL}) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning ({MDRL}) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in {MAL} and {RL}, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from {MDRL} works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of {MDRL} (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., {RL} and {MAL}) in a joint eﬀort to promote fruitful research in the multiagent community.},
	pages = {750--797},
	number = {6},
	journaltitle = {Autonomous Agents and Multi-Agent Systems},
	shortjournal = {Auton Agent Multi-Agent Syst},
	author = {Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E.},
	urldate = {2025-12-21},
	date = {2019-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.05587 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	annotation = {Comment: Under review since Oct 2018. Earlier versions of this work had the title: "Is multiagent deep reinforcement learning the answer or the question? A brief survey"},
	file = {PDF:/home/paqh/Zotero/storage/3SCCQDCP/Hernandez-Leal et al. - 2019 - A Survey and Critique of Multiagent Deep Reinforcement Learning.pdf:application/pdf},
}

@article{albrecht_autonomous_2018,
	title = {Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems},
	volume = {258},
	issn = {00043702},
	url = {http://arxiv.org/abs/1709.08071},
	doi = {10.1016/j.artint.2018.01.002},
	shorttitle = {Autonomous Agents Modelling Other Agents},
	abstract = {Much research in artiﬁcial intelligence is concerned with the development of autonomous agents that can interact eﬀectively with other agents. An important aspect of such agents is the ability to reason about the behaviours of other agents, by constructing models which make predictions about various properties of interest (such as actions, goals, beliefs) of the modelled agents. A variety of modelling approaches now exist which vary widely in their methodology and underlying assumptions, catering to the needs of the diﬀerent sub-communities within which they were developed and reﬂecting the diﬀerent practical uses for which they are intended. The purpose of the present article is to provide a comprehensive survey of the salient modelling methods which can be found in the literature. The article concludes with a discussion of open problems which may form the basis for fruitful future research.},
	pages = {66--95},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Albrecht, Stefano V. and Stone, Peter},
	urldate = {2025-12-21},
	date = {2018-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.08071 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	annotation = {Comment: Final manuscript (46 pages), published in Artificial Intelligence Journal. The {arXiv} version also contains a table of contents after the abstract, but is otherwise identical to the {AIJ} version. Keywords: autonomous agents, multiagent systems, modelling other agents, opponent modelling},
	file = {PDF:/home/paqh/Zotero/storage/RU7DVCZ9/Albrecht and Stone - 2018 - Autonomous Agents Modelling Other Agents A Comprehensive Survey and Open Problems.pdf:application/pdf},
}

@book{axelrod_evolution_2006,
	location = {New York, {NY}},
	edition = {Rev. ed},
	title = {The evolution of cooperation},
	isbn = {978-0-465-02121-5 978-0-465-00564-2 978-0-465-02122-2},
	pagetotal = {241},
	publisher = {Basic Books},
	author = {Axelrod, Robert M.},
	date = {2006},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/PBS83W75/Axelrod - 2006 - The evolution of cooperation.pdf:application/pdf},
}

@book{bonanno_game_2024,
	location = {Place of publication not identified},
	edition = {Third edition},
	title = {Game theory: volume 1: basic concepts},
	isbn = {978-1-9836-0463-8},
	shorttitle = {Game theory},
	publisher = {Kindle Direct Publishing},
	author = {Bonanno, Giacomo},
	date = {2024},
	langid = {english},
	note = {{OCLC}: 1506202870},
	file = {PDF:/home/paqh/Zotero/storage/E65NVD6P/Bonanno - 2024 - Game theory volume 1 basic concepts.pdf:application/pdf},
}

@article{sutton_reinforcement_2015,
	title = {Reinforcement Learning: An Introduction},
	author = {Sutton, Richard S and Barto, Andrew G},
	date = {2015},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/EINGZTN3/Sutton and Barto - Reinforcement Learning An Introduction.pdf:application/pdf},
}

@article{shoham_multiagent_2009,
	title = {Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations},
	author = {Shoham, Yoav},
	date = {2009},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/V2H8HELF/Shoham - Multiagent Systems Algorithmic, Game-Theoretic, and Logical Foundations.pdf:application/pdf},
}

@misc{bengio_scheduled_2015,
	title = {Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1506.03099},
	doi = {10.48550/arXiv.1506.03099},
	abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exempliﬁed by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields signiﬁcant improvements. Moreover, it was used successfully in our winning entry to the {MSCOCO} image captioning challenge, 2015.},
	number = {{arXiv}:1506.03099},
	publisher = {{arXiv}},
	author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
	urldate = {2026-02-16},
	date = {2015-09-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.03099 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/paqh/Zotero/storage/DKABT7DP/Bengio et al. - 2015 - Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.pdf:application/pdf},
}

@collection{nisan_algorithmic_2008,
	location = {Cambridge},
	edition = {Repr., [Nachdr.]},
	title = {Algorithmic game theory},
	isbn = {978-0-521-87282-9},
	pagetotal = {754},
	publisher = {Cambridge Univ. Press},
	editor = {Nisan, Noam and Roughgarden, Tim and Tardos, Éva and Vazirani, Vijay V.},
	date = {2008},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/E32E6JJY/Nisan et al. - 2008 - Algorithmic game theory.pdf:application/pdf},
}

@article{caruana_multitask_1997,
	title = {Multitask Learning},
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on {MTL}, presents new evidence that {MTL} in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for {MTL} with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	author = {Caruana, Rich},
	date = {1997},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/DCSF5XUI/Caruana - Multitask Learning.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2026-02-16},
	date = {1997-11-01},
	langid = {english},
	file = {PDF:/home/paqh/Zotero/storage/PR95KQKC/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf},
}

@article{nax_behavioral_2015,
	title = {Behavioral game theory},
	url = {https://www.zora.uzh.ch/handle/20.500.14742/203029},
	doi = {10.5167/UZH-227485},
	author = {Nax, Heinrich H.},
	urldate = {2026-02-16},
	date = {2015},
	note = {Publisher: {ETH} Zürich},
	file = {PDF:/home/paqh/Zotero/storage/HRPPQ4Q8/Nax - 2015 - Behavioral game theory.pdf:application/pdf},
}

