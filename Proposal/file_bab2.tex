
sebanyak 13 studi dimasukkan ke dalam tinjauan sistematis final. Judul dan karakteristik dari studi-studi tersebut disajikan pada Tabel~~\ref{tab:paper_characteristics}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[height=0.8\textheight]{images/PrismaIDN.png}
    \caption{Diagram alur seleksi studi.}\label{fig:prisma}
\end{figure}

\begin{table}[!htbp]
\centering
\caption{Karakteristik Studi yang Disertakan}\label{tab:paper_characteristics}
\renewcommand{\arraystretch}{1}
\begin{tabular}{c p{7.0cm} p{7.0cm}}
\midrule
\textbf{Ref.} & \textbf{Title} & \textbf{Characteristics} \\ \midrule

~\cite{qiao_o2m_2024} &
O2M\@: Online Opponent Modeling in Online General-Sum Matrix Games &
~\textit{Online opponent modelling} untuk permainan~\textit{general-sum} dinamis. \\ \midrule

~\cite{zhu_evolutionary_2025} &
Evolutionary Dynamics of Cooperation and Extortion on Networks With Fitness-Dependent Rules &
Menganalisis strategi~\textit{zero-determinant}. \\ \midrule

~\cite{lv_inducing_2023} &
Inducing Coordination in Multi-Agent Repeated Game through Hierarchical Gifting Policies &
Mengusulkan mekanisme~\textit{hierarchical gifting}. \\ \midrule

~\cite{gomez_grounded_2025} &
Grounded predictions of teamwork as a one-shot game: A multiagent multi-armed bandits approach &
Memodelkan kerja tim sukarela.\\ \midrule

~\cite{di_coupling_2023} &
The coupling effect between the environment and strategies drives the emergence of group cooperation &
Menganalisis keterkaitan memori dan lingkungan. \\ \midrule

~\cite{perera_learning_2025} &
Learning to cooperate against ensembles of diverse opponents &
Pendekatan RL yang skalabel untuk mendorong kerja sama. \\ \midrule

~\cite{li_exploiting_2025} &
Exploiting a No-Regret Opponent in Repeated Zero-Sum Games &
Kerangka untuk mengeksploitasi lawan~\textit{no-regret}. \\ \midrule

~\cite{freire_modeling_2023} &
Modeling Theory of Mind in Dyadic Games Using Adaptive Feedback Control &
Mengusulkan agen~\textit{Theory of Mind} berbasis kontrol. \\ \midrule

~\cite{hu_modeling_2023} &
Modeling opponent learning in multiagent repeated games &
Memperkenalkan metode berbasis Stackelberg. \\ \midrule

~\cite{elhamer_effects_2020} &
The effects of population size and information update rates on the emergent patterns of cooperative clusters in a large-scale social particle swarm model &
Pembaruan informasi cepat dalam jaringan SNS.\\ \midrule

~\cite{wang_achieving_2019} &
Achieving cooperation through deep multiagent reinforcement learning in sequential prisoner's dilemmas &
Memperkenalkan PD sekuensial dan metode~\textit{deep RL}.\\ \midrule

~\cite{jin_achieving_2025} &
Achieving collective welfare in multi-agent reinforcement learning via suggestion sharing &
Metode MARL dengan berbagi saran aksi.\\ \midrule

~\cite{de_weerd_higher-order_2022} &
Higher-order theory of mind is especially useful in unpredictable negotiations &
Menganalisis~\textit{higher-order Theory of Mind} dalam lingkungan yang tidak terprediksi.\\ \midrule

\end{tabular}
\end{table}

\section{Synthesis / Discussion}
Bagian ini mensintesis temuan-temuan utama dari studi yang ditinjau dengan tujuan mengidentifikasi pola asumsi perilaku lawan, keterbatasan metodologis yang berulang, serta celah penelitian yang masih terbuka dalam \textit{opponent modelling} untuk \textit{repeated games}. Sintesis difokuskan pada dimensi perilaku yang dapat diamati dari riwayat interaksi, alih-alih pada asumsi internal seperti tujuan optimisasi, struktur pembaruan parameter, atau representasi kebijakan lawan yang sering kali tidak dapat diakses secara langsung oleh agen.

Meskipun berbagai atribut telah diidentifikasi dalam studi-studi yang disertakan, hanya dimensi yang bersifat diskriminatif secara metodologis yang digunakan untuk perbandingan lintas karya. Atribut terkait lingkungan permainan dan protokol evaluasi dibahas secara terpisah untuk menghindari pencampuran antara asumsi perilaku dan pengaturan eksperimental.

\subsection{Asumsi perilaku lawan dalam \textit{opponent modelling} untuk \textit{Repeated Games}}
Untuk mengoperasionalkan asumsi perilaku lawan secara konsisten pada pengaturan permainan yang beragam, tinjauan ini mengabstraksikan properti perilaku yang dapat diinferensi dari \textit{interaction traces}. Secara khusus, perilaku lawan dikarakterisasi berdasarkan empat jenis dependensi yang dapat diamati: apakah aksi lawan (i) bervariasi lintas kondisi lingkungan dalam permainan yang sama, (ii) dimediasi oleh dinamika pada tingkat populasi, (iii) merespons secara langsung aksi agen, dan (iv) menunjukkan divergensi aksi pada riwayat interaksi terkini yang ekuivalen.

Kriteria-kriteria ini dievaluasi sepenuhnya pada tingkat perilaku eksternal dan tidak mengasumsikan adanya pengetahuan mengenai model internal lawan, mekanisme pembelajaran, maupun tujuan strategis yang dioptimalkan. Dengan demikian, kategorisasi yang digunakan tidak dimaksudkan sebagai taksonomi formal dari metode \textit{opponent modelling}, melainkan sebagai kerangka analitis untuk memungkinkan perbandingan lintas studi yang menggunakan paradigma pembelajaran, representasi strategi, dan abstraksi permainan yang berbeda.

Dalam studi yang tidak mendefinisikan asumsi perilaku lawan secara eksplisit, klasifikasi diturunkan secara konservatif berdasarkan pengaturan eksperimental dan dinamika interaksi yang dilaporkan. Tabel~\ref{tab:opponent-behavior} merangkum hasil kategorisasi tersebut beserta referensi terkait.

\begin{table}[!htbp]
\centering
\caption{Asumsi perilaku lawan berdasarkan dependensi perilaku yang dapat diamati.}\label{tab:opponent-behavior}
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\columnwidth}{cccc X p{2.5cm}}
\toprule
\textbf{Env.} 
& \textbf{Pop.} 
& \textbf{Agent}Sy
& \textbf{Div.}
& \textbf{Kategori Perilaku Lawan}
& \textbf{Ref.} \\
\midrule
--- & --- & \checkmark{} & --- & Reactive &~\cite{jin_achieving_2025} \\
\addlinespace{}

--- & \checkmark{} & --- & --- & Population-Conformist &~\cite{gomez_grounded_2025} \\
\addlinespace{}

--- & \checkmark{} & \checkmark{} & --- & Contextual Reactive &~\cite{elhamer_effects_2020} \\
\addlinespace{}

--- & --- & \checkmark{} & \checkmark{} & Learning Opponent &~\cite{qiao_o2m_2024, lv_inducing_2023, li_exploiting_2025, freire_modeling_2023, hu_modeling_2023, wang_achieving_2019, de_weerd_higher-order_2022} \\
\addlinespace{}

--- & \checkmark{} & \checkmark{} & \checkmark{} & Population-Contextual Strategic &~\cite{perera_learning_2025} \\
\addlinespace{}

\checkmark{} & \checkmark{} & --- & \checkmark{} & Heterogeneous Collective Behavior &~\cite{zhu_evolutionary_2025} \\
\addlinespace{}

\checkmark{} & \checkmark{} & \checkmark{} & \checkmark{} & Environment-Conditioned Strategic &~\cite{di_coupling_2023} \\
\bottomrule
\end{tabularx}

\begin{flushleft}
\vspace{2pt}
\footnotesize
\noindent~\textit{Catatan:} \\
Env. — perilaku bervariasi lintas lingkungan dalam permainan yang sama;\\
Pop. — perilaku dimediasi oleh interaksi tingkat populasi;\\
Agent — perilaku merespons secara langsung aksi agen;\\
Div. — divergensi aksi terjadi pada riwayat interaksi terkini yang ekuivalen.\\
Tanda centang menunjukkan adanya dependensi.
\end{flushleft}
\end{table}


Sejumlah pola konsisten muncul dari Tabel~\ref{tab:opponent-behavior}. Pertama, mayoritas studi terkini mengasumsikan lawan yang aksinya responsif terhadap agen dan menunjukkan divergensi perilaku pada riwayat interaksi yang setara~\cite{qiao_o2m_2024, lv_inducing_2023, li_exploiting_2025, freire_modeling_2023, hu_modeling_2023, wang_achieving_2019, de_weerd_higher-order_2022, perera_learning_2025, di_coupling_2023}. Pola ini mengindikasikan pergeseran fokus riset dari optimisasi terhadap strategi lawan yang tetap menuju ketahanan dan adaptasi terhadap lawan yang belajar atau berperilaku strategis secara dinamis.

Kedua, dependensi perilaku yang dimediasi populasi terutama muncul dalam studi pada lingkungan evolusioner atau berbasis jaringan, di mana aksi individu dipengaruhi secara tidak langsung oleh dinamika agregat populasi~\cite{zhu_evolutionary_2025, gomez_grounded_2025, elhamer_effects_2020, di_coupling_2023, perera_learning_2025}. Dalam pengaturan ini, responsivitas terhadap agen sering kali terpisah dari perubahan strategi pada tingkat populasi, menghasilkan dinamika yang berbeda secara kualitatif dibandingkan skenario pembelajaran dua pemain.

Terakhir, hanya sebagian kecil karya yang secara simultan memodelkan perilaku lawan yang bergantung pada lingkungan, dimediasi populasi, dan responsif terhadap aksi agen~\cite{di_coupling_2023}. Keterbatasan ini menunjukkan bahwa lawan strategis yang sepenuhnya terkondisi oleh konteks interaksi—baik pada tingkat individu maupun kolektif—masih relatif kurang dieksplorasi, terutama dalam pengaturan \textit{repeated games} dengan riwayat interaksi yang panjang dan tidak stasioner.

\subsection{Pendekatan metodologis dalam \textit{opponent modelling} untuk \textit{Repeated Games}}
Distribusi asumsi perilaku lawan pada Tabel~\ref{tab:opponent-behavior} mencerminkan pergeseran metodologis yang lebih luas dalam riset \textit{opponent modelling}, dari pengaturan dengan asumsi lawan yang tetap dan terkontrol menuju lawan yang adaptif, heterogen, dan berperilaku tidak stasioner. Namun, karakterisasi berbasis asumsi perilaku semata belum menjelaskan bagaimana kompleksitas tersebut dihadapi secara komputasional di sisi agen. Untuk itu, Tabel~\ref{tab:opponent_modelling_paradigm} mereorganisasi studi-studi terdahulu berdasarkan paradigma pemodelan dominan dan mekanisme pembelajaran yang digunakan oleh agen.

\begin{table}[!htbp]
\centering
\caption{Perbandingan pendekatan~\textit{opponent modelling} berdasarkan paradigma pemodelan dominan dan mekanisme pembelajaran.}
\label{tab:opponent_modelling_paradigm}
\renewcommand{\arraystretch}{1.25}
\begin{tabularx}{\columnwidth}{XX p{2.0cm}}
\toprule
\textbf{Paradigma Pemodelan} 
& \textbf{Mekanisme Pembelajaran / Pembaruan} 
& \textbf{Makalah} \\
\midrule

Reactive reinforcement learning
& RL berbasis nilai (DQN) 
&~\cite{lv_inducing_2023} \\

Gradient-based opponent shaping
&~\textit{Gradient descent} / pembaruan sadar lawan 
&~\cite{qiao_o2m_2024, hu_modeling_2023, wang_achieving_2019}\\

Recursive belief reasoning
& Pembaruan keyakinan Bayesian /~\textit{cognitive hierarchy} 
&~\cite{freire_modeling_2023, de_weerd_higher-order_2022} \\

Population-based training
&~\textit{Policy-gradient} MARL (sampling populasi) 
&~\cite{perera_learning_2025} \\

Evolutionary population dynamics
& Imitasi strategi berbasis~\textit{fitness} 
&~\cite{zhu_evolutionary_2025, di_coupling_2023, elhamer_effects_2020} \\

Bandit-based learning-in-games
&~\textit{Multi-armed bandit} /~\textit{smooth best response} 
&~\cite{gomez_grounded_2025} \\

System identification
& Model autoregresif dengan input eksogen (NARX) 
&~\cite{li_exploiting_2025} \\

Communication-driven coordination
&~\textit{Policy-gradient} dengan objektif komunikasi 
&~\cite{jin_achieving_2025} \\

\bottomrule
\end{tabularx}
\end{table}

Pendekatan-pendekatan tersebut dapat dibedakan lebih lanjut berdasarkan apakah adaptasi terhadap lawan ditangani secara eksplisit atau implisit. Paradigma \textit{opponent modelling} yang eksplisit—seperti \textit{gradient-based opponent shaping}~\cite{qiao_o2m_2024, hu_modeling_2023, wang_achieving_2019}, \textit{recursive belief reasoning}~\cite{freire_modeling_2023, de_weerd_higher-order_2022}, serta \textit{system identification}~\cite{li_exploiting_2025}—secara langsung membangun representasi internal perilaku lawan untuk memprediksi atau memengaruhi respons lawan di masa depan. Sebaliknya, pendekatan implisit, termasuk \textit{reactive reinforcement learning}~\cite{lv_inducing_2023}, \textit{population-based training}~\cite{perera_learning_2025}, dinamika evolusioner~\cite{zhu_evolutionary_2025, di_coupling_2023, elhamer_effects_2020}, dan \textit{bandit-based learning-in-games}~\cite{gomez_grounded_2025}, beradaptasi terhadap lawan tanpa mempertahankan model perilaku lawan yang terpisah.

Meskipun pendekatan berbasis \textit{reinforcement learning} dan \textit{policy-gradient} mendominasi literatur terkini, paradigma tersebut umumnya menggabungkan pemodelan lawan ke dalam proses optimisasi kebijakan agen. Akibatnya, dinamika perilaku lawan sering kali terenkapsulasi secara implisit dalam parameter kebijakan atau fungsi nilai, sehingga sulit untuk mengisolasi, menginterpretasi, atau memanfaatkan prediksi eksplisit terhadap respons lawan, terutama pada horizon interaksi yang panjang dan tidak stasioner.

Dalam konteks ini, pendekatan \textit{system identification}, khususnya model autoregresif dengan input eksogen (NARX), menawarkan alternatif metodologis yang berbeda. Alih-alih mengasumsikan struktur pembelajaran atau tujuan optimisasi lawan, NARX memodelkan perilaku lawan sebagai proses dinamis yang dapat diinferensi langsung dari riwayat interaksi. Dengan memanfaatkan dependensi temporal dan aksi agen sebagai sinyal eksogen, pendekatan ini secara alami selaras dengan asumsi perilaku lawan yang responsif dan menunjukkan divergensi aksi pada riwayat interaksi yang ekuivalen, sebagaimana diidentifikasi pada Tabel~\ref{tab:opponent-behavior}.

Selain itu, NARX memungkinkan pemisahan yang jelas antara proses prediksi perilaku lawan dan mekanisme pengambilan keputusan agen. Pemisahan ini memberikan fleksibilitas metodologis untuk menganalisis kualitas prediksi lawan secara independen dari kebijakan agen, serta memungkinkan integrasi dengan berbagai skema pengambilan keputusan tanpa memerlukan pelatihan ulang berbasis interaksi penuh seperti pada \textit{reinforcement learning}. Karakteristik ini menjadikan pendekatan berbasis NARX secara khusus menarik pada pengaturan dengan keterbatasan data, sumber daya komputasi, atau horizon waktu penelitian, sekaligus tetap mempertahankan kemampuan untuk menangkap dinamika perilaku lawan yang tidak stasioner dalam \textit{repeated games}.

\subsection{Bagaimana efektivitas strategi \textit{opponent modelling} dievaluasi dalam \textit{Repeated Games}?}

Praktik evaluasi secara implisit mendefinisikan apa yang dianggap sebagai keberhasilan dalam interaksi multi-agent yang adaptif, baik dalam bentuk hasil kerja sama, ketahanan terhadap eksploitasi, stabilitas perilaku, maupun akurasi prediksi. Tabel~\ref{tab:evaluation_metrics} merangkum lingkungan evaluasi dan metrik yang digunakan dalam penelitian-penelitian terdahulu, dengan tujuan mengidentifikasi pola evaluasi yang berulang serta aspek-aspek yang relatif terabaikan, alih-alih menetapkan standar normatif atau pemeringkatan kinerja.

\begin{table}[!htbp]
\centering
\caption{Lingkungan evaluasi dan metrik dalam penelitian}\label{tab:evaluation_metrics}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\columnwidth}{c >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
\midrule
\textbf{Ref.} & \textbf{Lingkungan Evaluasi} & \textbf{Metrik} \\
\midrule
~\cite{qiao_o2m_2024} & Self-play simetris & MSE selama pelatihan offline; akurasi memori laten \\

~\cite{zhu_evolutionary_2025} & Jaringan scale-free dengan strategi zero-determinant & Frekuensi kerja sama (C) dan eksploitasi (E) \\

~\cite{lv_inducing_2023} & Opponent adaptif (dirata-ratakan pada beberapa opponent) & Nilai reward \\

~\cite{gomez_grounded_2025} & Simulator teamwork-game khusus (aggregative public good games); eksperimen sintetis & Produktivitas tim agregat; uji kecocokan $\chi^2$ terhadap equilibrium; konvergensi ke Nash equilibrium; kontribusi individu \\

~\cite{di_coupling_2023} & Simulator evolutionary game pada jaringan terstruktur & Tingkat kerja sama; fraksi kooperator; ambang fase transisi \\

~\cite{perera_learning_2025} & Repeated matrix games dengan populasi opponent sintetis & Payoff rata-rata; tingkat kerja sama; robustness terhadap himpunan opponent; generalisasi \\

~\cite{li_exploiting_2025} & Repeated zero-sum games melawan Hedge, OMD, dan Regret Matching & Galat prediksi; payoff kumulatif; robustness terhadap non-stationarity \\

~\cite{freire_modeling_2023} & Repeated matrix games; simulasi robotik embodied waktu-kontinu & Efektivitas; stabilitas; akurasi prediksi \\

~\cite{hu_modeling_2023} & Simulasi repeated matrix game & Payoff rata-rata; kecepatan konvergensi; pemilihan equilibrium \\

~\cite{elhamer_effects_2020} & Simulasi continuous-space skala besar (FLAME GPU) & Tingkat kerja sama; ukuran dan jumlah klaster kooperatif; kecepatan agen; stabilitas klaster \\

~\cite{wang_achieving_2019} & Lingkungan SPD 2D khusus (Fruit Gathering; Apple---Pear games) & Reward individu rata-rata; total kesejahteraan sosial; akurasi deteksi derajat kerja sama \\

~\cite{jin_achieving_2025} & Benchmark MARL (Cleanup, Harvest, Sequential PD, Tragedy of the Commons) & Return ternormalisasi; tingkat kerja sama; kecepatan konvergensi; perbedaan kebijakan (MSE) \\

~\cite{de_weerd_higher-order_2022} & Simulasi Colored Trails dengan peningkatan ketidakpastian lingkungan & Skor allocator; skor responder; total kesejahteraan sosial \\
\midrule
\end{tabularx}
\end{table}

Seperti terlihat pada Tabel~\ref{tab:evaluation_metrics}, sebagian besar studi mengevaluasi efektivitas \textit{opponent modelling} melalui metrik kinerja agregat jangka panjang, khususnya tingkat kerja sama~\cite{di_coupling_2023, elhamer_effects_2020, wang_achieving_2019, jin_achieving_2025}, payoff rata-rata~\cite{lv_inducing_2023, perera_learning_2025, li_exploiting_2025, wang_achieving_2019, de_weerd_higher-order_2022}, serta konvergensi menuju equilibrium atau solusi stabil~\cite{gomez_grounded_2025, hu_modeling_2023, jin_achieving_2025}. Metrik-metrik ini secara inheren mengasumsikan interaksi berulang dengan horizon panjang, di mana kerugian eksplorasi pada tahap awal dapat dikompensasikan oleh perbaikan kinerja pada fase selanjutnya.

Namun demikian, asumsi horizon panjang ini membatasi daya representasi evaluasi terhadap skenario di mana interaksi bersifat terbatas, biaya eksplorasi signifikan, atau kesalahan awal sulit dipulihkan. Bahkan dalam studi yang mempertimbangkan lawan adaptif atau tidak stasioner, evaluasi umumnya dilakukan setelah fase pembelajaran mencapai stabilitas atau konvergensi~\cite{qiao_o2m_2024}, sehingga kinerja selama fase identifikasi lawan secara \textit{online} relatif kurang diperhatikan.

Keterbatasan ini menjadi semakin relevan dalam pengaturan \textit{repeated games} dengan horizon tetap yang pendek atau tidak pasti, di mana agen tidak dapat mengandalkan eksplorasi agresif tanpa risiko penurunan kinerja yang substansial. Dalam konteks tersebut, strategi eksplorasi yang terlalu invasif dapat menyebabkan salah koordinasi permanen, eksploitasi oleh lawan, atau kegagalan mencapai kerja sama sebelum interaksi berakhir. Oleh karena itu, evaluasi berbasis horizon panjang cenderung melebihkan keuntungan metode yang mengandalkan eksplorasi mendalam, sementara meremehkan pendekatan yang menekankan kehati-hatian dan efisiensi identifikasi perilaku lawan.

Sebagai respons terhadap celah ini, penggunaan horizon tetap yang pendek atau horizon stokastik dapat dipandang sebagai pilihan evaluasi yang lebih konservatif dan informatif. Horizon semacam ini secara eksplisit membatasi anggaran eksplorasi dan memaksa agen untuk menyeimbangkan antara identifikasi perilaku lawan dan kinerja langsung sejak tahap awal interaksi. Selain itu, horizon stokastik mengurangi insentif bagi strategi yang bergantung pada eksploitasi fase akhir permainan, sehingga mendorong perilaku yang lebih stabil dan berorientasi jangka pendek.

Dalam konteks \textit{opponent modelling} berbasis prediksi eksplisit, evaluasi dengan horizon terbatas juga memungkinkan analisis yang lebih tajam terhadap kegunaan prediksi perilaku lawan. Alih-alih menilai keberhasilan hanya berdasarkan hasil agregat jangka panjang, pengaturan ini menyoroti seberapa cepat dan seberapa akurat model lawan dapat memberikan informasi yang berguna untuk pengambilan keputusan, serta sejauh mana agen mampu memanfaatkan prediksi tersebut tanpa melakukan eksplorasi yang berlebihan. Dengan demikian, praktik evaluasi ini memberikan perspektif pelengkap terhadap literatur yang ada, khususnya dalam menilai efisiensi dan kehati-hatian strategi \textit{opponent modelling} pada pengaturan interaksi yang terbatas.

\section{Kesimpulan}
Bab ini mensintesis literatur \textit{opponent modelling} dalam interaksi strategis berulang dengan menelaah tiga dimensi utama: asumsi perilaku lawan yang mendasari, paradigma pemodelan yang digunakan oleh agen, serta praktik evaluasi yang menentukan kriteria keberhasilan. Tinjauan difokuskan pada pengaturan \textit{repeated games}, khususnya dilema sosial, di mana ketergantungan pada riwayat interaksi dan adaptasi perilaku lawan menjadi faktor sentral.

Analisis terhadap asumsi perilaku menunjukkan bahwa literatur terkini semakin menekankan lawan yang adaptif, responsif terhadap aksi agen, dan menunjukkan divergensi perilaku meskipun berada pada riwayat interaksi yang ekuivalen. Namun demikian, asumsi-asumsi tersebut sering kali tidak dinyatakan secara eksplisit dan tertanam secara implisit dalam desain eksperimen atau mekanisme pembelajaran. Akibatnya, pendekatan pemodelan yang tampak serupa secara metodologis dapat beroperasi di bawah asumsi perilaku lawan yang berbeda secara fundamental, sehingga menyulitkan perbandingan lintas studi dan interpretasi hasil secara konsisten.

Dari sisi metodologis, literatur menunjukkan keberagaman paradigma \textit{opponent modelling}, mulai dari pendekatan implisit berbasis optimisasi kebijakan dan dinamika populasi hingga pendekatan eksplisit yang mempertahankan representasi internal perilaku lawan. Meskipun pendekatan berbasis \textit{reinforcement learning} dan \textit{policy-gradient} mendominasi, dinamika perilaku lawan pada paradigma tersebut sering kali terenkapsulasi secara implisit dalam parameter kebijakan agen. Hal ini membatasi kemampuan untuk mengisolasi, mengevaluasi, dan memanfaatkan prediksi perilaku lawan secara langsung, terutama dalam pengaturan interaksi yang tidak stasioner.

Praktik evaluasi yang ada sebagian besar berfokus pada metrik kinerja agregat jangka panjang, seperti tingkat kerja sama, payoff rata-rata, dan konvergensi menuju equilibrium. Meskipun metrik-metrik ini relevan untuk menilai stabilitas pada horizon interaksi yang panjang, penerapannya secara luas mengasumsikan bahwa biaya eksplorasi dapat diabaikan dan kesalahan awal dapat dipulihkan seiring waktu. Akibatnya, skenario dengan horizon interaksi terbatas, eksplorasi yang mahal, serta sensitivitas tinggi terhadap keputusan awal masih relatif kurang terwakili dalam evaluasi yang ada.

Berdasarkan sintesis tersebut, dapat diidentifikasi celah penelitian pada \textit{opponent modelling} dalam pengaturan \textit{repeated games} yang bersifat \textit{online}, tidak stasioner, dan dibatasi oleh horizon interaksi yang pendek atau tidak pasti. Secara khusus, literatur masih kurang mengeksplorasi pendekatan yang memungkinkan identifikasi perilaku lawan dalam horizon yang pendek dan eksplisit dari riwayat interaksi, serta evaluasi yang menyoroti trade-off antara eksplorasi dan kinerja langsung pada fase awal interaksi. Celah ini membuka ruang bagi pendekatan metodologis yang berfokus pada pemodelan perilaku lawan berbasis observasi historis dan pengujian yang sensitif terhadap keterbatasan interaksi.

